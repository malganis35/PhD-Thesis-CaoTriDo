% $ biblatex auxiliary file $
% $ biblatex version 2.5 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated as
% required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup

\entry{Aizerman1964}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Aizerman}{A.}%
     {M.}{M.}%
     {}{}%
     {}{}}%
    {{}%
     {Braverman}{B.}%
     {E.}{E.}%
     {}{}%
     {}{}}%
    {{}%
     {Rozonoer}{R.}%
     {L.}{L.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{AMBERL1}
  \strng{fullhash}{AMBERL1}
  \field{labelalpha}{ABR64}
  \field{sortinit}{A}
  \field{abstract}{%
  Introduction of kernels%
  }
  \field{pages}{821\bibrangedash 837}
  \field{title}{{Theoretical foundations of the potential function method in
  pattern recognition learning}}
  \field{volume}{25}
  \field{journaltitle}{Automation and Remote Control}
  \field{year}{1964}
\endentry

\entry{Altman1992}{article}{}
  \name{author}{1}{}{%
    {{}%
     {Altman}{A.}%
     {N.S.}{N.}%
     {}{}%
     {}{}}%
  }
  \keyw{Confidence intervals,Local linear re- gression,Model building,Model
  checking,Smoothing.}
  \strng{namehash}{AN1}
  \strng{fullhash}{AN1}
  \field{labelalpha}{Alt92}
  \field{sortinit}{A}
  \field{abstract}{%
  Nonparametric regression is a set of techniques for es- timating a regression
  curve without making strong as- sumptions about the shape of the true
  regression func- tion. These techniques are therefore useful for building and
  checking parametric models, as well as for data description. Kernel and
  nearest-neighbor regression es- timators are local versions of univariate
  location esti- mators, and so they can readily be introduced to be- ginning
  students and consulting clients who are familiar with such summaries as the
  sample mean and median.%
  }
  \verb{doi}
  \verb 10.1080/00031305.1992.10475879
  \endverb
  \field{isbn}{0003-1305}
  \field{issn}{0003-1305}
  \field{number}{3}
  \field{pages}{175\bibrangedash 185}
  \field{title}{{An introduction to kernel and nearest-neighbor nonparametric
  regression}}
  \field{volume}{46}
  \field{journaltitle}{The American Statistician}
  \field{year}{1992}
\endentry

\entry{Abraham2010a}{inproceedings}{}
  \name{author}{2}{}{%
    {{}%
     {Abraham}{A.}%
     {Z.}{Z.}%
     {}{}%
     {}{}}%
    {{}%
     {Tan}{T.}%
     {P.N.}{P.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{AZTP1}
  \strng{fullhash}{AZTP1}
  \field{labelalpha}{AT10}
  \field{sortinit}{A}
  \field{abstract}{%
  Zero-inflated time series data are commonly encountered in many applications,
  including climate and ecological modeling, disease monitoring, manufacturing
  defect detection, and traffic monitoring. Such data often leads to poor model
  fitting using standard regression methods because they tend to underestimate
  the frequency of zeros and the magnitude of non-zero values. This paper
  presents an integrated framework that simultaneously performs classification
  and regression to accurately predict future values of a zero-inflated time
  series. A regression model is initially applied to predict the value of the
  time series. The regression output is then fed into a classification model to
  determine whether the predicted value should be adjusted to zero. Our
  regression and classification models are trained to optimize a joint
  objective function that considers both classification errors on the time
  series and regression errors on data points that have non-zero values. We
  demonstrate the effectiveness of our framework in the context of its
  application to a precipitation downscaling problem for climate impact
  assessment studies. Read More:
  http://epubs.siam.org/doi/abs/10.1137/1.9781611972801.57%
  }
  \field{booktitle}{ACM SIGKDD}
  \field{title}{{An Integrated Framework for Simultaneous Classification and
  Regression of Time-Series Data}}
  \verb{url}
  \verb https://siam.org/proceedings/datamining/2010/dm10{\_}057{\_}abrahamz.pd
  \verb f
  \endverb
  \field{year}{2010}
\endentry

\entry{Berndt1994a}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Berndt}{B.}%
     {D.}{D.}%
     {}{}%
     {}{}}%
    {{}%
     {Clifford}{C.}%
     {J.}{J.}%
     {}{}%
     {}{}}%
  }
  \keyw{dynamic programming,dynamic time warping,knowledge discovery,pat,tern
  analysis,time series}
  \strng{namehash}{BDCJ1}
  \strng{fullhash}{BDCJ1}
  \field{labelalpha}{BC94}
  \field{sortinit}{B}
  \field{abstract}{%
  Knowledge discovery in databases presents many interesting challenges within
  the context of providing computer tools for exploring large data archives.
  Electronic data repositories are growing qulckiy and contain data from
  commercial, scientific, and other domains. Much of this data is inherently
  temporal, such as stock prices or NASA telemetry data. Detecting patterns in
  such data streams or time series is an important knowledge discovery task.
  This paper describes some primary experiments with a dynamic programming
  approach to the problem. The pattern detection algorithm is based on the
  dynamic time warping technique used in the speech recognition field.
  Keywords: dynamic programming, dynamic time warping, knowledge discovery,
  pattern analysis, time series.%
  }
  \field{pages}{359\bibrangedash 370}
  \field{title}{{Using dynamic time warping to find patterns in time series}}
  \verb{url}
  \verb http://www.aaai.org/Papers/Workshops/1994/WS-94-03/WS94-03-031.pdf
  \endverb
  \field{volume}{398}
  \field{journaltitle}{Workshop on Knowledge Knowledge Discovery in Databases}
  \field{year}{1994}
\endentry

\entry{Benesty2009}{article}{}
  \name{author}{4}{}{%
    {{}%
     {Benesty}{B.}%
     {J.}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Chen}{C.}%
     {J.}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Huang}{H.}%
     {Y.}{Y.}%
     {}{}%
     {}{}}%
    {{}%
     {Cohen}{C.}%
     {I.}{I.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{BJ+1}
  \strng{fullhash}{BJCJHYCI1}
  \field{labelalpha}{Ben+09}
  \field{sortinit}{B}
  \field{abstract}{%
  This chapter develops several forms of the Pearson correlation coefficient in
  the different domains. This coefficient can be used as an optimization
  criterion to derive different optimal noise reduction filters [14], but is
  even more useful for analyzing these optimal filters for their noise
  reduction performance.%
  }
  \verb{doi}
  \verb 10.1007/978-3-642-00296-0
  \endverb
  \field{isbn}{978-3-642-00295-3}
  \field{title}{{Pearson correlation coefficient}}
  \verb{url}
  \verb http://www.springerlink.com/index/10.1007/978-3-642-00296-0$\backslash$
  \verb nhttp://link.springer.com/content/pdf/10.1007/978-3-642-00296-0{\_}5.pd
  \verb f
  \endverb
  \field{journaltitle}{Noise Reduction in Speech Processing}
  \field{year}{2009}
\endentry

\entry{Boser1992}{inproceedings}{}
  \name{author}{3}{}{%
    {{}%
     {Boser}{B.}%
     {B.E.}{B.}%
     {}{}%
     {}{}}%
    {{}%
     {Guyon}{G.}%
     {I.M.}{I.}%
     {}{}%
     {}{}}%
    {{}%
     {Vapnik}{V.}%
     {V.N.}{V.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{BBGIVV1}
  \strng{fullhash}{BBGIVV1}
  \field{labelalpha}{BGV92}
  \field{sortinit}{B}
  \field{abstract}{%
  A training algorithm that maximizes the margin between the training patterns
  and the decision boundary is presented. The technique is applicable to a wide
  variety of classifiaction functions, including Perceptrons, polynomials, and
  Radial Basis Functions. The effective number of parameters is adjusted
  automatically to match the complexity of the problem. The solution is
  expressed as a linear combination of supporting patterns. These are the
  subset of training patterns that are closest to the decision boundary. Bounds
  on the generalization performance based on the leave-one-out method and the
  VC-dimension are given. Experimental results on optical character recognition
  problems demonstrate the good generalization obtained when compared with
  other learning algorithms. 1 INTRODUCTION Good generalization performance of
  pattern classifiers is achieved when the capacity of the classification
  function is matched to the size of the training set. Classifiers with a large
  numb...%
  }
  \field{booktitle}{Proceedings of the 5th Annual ACM Workshop on Computational
  Learning Theory}
  \verb{doi}
  \verb 10.1.1.21.3818
  \endverb
  \field{isbn}{089791497X}
  \field{issn}{0-89791-497-X}
  \field{pages}{144\bibrangedash 152}
  \field{title}{{A Training Algorithm for Optimal Margin Classifiers}}
  \verb{url}
  \verb http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.21.3818
  \endverb
  \field{year}{1992}
\endentry

\entry{Bellet2012}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Bellet}{B.}%
     {A.}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Habrard}{H.}%
     {A.}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Sebban}{S.}%
     {M.}{M.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{BAHASM1}
  \strng{fullhash}{BAHASM1}
  \field{labelalpha}{BHS12}
  \field{sortinit}{B}
  \field{abstract}{%
  Similarity functions are a fundamental component of many learning algorithms.
  When dealing with string or tree-structured data, measures based on the edit
  distance are widely used, and there exist a few methods for learning them
  from data. However, these methods offer no theoretical guarantee as to the
  generalization ability and discriminative power of the learned similarities.
  In this paper, we propose an approach to edit similarity learning based on
  loss minimization, called GESL. It is driven by the notion of
  (ϵ,$\gamma$,$\tau$)-goodness, a theory that bridges the gap between the
  properties of a similarity function and its performance in classification.
  Using the notion of uniform stability, we derive generalization guarantees
  that hold for a large class of loss functions. We also provide experimental
  results on two real-world datasets which show that edit similarities learned
  with GESL induce more accurate and sparser classifiers than other (standard
  or learned) edit similarities.%
  }
  \verb{doi}
  \verb 10.1007/s10994-012-5293-8
  \endverb
  \field{isbn}{0885-6125}
  \field{issn}{0885-6125, 1573-0565}
  \field{number}{1-2}
  \field{pages}{5\bibrangedash 35}
  \field{title}{{Good edit similarity learning by loss minimization}}
  \verb{url}
  \verb http://link.springer.com/article/10.1007/s10994-012-5293-8$\backslash$n
  \verb http://link.springer.com/article/10.1007{\%}2Fs10994-012-5293-8$\backsl
  \verb ash$nhttp://link.springer.com/content/pdf/10.1007{\%}2Fs10994-012-5293-
  \verb 8.pdf
  \endverb
  \field{volume}{89}
  \field{journaltitle}{Machine Learning}
  \field{year}{2012}
\endentry

\entry{Bellet2013}{report}{}
  \name{author}{3}{}{%
    {{}%
     {Bellet}{B.}%
     {A.}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Habrard}{H.}%
     {A.}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Sebban}{S.}%
     {M.}{M.}%
     {}{}%
     {}{}}%
  }
  \keyw{(),edit distance,mahalanobis distance,metric learning,similarity
  learning}
  \strng{namehash}{BAHASM1}
  \strng{fullhash}{BAHASM1}
  \field{labelalpha}{BHS13}
  \field{sortinit}{B}
  \field{abstract}{%
  The need for appropriate ways to measure the distance or similarity between
  data is ubiquitous in machine learning, pattern recognition and data mining,
  but handcrafting such good metrics for specific problems is generally
  difficult. This has led to the emergence of metric learning, which aims at
  automatically learning a metric from data and has attracted a lot of interest
  in machine learning and related fields for the past ten years. This survey
  paper proposes a systematic review of the metric learning literature,
  highlighting the pros and cons of each approach. We pay particular attention
  to Mahalanobis distance metric learning, a well-studied and successful
  framework, but additionally present a wide range of methods that have
  recently emerged as powerful alternatives, including nonlinear metric
  learning, similarity learning and local metric learning. Recent trends and
  extensions, such as semi-supervised metric learning, metric learning for
  histogram data and the derivation of generalization guarantees, are also
  covered. Finally, this survey addresses metric learning for structured data,
  in particular edit distance learning, and attempts to give an overview of the
  remaining challenges in metric learning for the years to come.%
  }
  \verb{eprint}
  \verb 1306.6709
  \endverb
  \field{title}{{A Survey on Metric Learning for Feature Vectors and Structured
  Data}}
  \verb{url}
  \verb http://arxiv.org/abs/1306.6709
  \endverb
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/Desktop/ML Learning survey.pdf:pdf
  \endverb
  \field{type}{techreport}
  \field{eprinttype}{arXiv}
  \field{year}{2013}
\endentry

\entry{Bishop2006}{book}{}
  \name{author}{1}{}{%
    {{}%
     {Bishop}{B.}%
     {C.M.}{C.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{BC1}
  \strng{fullhash}{BC1}
  \field{labelalpha}{Bis06}
  \field{sortinit}{B}
  \field{abstract}{%
  The dramatic growth in practical applications for machine learning over the
  last ten years has been accompanied by many important developments in the
  underlying algorithms and techniques. For example, Bayesian methods have
  grown from a specialist niche to become mainstream, while graphical models
  have emerged as a general framework for describing and applying probabilistic
  techniques. The practical applicability of Bayesian methods has been greatly
  enhanced by the development of a range of approximate inference algorithms
  such as variational Bayes and expectation propagation, while new models based
  on kernels have had a significant impact on both algorithms and applications.
  This completely new textbook reflects these recent developments while
  providing a comprehensive introduction to the fields of pattern recognition
  and machine learning. It is aimed at advanced undergraduates or first-year
  PhD students, as well as researchers and practitioners. No previous knowledge
  of pattern recognition or machine learning concepts is assumed. Familiarity
  with multivariate calculus and basic linear algebra is required, and some
  experience in the use of probabilities would be helpful though not essential
  as the book includes a self-contained introduction to basic probability
  theory. The book is suitable for courses on machine learning, statistics,
  computer science, signal processing, computer vision, data mining, and
  bioinformatics. Extensive support is provided for course instructors,
  including more than 400 exercises, graded according to difficulty. Example
  solutions for a subset of the exercises are available from the book web site,
  while solutions for the remainder can be obtained by instructors from the
  publisher. The book is supported by a great deal of additional material, and
  the reader is encouraged to visit the book web site for the latest
  information. A forthcoming companion volume will deal with practical aspects
  of pattern recognition and machine learning, and will include free software
  implementations of the key algorithms along with example data sets and
  demonstration programs. Christopher Bishop is Assistant Director at Microsoft
  Research Cambridge, and also holds a Chair in Computer Science at the
  University of Edinburgh. He is a Fellow of Darwin College Cambridge, and was
  recently elected Fellow of the Royal Academy of Engineering. The author's
  previous textbook "Neural Networks for Pattern Recognition" has been widely
  adopted.%
  }
  \field{booktitle}{Pattern Recognition}
  \verb{doi}
  \verb 10.1117/1.2819119
  \endverb
  \verb{eprint}
  \verb 0-387-31073-8
  \endverb
  \field{isbn}{9780387310732}
  \field{issn}{10179909}
  \field{number}{4}
  \field{pages}{738}
  \field{title}{{Pattern Recognition and Machine Learning}}
  \verb{url}
  \verb http://www.library.wisc.edu/selectedtocs/bg0137.pdf
  \endverb
  \field{volume}{4}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/Google Drive/Th{\`{e}}se/Bibliographie
  \verb /Livre/Bishop - Pattern Recognition and Machine Learning.pdf:pdf
  \endverb
  \field{eprinttype}{arXiv}
  \field{year}{2006}
\endentry

\entry{Brigham1967}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Brigham}{B.}%
     {E.O.}{E.}%
     {}{}%
     {}{}}%
    {{}%
     {Morrow}{M.}%
     {R.E.}{R.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{BEMR1}
  \strng{fullhash}{BEMR1}
  \field{labelalpha}{BM67}
  \field{sortinit}{B}
  \field{abstract}{%
  The fast Fourier transform (FFT), a computer algorithm that computes the
  discrete Fourier transform much faster than other algorithms, is explained.
  Examples and detailed procedures are provided to assist the reader in
  learning how to use the algorithm. The savings in computer time can be huge;
  for example, an N = 210-point transform can be computed with the FFT 100
  times faster than with the use of a direct approach.%
  }
  \verb{doi}
  \verb 10.1109/MSPEC.1967.5217220
  \endverb
  \field{isbn}{0018-9235}
  \field{issn}{0018-9235}
  \field{number}{12}
  \field{pages}{63 \bibrangedash 70}
  \field{title}{{The fast Fourier transform}}
  \verb{url}
  \verb http://ieeexplore.ieee.org/ielx5/6/5217195/05217220.pdf?tp={\&}arnumber
  \verb =5217220{\&}isnumber=5217195$\backslash$nhttp://ieeexplore.ieee.org/sta
  \verb mp/stamp.jsp?tp={\&}arnumber=5217220
  \endverb
  \field{volume}{4}
  \field{journaltitle}{Spectrum, IEEE}
  \field{year}{1967}
\endentry

\entry{Belongie2002}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Belongie}{B.}%
     {S.}{S.}%
     {}{}%
     {}{}}%
    {{}%
     {Malik}{M.}%
     {J.}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Puzicha}{P.}%
     {J.}{J.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{BSMJPJ1}
  \strng{fullhash}{BSMJPJ1}
  \field{labelalpha}{BMP02}
  \field{sortinit}{B}
  \field{abstract}{%
  We present a novel approach to measuring similarity between shapes and
  exploit it for object recognition. In our framework, the measurement of
  similarity is preceded by 1) solving for correspondences between points on
  the two shapes, 2) using the correspondences to estimate an aligning
  transform. In order to solve the correspondence problem, we attach a
  descriptor, the shape context, to each point. The shape context at a
  reference point captures the distribution of the remaining points relative to
  it, thus offering a globally discriminative characterization. Corresponding
  points on two similar shapes will have similar shape contexts, enabling us to
  solve for correspondences as an optimal assignment problem. Given the point
  correspondences, we estimate the transformation that best aligns the two
  shapes; regularized thin-plate splines provide a flexible class of
  transformation maps for this purpose. The dissimilarity between the two
  shapes is computed as a sum of matching errors between corresponding points,
  together with a term measuring the magnitude of the aligning transform. We
  treat recognition in a nearest-neighbor classification framework as the
  problem of finding the stored prototype shape that is maximally similar to
  that in the image. Results are presented for silhouettes, trademarks,
  handwritten digits, and the COIL data set.%
  }
  \verb{doi}
  \verb 10.1.1.18.8852
  \endverb
  \field{isbn}{9781424455409}
  \field{issn}{01628828}
  \field{pages}{509\bibrangedash 522}
  \field{title}{{Shape Matching and Object Recognition Using Shape Contexts}}
  \verb{url}
  \verb http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.18.8852
  \endverb
  \field{volume}{24}
  \field{journaltitle}{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}
  \field{year}{2002}
\endentry

\entry{Carroll1980}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Carroll}{C.}%
     {J.D.}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Arabie}{A.}%
     {P.}{P.}%
     {}{}%
     {}{}}%
  }
  \keyw{article is focused on,behavior,comparison are un-,data
  analysis,hypothesis testing,implicit equation,known or
  indefinable,lognormal,multidimensional scaling,research methodology,the
  spatial distance,the specific bases of,this introductory}
  \strng{namehash}{CJAP1}
  \strng{fullhash}{CJAP1}
  \field{labelalpha}{CA80}
  \field{sortinit}{C}
  \field{abstract}{%
  Individual-differences multidimensional scaling (INDSCAL) determined the
  dimensions underlying ratings of electrocutaneous stimuli, which ranged from
  innocuous levels to individual pain intolerance at each of three frequencies.
  Twenty-five healthy males made pairwise similarity judgments of these 15
  stimuli for the INDSCAL procedure, and then rated each stimulus on nine
  property scales. Signal detection theory indices, as well as ratings on the
  McGill Pain Questionnaire (MPQ), were also obtained. A Sensory Magnitude
  dimension scaled the stimuli from lowest to highest perceived intensity; this
  dimension was related to sensory, affective, and arousal property scales. A
  Frequency dimension ordered the stimuli from lowest to highest frequency;
  this dimension was related to the Fast-Slow property. Compared to the
  Frequency dimension, the Sensory Magnitude dimension was more salient to
  subjects who better discriminated among painful stimulus intensities, set a
  more stoical pain report criterion, and were less apt to endorse
  frequency-related MPQ descriptors. Thus, variation of physical intensity and
  frequency elicited complementary dimensions of subjective judgment, which
  were related to perceptual and attitudinal differences among individuals.%
  }
  \verb{doi}
  \verb 10.1146/annurev.ps.31.020180.003135
  \endverb
  \field{isbn}{pdf vorhanden}
  \field{issn}{0066-4308}
  \field{number}{1}
  \field{pages}{607\bibrangedash 649}
  \field{title}{{Multidimensional scaling.}}
  \field{volume}{31}
  \field{journaltitle}{Annual review of psychology}
  \field{year}{1980}
\endentry

\entry{Caiado2006c}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Caiado}{C.}%
     {J.}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Crato}{C.}%
     {N.}{N.}%
     {}{}%
     {}{}}%
    {{}%
     {Pe{\~{n}}a}{P.}%
     {D.}{D.}%
     {}{}%
     {}{}}%
  }
  \keyw{Autocorrelation function,Classification,Clustering,Euclidean
  distance,Periodogram,Stationary and non-stationary time series}
  \strng{namehash}{CJCNPD1}
  \strng{fullhash}{CJCNPD1}
  \field{labelalpha}{CCP06}
  \field{sortinit}{C}
  \field{abstract}{%
  The statistical discrimination and clustering literature has studied the
  problem of identifying similarities in time series data. Some studies use
  non-parametric approaches for splitting a set of time series into clusters by
  looking at their Euclidean distances in the space of points. A new measure of
  distance between time series based on the normalized periodogram is proposed.
  Simulation results comparing this measure with others parametric and
  non-parametric metrics are provided. In particular, the classification of
  time series as stationary or as non-stationary is discussed. The use of both
  hierarchical and non-hierarchical clustering algorithms is considered. An
  illustrative example with economic time series data is also presented. ©
  2005 Elsevier B.V. All rights reserved.%
  }
  \verb{doi}
  \verb 10.1016/j.csda.2005.04.012
  \endverb
  \field{isbn}{01679473}
  \field{issn}{01679473}
  \field{number}{10}
  \field{pages}{2668\bibrangedash 2684}
  \field{title}{{A periodogram-based metric for time series classification}}
  \field{volume}{50}
  \field{journaltitle}{Computational Statistics and Data Analysis}
  \field{year}{2006}
\endentry

\entry{Cover1967b}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Cover}{C.}%
     {T.}{T.}%
     {}{}%
     {}{}}%
    {{}%
     {Hart}{H.}%
     {P.}{P.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{CTHP1}
  \strng{fullhash}{CTHP1}
  \field{labelalpha}{CH67}
  \field{sortinit}{C}
  \field{abstract}{%
  The nearest neighbor decision rule assigns to an unclassified sample point
  the classification of the nearest of a set of previously classified points.
  This rule is independent of the underlying joint distribution on the sample
  points and their classifications, and hence the probability of
  error<tex>R</tex>of such a rule must be at least as great as the Bayes
  probability of error<tex>R{\^{}}{\{}ast{\}}</tex>--the minimum probability of
  error over all decision rules taking underlying probability structure into
  account. However, in a large sample analysis, we will show in
  the<tex>M</tex>-category case that<tex>R{\^{}}{\{}ast{\}} leq R leq
  R{\^{}}{\{}ast{\}}(2 --MR{\^{}}{\{}ast{\}}/(M-1))</tex>, where these bounds
  are the tightest possible, for all suitably smooth underlying distributions.
  Thus for any number of categories, the probability of error of the nearest
  neighbor rule is bounded above by twice the Bayes probability of error. In
  this sense, it may be said that half the classification information in an
  infinite sample set is contained in the nearest neighbor.%
  }
  \verb{doi}
  \verb 10.1109/TIT.1967.1053964
  \endverb
  \field{isbn}{0018-9448}
  \field{issn}{0018-9448}
  \field{number}{1}
  \field{pages}{21\bibrangedash 27}
  \field{title}{{Nearest neighbor pattern classification}}
  \field{volume}{13}
  \field{journaltitle}{IEEE Transactions on Information Theory}
  \field{year}{1967}
\endentry

\entry{Chan2008}{article}{}
  \name{author}{4}{}{%
    {{}%
     {Chan}{C.}%
     {A.}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Hamdy}{H.}%
     {M.}{M.}%
     {}{}%
     {}{}}%
    {{}%
     {Badre}{B.}%
     {A.}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Badee}{B.}%
     {V.}{V.}%
     {}{}%
     {}{}}%
  }
  \keyw{Biometric,Electrocardiogram (ECG),Intra subject variability,Person
  identification,Wavelets}
  \strng{namehash}{CA+1}
  \strng{fullhash}{CAHMBABV1}
  \field{labelalpha}{Cha+08}
  \field{sortinit}{C}
  \field{abstract}{%
  In this paper, the authors present an evaluation of a new biometric based on
  electrocardiogram (ECG) waveforms. ECG data were collected from 50 subjects
  during three data-recording sessions on different days using a simple user
  interface, where subjects held two electrodes on the pads of their thumbs
  using their thumb and index fingers. Data from session 1 were used to
  establish an enrolled database, and data from the remaining two sessions were
  used as test cases. Classification was performed using three different
  quantitative measures: percent residual difference, correlation coefficient,
  and a novel distance measure based on wavelet transform. The wavelet distance
  measure has a classification accuracy of 89{\%}, outperforming the other
  methods by nearly 10{\%}. This ECG person-identification modality would be a
  useful supplement for conventional biometrics, such as fingerprint and palm
  recognition systems.%
  }
  \verb{doi}
  \verb 10.1109/TIM.2007.909996
  \endverb
  \field{isbn}{0018-9456}
  \field{issn}{00189456}
  \field{number}{2}
  \field{pages}{248\bibrangedash 253}
  \field{title}{{Wavelet distance measure for person identification using
  electrocardiograms}}
  \field{volume}{57}
  \field{journaltitle}{IEEE Transactions on Instrumentation and Measurement}
  \field{year}{2008}
\endentry

\entry{Chatpatanasiri2010}{article}{}
  \name{author}{4}{}{%
    {{}%
     {Chatpatanasiri}{C.}%
     {R.}{R.}%
     {}{}%
     {}{}}%
    {{}%
     {Korsrilabutr}{K.}%
     {T.}{T.}%
     {}{}%
     {}{}}%
    {{}%
     {Tangchanachaianan}{T.}%
     {P.}{P.}%
     {}{}%
     {}{}}%
    {{}%
     {Kijsirikul}{K.}%
     {B.}{B.}%
     {}{}%
     {}{}}%
  }
  \keyw{Dimensionality reduction,Distance metric learning,Kernel
  alignment,Kernel machines,Representer theorem}
  \strng{namehash}{CR+1}
  \strng{fullhash}{CRKTTPKB1}
  \field{labelalpha}{Cha+10}
  \field{sortinit}{C}
  \field{abstract}{%
  This paper focuses on developing a new framework of kernelizing Mahalanobis
  distance learners. The new KPCA trick framework offers several practical
  advantages over the classical kernel trick framework, e.g. no mathematical
  formulas and no reprogramming are required for a kernel implementation, a way
  to speed up an algorithm is provided with no extra work, the framework avoids
  troublesome problems such as singularity. Rigorous representer theorems in
  countably infinite dimensional spaces are given to validate our framework.
  Furthermore, unlike previous works which always apply brute force methods to
  select a kernel, we derive a kernel alignment formula based on quadratic
  programming which can efficiently construct an appropriate kernel for a given
  dataset. ?? 2010.%
  }
  \verb{doi}
  \verb 10.1016/j.neucom.2009.11.037
  \endverb
  \field{issn}{09252312}
  \field{number}{10-12}
  \field{pages}{1570\bibrangedash 1579}
  \field{title}{{A new kernelization framework for Mahalanobis distance
  learning algorithms}}
  \verb{url}
  \verb http://dx.doi.org/10.1016/j.neucom.2009.11.037$\backslash$nhttp://ac.el
  \verb s-cdn.com/S0925231210001165/1-s2.0-S0925231210001165-main.pdf?{\_}tid=9
  \verb 23c9f62-a756-11e4-970c-00000aab0f6b{\&}acdnat=1422495279{\_}136843023ec
  \verb d8d5dc1ba9d83e2539013
  \endverb
  \field{volume}{73}
  \field{journaltitle}{Neurocomputing}
  \field{year}{2010}
\endentry

\entry{Chatfield2004}{book}{}
  \name{author}{1}{}{%
    {{}%
     {Chatfield}{C.}%
     {C.}{C.}%
     {}{}%
     {}{}}%
  }
  \keyw{Time-series analysis.}
  \strng{namehash}{CC1}
  \strng{fullhash}{CC1}
  \field{labelalpha}{Cha04}
  \field{sortinit}{C}
  \field{abstract}{%
  "Since 1975, The Analysis of Time Series: An Introduction has introduced
  legions of statistics students and researchers to the theory and practice of
  time series analysis. The sixth edition provides an accessible, comprehensive
  introduction to the theory and practice of time series analysis. The
  treatment covers a wide range of topics, including ARIMA probability models,
  forecasting methods, spectral analysis, linear systems, state-space models,
  and the Kalman filter. It also addresses nonlinear, multivariate, and
  long-memory models. The author has carefully updated each chapter, added new
  discussions, incorporated new datasets, and made those datasets available at
  www.crcpress.com."--BOOK JACKET.%
  }
  \field{booktitle}{Texts in statistical science}
  \field{isbn}{1584883170}
  \field{pages}{xiii, 333 p.}
  \field{title}{{The analysis of time series : an introduction}}
  \field{year}{2004}
\endentry

\entry{Chen1996}{misc}{}
  \name{author}{3}{}{%
    {{}%
     {Chen}{C.}%
     {M.S.}{M.}%
     {}{}%
     {}{}}%
    {{}%
     {Han}{H.}%
     {J.}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Yu}{Y.}%
     {P.S.}{P.}%
     {}{}%
     {}{}}%
  }
  \keyw{Association rules,Classification,Data clustering,Data cubes,Data
  generalization and characterization,Data mining,Knowledge
  discovery,Multiple-dimensional databases,Pattern matching algorithms}
  \strng{namehash}{CMHJYP1}
  \strng{fullhash}{CMHJYP1}
  \field{labelalpha}{CHY96}
  \field{sortinit}{C}
  \field{abstract}{%
  Mining information and knowledge from large databases has been recognized by
  many researchers as a key research topic in database systems and machine
  learning, and by many industrial companies as an important area with an
  opportunity of major revenues. Researchers in many different fields have
  shown great interest in data mining. Several emerging applications in
  information-providing services, such as data warehousing and online services
  over the Internet, also call for various data mining techniques to better
  understand user behavior, to improve the service provided and to increase
  business opportunities. In response to such a demand, this article provides a
  survey, from a database researcher's point of view, on the data mining
  techniques developed recently. A classification of the available data mining
  techniques is provided and a comparative study of such techniques is
  presented%
  }
  \field{booktitle}{IEEE Transactions on Knowledge and Data Engineering}
  \verb{doi}
  \verb 10.1109/69.553155
  \endverb
  \field{isbn}{1041-4347}
  \field{issn}{10414347}
  \field{number}{6}
  \field{pages}{866\bibrangedash 883}
  \field{title}{{Data mining: An Overview from a Database Perspective}}
  \field{volume}{8}
  \field{year}{1996}
\endentry

\entry{Cochran1977}{article}{}
  \name{author}{1}{}{%
    {{}%
     {Cochran}{C.}%
     {W.}{W.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{CW1}
  \strng{fullhash}{CW1}
  \field{labelalpha}{Coc77}
  \field{sortinit}{C}
  \field{abstract}{%
  Commentary by : Cochran William C. Current Contents : {\#}19, May 9, 1977%
  }
  \field{pages}{1}
  \field{title}{{Statistical methods applied to experiments in agriculture and
  biology. 5th ed. Ames, Iowa: Iowa State University Press, 1956.}}
  \verb{url}
  \verb papers3://publication/uuid/8C5C843E-F853-4CB4-82BC-1141F0C01CB4
  \endverb
  \field{volume}{19}
  \field{journaltitle}{Citation Classics}
  \field{year}{1977}
\endentry

\entry{Crammer2001}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Crammer}{C.}%
     {K.}{K.}%
     {}{}%
     {}{}}%
    {{}%
     {Singer}{S.}%
     {Y.}{Y.}%
     {}{}%
     {}{}}%
  }
  \keyw{kernel machines,multiclass problems,svm}
  \strng{namehash}{CKSY1}
  \strng{fullhash}{CKSY1}
  \field{labelalpha}{CS01}
  \field{sortinit}{C}
  \field{abstract}{%
  In this paper we describe the algorithmic implementation of multiclass
  kernel-based vector machines. Our starting point is a generalized notion of
  the margin to multiclass problems. Using this notion we cast multiclass
  categorization problems as a constrained optimization problem with a
  quadratic objective function. Unlike most of previous approaches which
  typically decompose a multiclass problem into multiple independent binary
  classification tasks, our notion of margin yields a direct method for
  training multiclass predictors. By using the dual of the optimization problem
  we are able to incorporate kernels with a compact set of constraints and
  decompose the dual problem into multiple optimization problems of reduced
  size. We describe an efficient fixed-point algorithm for solving the reduced
  optimization problems and prove its convergence. We then discuss technical
  details that yield significant running time improvements for large datasets.
  Finally, we describe various experiments with our approach comparing it to
  previously studied kernel-based methods. Our experiments indicate that for
  multiclass problems we attain state-of-the-art accuracy%
  }
  \verb{doi}
  \verb 10.1162/15324430260185628
  \endverb
  \field{isbn}{1532-4435}
  \field{issn}{15324435}
  \field{pages}{265\bibrangedash 292}
  \field{title}{{On the Algorithmic Implementation of Multiclass Kernel-based
  Vector Machines}}
  \verb{url}
  \verb http://machinelearning.wustl.edu/mlpapers/paper{\_}files/CrammerS01.pdf
  \verb $\backslash$nhttp://www.jmlr.org/papers/volume2/crammer01a/crammer01a.p
  \verb df
  \endverb
  \field{volume}{2}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley D
  \verb esktop/Downloaded/Crammer, Singer - 2001 - On the Algorithmic Implement
  \verb ation of Multiclass Kernel-based Vector Machines.pdf:pdf
  \endverb
  \field{journaltitle}{Journal of Machine Learning Research}
  \field{year}{2001}
\endentry

\entry{Cao2001}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Cao}{C.}%
     {L.}{L.}%
     {}{}%
     {}{}}%
    {{}%
     {Tay}{T.}%
     {F.E.H.}{F.}%
     {}{}%
     {}{}}%
  }
  \keyw{back propagation
  algorithm,financial,generalisation,multi-layer,perceptron,support vector
  machines,time series forecasting}
  \strng{namehash}{CLTF1}
  \strng{fullhash}{CLTF1}
  \field{labelalpha}{CT01}
  \field{sortinit}{C}
  \field{pages}{184\bibrangedash 192}
  \field{title}{{Financial Forecasting Using Support Vector Machines}}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley D
  \verb esktop/Downloaded/Cao, Tay - 2001 - Financial Forecasting Using Support
  \verb  Vector Machines.pdf:pdf
  \endverb
  \field{journaltitle}{Neural Computing {\&} Applications}
  \field{year}{2001}
\endentry

\entry{Cortes1995}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Cortes}{C.}%
     {C.}{C.}%
     {}{}%
     {}{}}%
    {{}%
     {Vapnik}{V.}%
     {V.}{V.}%
     {}{}%
     {}{}}%
  }
  \keyw{efficient learning algorithms,neural networks,pattern
  recognition,polynomial classifiers,radial basis function classifiers}
  \strng{namehash}{CCVV1}
  \strng{fullhash}{CCVV1}
  \field{labelalpha}{CV95}
  \field{sortinit}{C}
  \field{abstract}{%
  The support-vector network is a new leaming machine for two-group
  classification problems. The machine conceptually implements the following
  idea: input vectors are non-linearly mapped to a very high- dimension feature
  space. In this feature space a linear decision surface is constructed.
  Special properties of the decision surface ensures high generalization
  ability of the learning machine. The idea behind the support-vector network
  was previously implemented for the restricted case where the training data
  can be separated without errors. We here extend this result to non-separable
  training data. High generalization ability of support-vector networks
  utilizing polynomial input transformations is demon- strated. We also compare
  the performance of the support-vector network to various classical learning
  algorithms that all took part in a benchmark study of Optical Character
  Recognition.%
  }
  \verb{doi}
  \verb 10.1007/BF00994018
  \endverb
  \verb{eprint}
  \verb arXiv:1011.1669v3
  \endverb
  \field{isbn}{0885-6125}
  \field{issn}{08856125}
  \field{number}{3}
  \field{pages}{273\bibrangedash 297}
  \field{title}{{Support-vector networks}}
  \field{volume}{20}
  \field{journaltitle}{Machine Learning}
  \field{eprinttype}{arXiv}
  \field{year}{1995}
\endentry

\entry{Campbell2011}{book}{}
  \name{author}{2}{}{%
    {{}%
     {Campbell}{C.}%
     {C.}{C.}%
     {}{}%
     {}{}}%
    {{}%
     {Ying}{Y.}%
     {Y.}{Y.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{CCYY1}
  \strng{fullhash}{CCYY1}
  \field{labelalpha}{CY11}
  \field{sortinit}{C}
  \field{booktitle}{Synthesis Lectures on Artificial Intelligence and Machine
  Learning}
  \verb{doi}
  \verb 10.2200/S00324ED1V01Y201102AIM010
  \endverb
  \field{isbn}{9781608456161}
  \field{issn}{1939-4608}
  \field{number}{1}
  \field{pages}{1\bibrangedash 95}
  \field{title}{{Learning with Support Vector Machines}}
  \verb{url}
  \verb http://www.morganclaypool.com/doi/abs/10.2200/S00324ED1V01Y201102AIM010
  \endverb
  \field{volume}{5}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley D
  \verb esktop/Downloaded/Campbell, Ying - 2011 - Learning with Support Vector
  \verb Machines.pdf:pdf
  \endverb
  \field{year}{2011}
  \warn{\item Invalid format of field 'month'}
\endentry

\entry{Douzal-Chouakria2003}{inproceedings}{}
  \name{author}{1}{}{%
    {{}%
     {Douzal-Chouakria}{D.-C.}%
     {A.}{A.}%
     {}{}%
     {}{}}%
  }
  \list{publisher}{1}{%
    {Springer Berlin Heidelberg}%
  }
  \strng{namehash}{DCA1}
  \strng{fullhash}{DCA1}
  \field{labelalpha}{DC03}
  \field{sortinit}{D}
  \field{booktitle}{Advances in Intelligent Data Analysis V}
  \field{pages}{566\bibrangedash 577}
  \field{title}{{Compression technique preserving correlations of a
  multivariate temporal sequence}}
  \field{year}{2003}
\endentry

\entry{AhlameDouzal-Chouakria2012}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Douzal-Chouakria}{D.-C.}%
     {A.}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Amblard}{A.}%
     {C.}{C.}%
     {}{}%
     {}{}}%
  }
  \list{publisher}{1}{%
    {VLDB Endowment}%
  }
  \keyw{Classification,Supervised classification,Time series proximity
  measures,trees Learning metric}
  \strng{namehash}{DCAAC1}
  \strng{fullhash}{DCAAC1}
  \field{labelalpha}{DCA12}
  \field{sortinit}{D}
  \field{abstract}{%
  This paper proposes an extension of classification trees to time series input
  variables. A new split criterion based on time series proximities is
  introduced. First, the criterion relies on an adaptive (i.e., parameterized)
  time series metric to cover both behaviors and values proximities. The
  metrics parameters may change from one internal node to another to achieve
  the best bisection of the set of time series. Second, the criterion involves
  the automatic extraction of the most discriminating subsequences. The
  proposed time series classification tree is applied to a wide range of
  datasets: public and new, real and synthetic, univariate and multivariate
  data. We show, through the experiments performed in this study, that the
  proposed tree outperforms temporal trees using standard time series distances
  and performs well compared to other competitive time series classifiers%
  }
  \field{isbn}{0000000000000}
  \field{issn}{2150-8097}
  \field{number}{3}
  \field{pages}{1076\bibrangedash 1091}
  \field{title}{{Classification trees for time series}}
  \field{volume}{45}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley D
  \verb esktop/Downloaded/Douzal-Chouakria, Amblard - 2011 - Classification tre
  \verb es for time series.pdf:pdf
  \endverb
  \field{journaltitle}{Pattern Recognition}
  \field{year}{2012}
\endentry

\entry{Chouakria2007}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Douzal-Chouakria}{D.-C.}%
     {A.}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Nagabhushan}{N.}%
     {P.}{P.}%
     {}{}%
     {}{}}%
  }
  \keyw{Classification,Dynamic time warping,Fr??chet distance,Time Series}
  \strng{namehash}{DCANP1}
  \strng{fullhash}{DCANP1}
  \field{labelalpha}{DCN07}
  \field{sortinit}{D}
  \field{abstract}{%
  Abstract The most widely used measures of time series proximity are the
  Euclidean distance and dynamic time warping. The latter can be derived from
  the distance introduced by Maurice Frchet in 1906 to account for the
  proximity between curves. The major limitation of these proximity measures is
  that they are based on the closeness of the values regardless of the
  similarity w.r.t. the growth behavior of the time series. To alleviate this
  drawback we propose a new dissimilarity index, based on an automatic adaptive
  tuning function, to include both proximity measures w.r.t. values and w.r.t.
  behavior. A comparative numerical analysis between the proposed index and the
  classical distance measures is performed on the basis of two datasets: a
  synthetic dataset and a dataset from a public health study.%
  }
  \verb{doi}
  \verb 10.1007/s11634-006-0004-6
  \endverb
  \field{issn}{18625347}
  \field{pages}{5\bibrangedash 21}
  \field{title}{{Adaptive dissimilarity index for measuring time series
  proximity}}
  \field{journaltitle}{Advances in Data Analysis and Classification}
  \field{year}{2007}
\endentry

\entry{Deza2009}{book}{}
  \name{author}{2}{}{%
    {{}%
     {Deza}{D.}%
     {M.M.}{M.}%
     {}{}%
     {}{}}%
    {{}%
     {Deza}{D.}%
     {E.}{E.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{DMDE1}
  \strng{fullhash}{DMDE1}
  \field{labelalpha}{DD09}
  \field{sortinit}{D}
  \field{abstract}{%
  Distance metrics and distances have become an essential tool in many areas of
  pure and applied Mathematics, and this encyclopedia is the first one to treat
  the ...%
  }
  \field{booktitle}{Media}
  \verb{doi}
  \verb 10.1007/978-3-642-00234-2
  \endverb
  \verb{eprint}
  \verb 0505065
  \endverb
  \field{isbn}{9783642002335}
  \field{issn}{14338351}
  \field{pages}{590}
  \field{title}{{Encyclopedia of Distances}}
  \verb{url}
  \verb http://www.springerlink.com/index/10.1007/978-3-642-00234-2
  \endverb
  \field{volume}{2006}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/Google Drive/Th{\`{e}}se/Bibliographie
  \verb /Livre/Deza - 2014 - Encyclopedia of Distances - Livre.pdf:pdf
  \endverb
  \field{eprinttype}{arXiv}
  \field{eprintclass}{arXiv:gr-qc}
  \field{year}{2009}
\endentry

\entry{Denoeux1995}{article}{}
  \name{author}{1}{}{%
    {{}%
     {Denoeux}{D.}%
     {T.}{T.}%
     {}{}%
     {}{}}%
  }
  \keyw{Dempster's rule of combination,Dempster-Shafer theory,Density
  functional theory,Error analysis,H infinity control,Medical services,Nearest
  neighbor searches,Neural networks,Voting,ambiguity,class membership,distance
  rejection,distance-weighted k-NN procedures,evidence,imperfect
  knowledge,inference mechanisms,k-nearest neighbor classification rule,pattern
  classification,statistical analysis,unseen pattern classification,voting}
  \strng{namehash}{DT1}
  \strng{fullhash}{DT1}
  \field{labelalpha}{Den95}
  \field{sortinit}{D}
  \field{abstract}{%
  In this paper, the problem of classifying an unseen pattern on the basis of
  its nearest neighbors in a recorded data set is addressed from the point of
  view of Dempster-Shafer theory. Each neighbor of a sample to be classified is
  considered as an item of evidence that supports certain hypotheses regarding
  the class membership of that pattern. The degree of support is defined as a
  function of the distance between the two vectors. The evidence of the k
  nearest neighbors is then pooled by means of Dempster's rule of combination.
  This approach provides a global treatment of such issues as ambiguity and
  distance rejection, and imperfect knowledge regarding the class membership of
  training patterns. The effectiveness of this classification scheme as
  compared to the voting and distance-weighted k-NN procedures is demonstrated
  using several sets of simulated and real-world data%
  }
  \verb{doi}
  \verb 10.1109/21.376493
  \endverb
  \field{issn}{00189472}
  \field{number}{5}
  \field{pages}{804\bibrangedash 813}
  \field{title}{{A k-nearest neighbor classification rule based on
  Dempster-Shafer theory}}
  \verb{url}
  \verb http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=376493
  \endverb
  \field{volume}{25}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/Desktop/smc95.pdf:pdf
  \endverb
  \field{journaltitle}{IEEE Transactions on Systems, Man, and Cybernetics}
  \field{year}{1995}
\endentry

\entry{Duda1973}{book}{}
  \name{author}{2}{}{%
    {{}%
     {Duda}{D.}%
     {R.}{R.}%
     {}{}%
     {}{}}%
    {{}%
     {Hart}{H.}%
     {P.}{P.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{DRHP1}
  \strng{fullhash}{DRHP1}
  \field{labelalpha}{DH73}
  \field{sortinit}{D}
  \field{abstract}{%
  Classic book on pattern recognition. Interesting points: 1) p. 66, and p.
  114: Mentions the problems with dimensionality curse. 2) p. 243-246: Mentions
  Multidimensional scaling (MDS), Karhunen-Loeve and dimensionality reduction.
  Also, has the spiral data-set as a sample. 3) p. 333: mentions
  SVD/eigenvalues for linear fitting.%
  }
  \field{booktitle}{Leonardo}
  \verb{doi}
  \verb 10.2307/1573081
  \endverb
  \field{isbn}{0471223611}
  \field{issn}{0024094X}
  \field{pages}{482}
  \field{title}{{Pattern Classification and Scene Analysis}}
  \verb{url}
  \verb http://www.jstor.org/stable/1573081?origin=crossref
  \endverb
  \field{volume}{7}
  \field{year}{1973}
\endentry

\entry{Dietterich1995}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Dietterich}{D.}%
     {T.}{T.}%
     {}{}%
     {}{}}%
    {{}%
     {Hild}{H.}%
     {H.}{H.}%
     {}{}%
     {}{}}%
    {{}%
     {Bakiri}{B.}%
     {G.}{G.}%
     {}{}%
     {}{}}%
  }
  \keyw{ID3,backpropagation,experimental comparisons,text-to-speech}
  \strng{namehash}{DTHHBG1}
  \strng{fullhash}{DTHHBG1}
  \field{labelalpha}{DHB95}
  \field{sortinit}{D}
  \field{abstract}{%
  The performance of the error backpropagation (BP) and ID3 learning algorithms
  was compared on the task of mapping English text to phonemes and stresses.
  Under the distributed output code developed by Sejnowski and Rosenberg, it is
  shown that BP consistently out-performs ID3 on this task by several
  percentage points. Three hypotheses explaining this difference were explored:
  (a) ID3 is overfitting the training data, (b) BP is able to share hidden
  units across several output units and hence can learn the output units
  better, and (c) BP captures statistical information that ID3 does not. We
  conclude that only hypothesis (c) is correct. By augmenting ID3 with a simple
  statistical learning procedure, the performance of BP can be closely matched.
  More complex statistical procedures can improve the performance of both BP
  and ID3 substantially in this domain.%
  }
  \verb{doi}
  \verb 10.1007/BF00993821
  \endverb
  \field{isbn}{0885-6125}
  \field{issn}{08856125}
  \field{number}{1}
  \field{pages}{51\bibrangedash 80}
  \field{title}{{A comparison of ID3 and backpropagation for English
  text-to-speech mapping}}
  \field{volume}{18}
  \field{journaltitle}{Machine Learning}
  \field{year}{1995}
\endentry

\entry{Dietterich1997}{article}{}
  \name{author}{1}{}{%
    {{}%
     {Dietterich}{D.}%
     {T.}{T.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{DT2}
  \strng{fullhash}{DT2}
  \field{labelalpha}{Die98}
  \field{sortinit}{D}
  \field{pages}{1895\bibrangedash 1923}
  \field{title}{{Approximate Statistical Tests for Comparing Supervised
  Classification Learning Algorithms}}
  \field{volume}{10}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley D
  \verb esktop/Downloaded/Dietterich - 1997 - Approximate Statistical Tests for
  \verb  Comparing Supervised Classification Learning Algorithms.pdf:pdf
  \endverb
  \field{journaltitle}{Neural Computation}
  \field{year}{1998}
\endentry

\entry{Ding2008}{article}{}
  \name{author}{5}{}{%
    {{}%
     {Ding}{D.}%
     {H.}{H.}%
     {}{}%
     {}{}}%
    {{}%
     {Trajcevski}{T.}%
     {G.}{G.}%
     {}{}%
     {}{}}%
    {{}%
     {Scheuermann}{S.}%
     {P.}{P.}%
     {}{}%
     {}{}}%
    {{}%
     {Wang}{W.}%
     {X.}{X.}%
     {}{}%
     {}{}}%
    {{}%
     {Keogh}{K.}%
     {E.}{E.}%
     {}{}%
     {}{}}%
  }
  \list{publisher}{1}{%
    {VLDB Endowment}%
  }
  \strng{namehash}{DH+1}
  \strng{fullhash}{DHTGSPWXKE1}
  \field{labelalpha}{Din+08}
  \field{sortinit}{D}
  \field{abstract}{%
  The last decade has witnessed a tremendous growths of interests in
  applications that deal with querying and mining of time series data. Numerous
  representation methods for dimensionality reduction and similarity measures
  geared towards time series have been introduced. Each individual work
  introducing a particular method has made specific claims and, aside from the
  occasional theoretical justifications, provided quantitative experimental
  observations. However, for the most part, the comparative aspects of these
  experiments were too narrowly focused on demonstrating the benefits of the
  proposed methods over some of the previously introduced ones. In order to
  provide a comprehensive validation, we conducted an extensive set of time
  series experiments re-implementing 8 different representation methods and 9
  similarity measures and their variants, and testing their effectiveness on 38
  time series data sets from a wide variety of application domains. In this
  paper, we give an overview of these different techniques and present our
  comparative experimental findings regarding their effectiveness. Our
  experiments have provided both a unified validation of some of the existing
  achievements, and in some cases, suggested that certain claims in the
  literature may be unduly optimistic. 1.%
  }
  \verb{doi}
  \verb 10.1145/1454159.1454226
  \endverb
  \verb{eprint}
  \verb 1012.2789v1
  \endverb
  \field{isbn}{0000000000000}
  \field{issn}{2150-8097}
  \field{number}{2}
  \field{pages}{1542\bibrangedash 1552}
  \field{title}{{Querying and Mining of Time Series Data : Experimental
  Comparison of Representations and Distance Measures}}
  \verb{url}
  \verb http://dl.acm.org/citation.cfm?id=1454226
  \endverb
  \field{volume}{1}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley D
  \verb esktop/Downloaded/Ding, Trajcevski, Scheuermann - 2008 - Querying and M
  \verb ining of Time Series Data Experimental Comparison of Representations an
  \verb d Distance.pdf:pdf
  \endverb
  \field{journaltitle}{Proceedings of the VLDB Endowment}
  \field{eprinttype}{arXiv}
  \field{year}{2008}
\endentry

\entry{Do2012}{article}{}
  \name{author}{4}{}{%
    {{}%
     {Do}{D.}%
     {H.}{H.}%
     {}{}%
     {}{}}%
    {{}%
     {Kalousis}{K.}%
     {A.}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Wang}{W.}%
     {J.}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Woznica}{W.}%
     {A.}{A.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{DH+2}
  \strng{fullhash}{DHKAWJWA1}
  \field{labelalpha}{Do+12}
  \field{sortinit}{D}
  \verb{eprint}
  \verb arXiv:1201.4714v1
  \endverb
  \field{pages}{308\bibrangedash 317}
  \field{title}{{A metric learning perspective of SVM: on the relation of LMNN
  and SVM}}
  \verb{url}
  \verb http://cui.unige.ch/{~}wangjun/papers/svm{\_}lmnn{\_}aistats12.pdf
  \endverb
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/Desktop/ML and SVM.pdf:pdf
  \endverb
  \field{journaltitle}{Proceedings of the 15th International Con- ference on
  Artificial Intelligence and Statistics (AISTAS '12)}
  \field{eprinttype}{arXiv}
  \field{year}{2012}
\endentry

\entry{Dreyfus2006}{book}{}
  \name{author}{6}{}{%
    {{}%
     {Dreyfus}{D.}%
     {G.}{G.}%
     {}{}%
     {}{}}%
    {{}%
     {Martinez}{M.}%
     {J.-M.}{J.-M.}%
     {}{}%
     {}{}}%
    {{}%
     {Samuelides}{S.}%
     {M.}{M.}%
     {}{}%
     {}{}}%
    {{}%
     {Gordon}{G.}%
     {M.~B.}{M.~B.}%
     {}{}%
     {}{}}%
    {{}%
     {Badran}{B.}%
     {F.}{F.}%
     {}{}%
     {}{}}%
    {{}%
     {Thiria}{T.}%
     {S.}{S.}%
     {}{}%
     {}{}}%
  }
  \keyw{Bio-ing{\'{e}}nierie,Machine {\`{a}} Vecteurs
  Supports,Pr{\'{e}}vision,Reconaissance de formes,Robotique et commande de
  processus,R{\'{e}}seaux de neurones,cartes topologiques,data mining}
  \strng{namehash}{DG+1}
  \strng{fullhash}{DGMJMSMGMBBFTS1}
  \field{labelalpha}{Dre+06}
  \field{sortinit}{D}
  \field{abstract}{%
  En une vingtaine d’ann{\'{e}}es, l’apprentissage artificiel est devenu
  une branche majeure des math{\'{e}}matiques appliqu{\'{e}}es, {\`{a}}
  l’intersection des statistiques et de l’intelligence artificielle. Son
  objectif est de r{\'{e}}aliser des mod{\`{e}}les qui apprennent « par
  l’exemple » : il s’appuie sur des donn{\'{e}}es num{\'{e}}riques
  (r{\'{e}}sultats de mesures ou de simulations), contrairement aux
  mod{\`{e}}les « de connaissances » qui s’appuient sur des {\'{e}}quations
  issues des premiers principes de la physique, de la chimie, de la biologie,
  de l’{\'{e}}conomie, etc. L’apprentis- sage statistique est d’une
  grande utilit{\'{e}} lorsque l’on cherche {\`{a}} mod{\'{e}}liser des
  processus complexes, souvent non lin{\'{e}}aires, pour lesquels les
  connaissances th{\'{e}}oriques sont trop impr{\'{e}}cises pour permettre des
  pr{\'{e}}dictions pr{\'{e}}cises. Ses domaines d’applications sont
  multiples : fouille de donn{\'{e}}es, bio-informatique, g{\'{e}}nie des
  proc{\'{e}}d{\'{e}}s, aide au diagnostic m{\'{e}}dical,
  t{\'{e}}l{\'{e}}communications, interface cerveau-machines, et bien
  d’autres. Cet ouvrage refl{\`{e}}te en partie l’{\'{e}}volution de cette
  discipline, depuis ses balbutiements au d{\'{e}}but des ann{\'{e}}es 1980,
  jusqu’{\`{a}} sa situation actuelle ; il n’a pas du tout la
  pr{\'{e}}tention de faire un point, m{\^{e}}me partiel, sur l’ensemble des
  d{\'{e}}veloppements pass{\'{e}}s et actuels, mais plut{\^{o}}t d’insister
  sur les principes et sur les m{\'{e}}thodes {\'{e}}prouv{\'{e}}s, dont les
  bases scientifiques sont s{\^{u}}res. Dans un domaine sans cesse parcouru de
  modes multiples et {\'{e}}ph{\'{e}}m{\`{e}}res, il est utile, pour qui
  cherche {\`{a}} acqu{\'{e}}rir les connaissances et principes de base,
  d’insister sur les aspects p{\'{e}}rennes du domaine. Cet ouvrage fait
  suite {\`{a}} R{\'{e}}seaux de neurones, m{\'{e}}thodologies et applications,
  des m{\^{e}}mes auteurs, paru en 2000, r{\'{e}}{\'{e}}dit{\'{e}} en 2004,
  chez le m{\^{e}}me {\'{e}}diteur, puis publi{\'{e}} en traduction anglaise
  chez Springer. Consacr{\'{e}} essentiellement aux r{\'{e}}seaux de neurones
  et aux cartes auto-adaptatives, il a largement contribu{\'{e}} {\`{a}}
  populariser ces techniques et {\`{a}} convaincre leurs utilisateurs qu’il
  est possible d’obtenir des r{\'{e}}sultats remarquables, {\`{a}} condition
  de mettre en {\oe}uvre une m{\'{e}}thodologie de conception rigoureuse,
  scientifique- ment fond{\'{e}}e, dans un domaine o{\`{u}} l’empirisme a
  longtemps tenu lieu de m{\'{e}}thode. Tout en restant fid{\`{e}}le {\`{a}}
  l’esprit de cet ouvrage, combinant fondements math{\'{e}}matiques et
  m{\'{e}}thodologie de mise en {\oe}uvre, les auteurs ont {\'{e}}largi le
  champ de la pr{\'{e}}sentation, afin de permettre au lecteur d’aborder
  d’autres m{\'{e}}thodes d’apprentissage statistique que celles qui sont
  directement d{\'{e}}crites dans cet ouvrage. En effet, les succ{\`{e}}s de
  l’apprentissage dans un grand nombre de domaines ont pouss{\'{e}} au
  d{\'{e}}veloppement de tr{\`{e}}s nombreuses variantes, souvent
  destin{\'{e}}es {\`{a}} r{\'{e}}pondre efficacement aux exigences de telle ou
  telle classe d’applications. Toutes ces variantes ont n{\'{e}}anmoins des
  bases th{\'{e}}oriques et des aspects m{\'{e}}thodolo- giques communs,
  qu’il est important d’avoir pr{\'{e}}sents {\`{a}} l’esprit. Le terme
  d’apprentissage, comme celui de r{\'{e}}seau de neurones, {\'{e}}voque
  {\'{e}}videmment le fonctionnement du cerveau. Il ne faut pourtant pas
  s’attendre {\`{a}} trouver ici d’explications sur les m{\'{e}}canismes de
  traitement des informations dans les syst{\`{e}}mes nerveux ; ces derniers
  sont d’une grande complexit{\'{e}}, r{\'{e}}sultant de processus
  {\'{e}}lectriques et chimiques subtils, encore mal compris en d{\'{e}}pit de
  la grande quantit{\'{e}} de donn{\'{e}}es exp{\'{e}}rimentales disponibles.
  Si les m{\'{e}}thodes d’apprentissage statistique peuvent {\^{e}}tre
  d’une grande utilit{\'{e}} pour cr{\'{e}}er des mod{\`{e}}les empiriques de
  telle ou telle fonction r{\'{e}}alis{\'{e}}e par le syst{\`{e}}me nerveux,
  celles qui sont d{\'{e}}crites dans cet ouvrage n’ont aucunement la
  pr{\'{e}}tention d’imiter, m{\^{e}}me vaguement, le fonctionne- ment du
  cerveau. L’apprentissage artificiel, notamment statistique, permettra-t-il
  un jour de donner aux ordinateurs des capacit{\'{e}}s analogues {\`{a}}
  celles des {\^{e}}tres humains ? Se rapprochera-t-on de cet objectif en
  perfectionnant les techniques actuelles d’apprentissage, ou bien des
  approches radicalement nouvelles sont-elles indispensables ? Faut-il
  s’inspirer de ce que l’on sait, ou croit savoir, sur le fonctionnement du
  cerveau ? Ces questions font l’objet de d{\'{e}}bats passionn{\'{e}}s, et
  passionnants, au sein de la communaut{\'{e}} scientifique : on n’en
  trouvera pas les r{\'{e}}ponses ici.%
  }
  \field{edition}{Eyrolles}
  \field{isbn}{9782212114645}
  \field{pages}{471}
  \field{title}{{Apprentissage Apprentissage statistique}}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley D
  \verb esktop/Downloaded/G. Dreyfus, J.-M. Martinez, M. Samuelides M. B. Gordo
  \verb n, F. Badran - 2006 - Apprentissage Apprentissage statistique.pdf:pdf
  \endverb
  \field{year}{2006}
\endentry

\entry{Dudani1976}{article}{}
  \name{author}{1}{}{%
    {{}%
     {Dudani}{D.}%
     {S.}{S.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{DS1}
  \strng{fullhash}{DS1}
  \field{labelalpha}{Dud76}
  \field{sortinit}{D}
  \field{abstract}{%
  Among the simplest and most intuitively appealing classes of nonprobabilistic
  classification procedures are those that weight the evidence of nearby sample
  observations most heavily. More specifically, one might wish to weight the
  evidence of a neighbor close to an unclassified observation more heavily than
  the evidence of another neighbor which is at a greater distance from the
  unclassified observation. One such classification rule is described which
  makes use of a neighbor weighting function for the purpose of assigning a
  class to an unclassified sample. The admissibility of such a rule is also
  considered.%
  }
  \verb{doi}
  \verb 10.1109/TSMC.1976.5408784
  \endverb
  \field{isbn}{0018-9472}
  \field{issn}{00189472}
  \field{number}{4}
  \field{pages}{325\bibrangedash 327}
  \field{title}{{Distance weighed k-Nearest Neighbor rule}}
  \field{volume}{SMC-6}
  \field{journaltitle}{IEEE Transactions on Systems, Man and Cybernetics}
  \field{year}{1976}
\endentry

\entry{Fan2008}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Fan}{F.}%
     {R.E.}{R.}%
     {}{}%
     {}{}}%
    {{}%
     {Chang}{C.}%
     {K.W.}{K.}%
     {}{}%
     {}{}}%
    {{}%
     {Hsieh}{H.}%
     {C.J.}{C.}%
     {}{}%
     {}{}}%
  }
  \keyw{large-scale linear classification,logistic regression,machine
  learning,open,source,support vector machines}
  \strng{namehash}{FRCKHC1}
  \strng{fullhash}{FRCKHC1}
  \field{labelalpha}{FCH08}
  \field{sortinit}{F}
  \field{abstract}{%
  LIBLINEAR is an open source library for large-scale linear classification. It
  supports logistic regression and linear support vector machines. We provide
  easy-to-use command-line tools and library calls for users and developers.
  Comprehensive documents are available for both beginners and advanced users.
  Experiments demonstrate that LIBLINEAR is very efficient on large sparse data
  sets.%
  }
  \verb{doi}
  \verb 10.1038/oby.2011.351
  \endverb
  \field{isbn}{089791497X}
  \field{issn}{15324435}
  \field{title}{{LIBLINEAR: A library for large linear classification}}
  \verb{url}
  \verb http://dl.acm.org/citation.cfm?id=1442794
  \endverb
  \field{journaltitle}{The Journal of Machine Learning}
  \field{year}{2008}
\endentry

\entry{Frambourg2013a}{inproceedings}{}
  \name{author}{3}{}{%
    {{}%
     {Frambourg}{F.}%
     {C.}{C.}%
     {}{}%
     {}{}}%
    {{}%
     {Douzal-Chouakria}{D.-C.}%
     {A.}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Gaussier}{G.}%
     {E.}{E.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{FCDCAGE1}
  \strng{fullhash}{FCDCAGE1}
  \field{labelalpha}{FDCG13}
  \field{sortinit}{F}
  \field{booktitle}{Intelligent Data Analysis}
  \field{pages}{198\bibrangedash 209}
  \field{title}{{Learning multiple temporal matching for time series
  classification}}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/Desktop/22488{\_}FRAMBOURG{\_}2013{\_}
  \verb archivage.pdf:pdf
  \endverb
  \field{year}{2013}
\endentry

\entry{Fu2011b}{misc}{}
  \name{author}{1}{}{%
    {{}%
     {Fu}{F.}%
     {T.}{T.}%
     {}{}%
     {}{}}%
  }
  \keyw{Representation,Segmentation,Similarity measure,Time series data
  mining,Visualization}
  \strng{namehash}{FT1}
  \strng{fullhash}{FT1}
  \field{labelalpha}{Fu11}
  \field{sortinit}{F}
  \field{abstract}{%
  Time series is an important class of temporal data objects and it can be
  easily obtained from scientific and financial applications. A time series is
  a collection of observations made chronologically. The nature of time series
  data includes: large in data size, high dimensionality and necessary to
  update continuously. Moreover time series data, which is characterized by its
  numerical and continuous nature, is always considered as a whole instead of
  individual numerical field. The increasing use of time series data has
  initiated a great deal of research and development attempts in the field of
  data mining. The abundant research on time series data mining in the last
  decade could hamper the entry of interested researchers, due to its
  complexity. In this paper, a comprehensive revision on the existing time
  series data mining research is given. They are generally categorized into
  representation and indexing, similarity measure, segmentation, visualization
  and mining. Moreover state-of-the-art research issues are also highlighted.
  The primary objective of this paper is to serve as a glossary for interested
  researchers to have an overall picture on the current time series data mining
  development and identify their potential research direction to further
  investigation. © 2010 Elsevier Ltd. All rights reserved.%
  }
  \field{booktitle}{Engineering Applications of Artificial Intelligence}
  \verb{doi}
  \verb 10.1016/j.engappai.2010.09.007
  \endverb
  \field{isbn}{2472061331}
  \field{issn}{09521976}
  \field{number}{1}
  \field{pages}{164\bibrangedash 181}
  \field{title}{{A review on time series data mining}}
  \field{volume}{24}
  \field{year}{2011}
\endentry

\entry{Geng2005}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Geng}{G.}%
     {X.}{X.}%
     {}{}%
     {}{}}%
    {{}%
     {Zhan}{Z.}%
     {D.}{D.}%
     {}{}%
     {}{}}%
    {{}%
     {Zhou}{Z.}%
     {Z.}{Z.}%
     {}{}%
     {}{}}%
  }
  \keyw{Classification,Dimensionality reduction,Manifold learning,Supervised
  learning,Visualization}
  \strng{namehash}{GXZDZZ1}
  \strng{fullhash}{GXZDZZ1}
  \field{labelalpha}{GZZ05}
  \field{sortinit}{G}
  \field{abstract}{%
  When performing visualization and classification, people often confront the
  problem of dimensionality reduction. Isomap is one of the most promising
  nonlinear dimensionality reduction techniques. However, when Isomap is
  applied to real-world data, it shows some limitations, such as being
  sensitive to noise. In this paper, an improved version of Isomap, namely
  S-Isomap, is proposed. S-Isomap utilizes class information to guide the
  procedure of nonlinear dimensionality reduction. Such a kind of procedure is
  called supervised nonlinear dimensionality reduction. In S-Isomap, the
  neighborhood graph of the input data is constructed according to a certain
  kind of dissimilarity between data points, which is specially designed to
  integrate the class information. The dissimilarity has several good
  properties which help to discover the true neighborhood of the data and,
  thus, makes S-Isomap a robust technique for both visualization and
  classification, especially for real-world problems. In the visualization
  experiments, S-Isomap is compared with Isomap, LLE, and WeightedIso. The
  results show that S-Isomap performs the best. In the classification
  experiments, S-Isomap is used as a preprocess of classification and compared
  with Isomap, WeightedIso, as well as some other well-established
  classification methods, including the K-nearest neighbor classifier, BP
  neural network, J4.8 decision tree, and SVM. The results reveal that S-Isomap
  excels compared to Isomap and WeightedIso in classification, and it is highly
  competitive with those well-known classification methods.%
  }
  \verb{doi}
  \verb 10.1109/TSMCB.2005.850151
  \endverb
  \field{isbn}{1083-4419}
  \field{issn}{10834419}
  \field{number}{6}
  \field{pages}{1098\bibrangedash 1107}
  \field{title}{{Supervised nonlinear dimensionality reduction for
  visualization and classification}}
  \field{volume}{35}
  \field{journaltitle}{IEEE Transactions on Systems, Man, and Cybernetics, Part
  B: Cybernetics}
  \field{year}{2005}
\endentry

\entry{Hsu2008}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Hsu}{H.}%
     {C.}{C.}%
     {}{}%
     {}{}}%
    {{}%
     {Chang}{C.}%
     {C.}{C.}%
     {}{}%
     {}{}}%
    {{}%
     {Lin}{L.}%
     {C.}{C.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{HCCCLC1}
  \strng{fullhash}{HCCCLC1}
  \field{labelalpha}{HCL08}
  \field{sortinit}{H}
  \field{abstract}{%
  The support vector machine (SVM) is a popular classi cation technique.
  However, beginners who are not familiar with SVM often get unsatisfactory
  results since they miss some easy but signi cant steps. In this guide, we
  propose a simple procedure which usually gives reasonable results. developed
  well-differentiated superficial transitional cell bladder cancer.
  CONCLUSIONS: Patients with SCI often prefer SPC than other methods offered to
  them, because of quality-of-life issues. The incidence of significant
  complications might not be as high as previously reported, and with a
  commitment to careful follow-up, SPC can be a safe option for carefully
  selected patients if adequate surveillance can be ensured.%
  }
  \verb{doi}
  \verb 10.1177/02632760022050997
  \endverb
  \verb{eprint}
  \verb 0-387-31073-8
  \endverb
  \field{isbn}{013805326X}
  \field{issn}{1464-410X}
  \field{number}{1}
  \field{pages}{1396\bibrangedash 400}
  \field{title}{{A Practical Guide to Support Vector Classification}}
  \verb{url}
  \verb http://www.csie.ntu.edu.tw/{~}cjlin/papers/guide/guide.pdf
  \endverb
  \field{volume}{101}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/Google Drive/Th{\`{e}}se/Bibliographie
  \verb /SVM Librairie/A Practical Guide to Support Vector Classification.pdf:p
  \verb df
  \endverb
  \field{journaltitle}{BJU international}
  \field{eprinttype}{arXiv}
  \field{year}{2008}
\endentry

\entry{Hwang2012}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Hwang}{H.}%
     {S.}{S.}%
     {}{}%
     {}{}}%
    {{}%
     {Ham}{H.}%
     {D.}{D.}%
     {}{}%
     {}{}}%
    {{}%
     {Kim}{K.}%
     {J.}{J.}%
     {}{}%
     {}{}}%
  }
  \keyw{forecasting,forecasting performance,support vector machine}
  \strng{namehash}{HSHDKJ1}
  \strng{fullhash}{HSHDKJ1}
  \field{labelalpha}{HHK12}
  \field{sortinit}{H}
  \verb{doi}
  \verb 10.1007/s12205-012-1519-3
  \endverb
  \field{issn}{1226-7988}
  \field{number}{5}
  \field{pages}{870\bibrangedash 882}
  \field{title}{{Forecasting performance of LS-SVM for nonlinear hydrological
  time series}}
  \verb{url}
  \verb http://link.springer.com/10.1007/s12205-012-1519-3
  \endverb
  \field{volume}{16}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/Google Drive/Th{\`{e}}se/Bibliographie
  \verb /SVM + Time Series/Hwang-KSCE Journal of Civil Engineering-2012{\_}Fore
  \verb casting performance of LS-SVM for nonlinear hydrological time series.pd
  \verb f:pdf
  \endverb
  \field{journaltitle}{KSCE Journal of Civil Engineering}
  \field{year}{2012}
\endentry

\entry{Heisele2001}{inproceedings}{}
  \name{author}{3}{}{%
    {{}%
     {Heisele}{H.}%
     {B.}{B.}%
     {}{}%
     {}{}}%
    {{}%
     {Ho}{H.}%
     {P.}{P.}%
     {}{}%
     {}{}}%
    {{}%
     {Poggio}{P.}%
     {T.}{T.}%
     {}{}%
     {}{}}%
  }
  \keyw{Active shape model,Biology computing,Face recognition,Image
  databases,Image recognition,Mouth,Robustness,SVM classifier,Solid
  modeling,Support vector machine classification,Support vector
  machines,clustering,component-based approach,facial components,feature
  extraction,feature vector,global methods,learning automata}
  \strng{namehash}{HBHPPT1}
  \strng{fullhash}{HBHPPT1}
  \field{labelalpha}{HHP01}
  \field{sortinit}{H}
  \field{abstract}{%
  We present a component-based method and two global methods for face
  recognition and evaluate them with respect to robustness against pose
  changes. In the component system we first locate facial components, extract
  them and combine them into a single feature vector which is classified by a
  Support Vector Machine (SVM). The two global systems recognize faces by
  classifying a single feature vector consisting of the gray values of the
  whole face image. In the first global system we trained a single SVM
  classifier for each person in the database. The second system consists of
  sets of viewpoint-specific SVM classifiers and involves clustering during
  training. We performed extensive tests on a database which included faces
  rotated up to about 40° in depth. The component system clearly outperformed
  both global systems on all tests.%
  }
  \field{booktitle}{IEEE International Conference on Computer Vision, ICCV}
  \verb{doi}
  \verb 10.1109/ICCV.2001.937693
  \endverb
  \field{isbn}{0-7695-1143-0}
  \field{issn}{1089-7801}
  \field{number}{July}
  \field{pages}{688\bibrangedash 694}
  \field{title}{{Face recognition with support vector machines: global versus
  component-based approach}}
  \verb{url}
  \verb http://dx.doi.org/10.1109/ICCV.2001.937693
  \endverb
  \field{volume}{2}
  \field{year}{2001}
\endentry

\entry{Hu2013}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Hu}{H.}%
     {J.}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Wang}{W.}%
     {J.}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Zeng}{Z.}%
     {G.}{G.}%
     {}{}%
     {}{}}%
  }
  \list{publisher}{1}{%
    {Elsevier Ltd}%
  }
  \keyw{Ensemble Empirical Mode Decomposition (EEMD),Support Vector Machine
  (SVM),Wind farm,Wind speed forecasting}
  \strng{namehash}{HJWJZG1}
  \strng{fullhash}{HJWJZG1}
  \field{labelalpha}{HWZ13}
  \field{sortinit}{H}
  \field{abstract}{%
  In this paper, a hybrid forecasting approach, which combines the Ensemble
  Empirical Mode Decomposition (EEMD) and the Support Vector Machine (SVM), is
  proposed to improve the quality of wind speed forecasting. The essence of the
  methodology incorporates three phases. First, the original data of wind speed
  are decomposed into a number of independent Intrinsic Mode Functions (IMFs)
  and one residual series by EEMD using the principle of decomposition. In
  order to forecast these IMFs, excepting the highest frequency acquired by
  EEMD, the respective estimates are yielded using the SVM algorithm. Finally,
  these respective estimates are combined into the final wind speed forecasts
  using the principle of ensemble. The proposed hybrid method is examined by
  forecasting the mean monthly wind speed of three wind farms located in
  northwest China. The obtained results confirm an observable improvement for
  the forecasting validity of the proposed hybrid approach. This tool shows
  great promise for the forecasting of intricate time series which are
  intrinsically highly volatile and irregular. ?? 2013 Elsevier Ltd.%
  }
  \verb{doi}
  \verb 10.1016/j.renene.2013.05.012
  \endverb
  \field{isbn}{0960-1481}
  \field{issn}{09601481}
  \field{pages}{185\bibrangedash 194}
  \field{title}{{A hybrid forecasting approach applied to wind speed time
  series}}
  \verb{url}
  \verb http://dx.doi.org/10.1016/j.renene.2013.05.012
  \endverb
  \field{volume}{60}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/Google Drive/Th{\`{e}}se/Bibliographie
  \verb /SVM + Time Series/Hu-Renewable Energy-2013{\_}A hybrid forecasting app
  \verb roach applied to wind speed time series.pdf:pdf
  \endverb
  \field{journaltitle}{Renewable Energy}
  \field{year}{2013}
\endentry

\entry{Jeong2011}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Jeong}{J.}%
     {Y.}{Y.}%
     {}{}%
     {}{}}%
    {{}%
     {Jeong}{J.}%
     {M.K.}{M.}%
     {}{}%
     {}{}}%
    {{}%
     {Omitaomu}{O.}%
     {O.A.}{O.}%
     {}{}%
     {}{}}%
  }
  \keyw{Adaptive weights,Dynamic time warping,Modified logistic weight
  function,Time series classification,Time series clustering,Weighted dynamic
  time warping}
  \strng{namehash}{JYJMOO1}
  \strng{fullhash}{JYJMOO1}
  \field{labelalpha}{JJO11}
  \field{sortinit}{J}
  \field{abstract}{%
  Dynamic time warping (DTW), which finds the minimum path by providing
  non-linear alignments between two time series, has been widely used as a
  distance measure for time series classification and clustering. However, DTW
  does not account for the relative importance regarding the phase difference
  between a reference point and a testing point. This may lead to
  misclassification especially in applications where the shape similarity
  between two sequences is a major consideration for an accurate recognition.
  Therefore, we propose a novel distance measure, called a weighted DTW (WDTW),
  which is a penalty-based DTW. Our approach penalizes points with higher phase
  difference between a reference point and a testing point in order to prevent
  minimum distance distortion caused by outliers. The rationale underlying the
  proposed distance measure is demonstrated with some illustrative examples. A
  new weight function, called the modified logistic weight function (MLWF), is
  also proposed to systematically assign weights as a function of the phase
  difference between a reference point and a testing point. By applying
  different weights to adjacent points, the proposed algorithm can enhance the
  detection of similarity between two time series. We show that some popular
  distance measures such as DTW and Euclidean distance are special cases of our
  proposed WDTW measure. We extend the proposed idea to other variants of DTW
  such as derivative dynamic time warping (DDTW) and propose the weighted
  version of DDTW. We have compared the performances of our proposed procedures
  with other popular approaches using public data sets available through the
  UCR Time Series Data Mining Archive for both time series classification and
  clustering problems. The experimental results indicate that the proposed
  approaches can achieve improved accuracy for time series classification and
  clustering problems.%
  }
  \verb{doi}
  \verb 10.1016/j.patcog.2010.09.022
  \endverb
  \field{isbn}{0031-3203}
  \field{issn}{00313203}
  \field{number}{9}
  \field{pages}{2231\bibrangedash 2240}
  \field{title}{{Weighted dynamic time warping for time series classification}}
  \verb{url}
  \verb http://www.sciencedirect.com/science/article/pii/S003132031000484X
  \endverb
  \field{volume}{44}
  \field{journaltitle}{Pattern Recognition}
  \field{year}{2011}
\endentry

\entry{Jain1999}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Jain}{J.}%
     {A.K.}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Murty}{M.}%
     {M.N.}{M.}%
     {}{}%
     {}{}}%
    {{}%
     {Flynn}{F.}%
     {P.J.}{P.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{JAMMFP1}
  \strng{fullhash}{JAMMFP1}
  \field{labelalpha}{JMF99}
  \field{sortinit}{J}
  \field{abstract}{%
  Clustering is the unsupervised classification of patterns (observations, data
  items, or feature vectors) into groups (clusters). The clustering problem has
  been addressed in many contexts and by researchers in many disciplines; this
  reflects its broad appeal and usefulness as one of the steps in exploratory
  data analysis. However, clustering is a difficult problem combinatorially,
  and differences in assumptions and contexts in different communities has made
  the transfer of useful generic concepts and methodologies slow to occur. This
  paper presents an overview of pattern clustering methods from a statistical
  pattern recognition perspective, with a goal of providing useful advice and
  references to fundamental concepts accessible to the broad community of
  clustering practitioners. We present a taxonomy of clustering techniques, and
  identify cross-cutting themes and recent advances. We also describe some
  important applications of clustering algorithms such as image segmentation,
  object recognition, and information retrieval.%
  }
  \verb{doi}
  \verb 10.1145/331499.331504
  \endverb
  \verb{eprint}
  \verb arXiv:1101.1881v2
  \endverb
  \field{isbn}{0360-0300}
  \field{issn}{03600300}
  \field{number}{3}
  \field{pages}{264\bibrangedash 323}
  \field{title}{{Data clustering: a review}}
  \verb{url}
  \verb http://portal.acm.org/citation.cfm?doid=331499.331504
  \endverb
  \field{volume}{31}
  \field{journaltitle}{ACM Computing Surveys}
  \field{eprinttype}{arXiv}
  \field{year}{1999}
\endentry

\entry{Kalman1960}{article}{}
  \name{author}{1}{}{%
    {{}%
     {Kalman}{K.}%
     {R.E.}{R.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{KR1}
  \strng{fullhash}{KR1}
  \field{labelalpha}{Kal60}
  \field{sortinit}{K}
  \field{abstract}{%
  The classical filtering and prediction problem is re-examined using the Bode-
  Shannon representation of random processes and the ``state
  transition{\&}apos;{\&}apos; method of analysis of dynamic systems. New
  results are: (1) The formulation and methods of solution of the problem apply
  without modifica- tion to stationary and nonstationary statistics and to
  growing-memory and infinite- memory filters. (2) A nonlinear difference (or
  differential) equation is derived for the covariance matrix of the optimal
  estimation error. From the solution of this equation the co- efficients of
  the difference (or differential) equation of the optimal linear filter are
  ob- tained without further calculations. (3) The filtering problem is shown
  to be the dual of the noise-free regulator problem. The new method developed
  here is applied to two well-known problems, confirming and extending earlier
  results. The discussion is largely self-contained and proceeds from first
  principles; basic concepts of the theory of random processes are reviewed in
  the Appendix.%
  }
  \verb{doi}
  \verb 10.1115/1.3662552
  \endverb
  \field{isbn}{9783540769897}
  \field{issn}{0021-9223}
  \field{number}{Series D}
  \field{pages}{35\bibrangedash 45}
  \field{title}{{A New Approach to Linear Filtering and Prediction Problems}}
  \field{volume}{82}
  \field{journaltitle}{Transactions of the ASME Journal of Basic Engineering}
  \field{year}{1960}
\endentry

\entry{Keogh2003a}{article}{}
  \name{author}{4}{}{%
    {{}%
     {Keogh}{K.}%
     {E.}{E.}%
     {}{}%
     {}{}}%
    {{}%
     {Chu}{C.}%
     {S.}{S.}%
     {}{}%
     {}{}}%
    {{}%
     {Hart}{H.}%
     {D.}{D.}%
     {}{}%
     {}{}}%
    {{}%
     {Pazzani}{P.}%
     {M.}{M.}%
     {}{}%
     {}{}}%
  }
  \keyw{data mining,piecewise linear approxi,time series}
  \strng{namehash}{KE+1}
  \strng{fullhash}{KECSHDPM1}
  \field{labelalpha}{Keo+03}
  \field{sortinit}{K}
  \field{abstract}{%
  In recent years, there has been an explosion of interest in mining time
  series databases. As with most computer science problems, representation of
  the data is the key to efficient and effective solutions. One of the most
  commonly used representations is piecewise linear approximation. This
  representation has been used by various researchers to support clustering,
  classification, indexing and association rule mining of time series data. A
  variety of algorithms have been proposed to obtain this representation, with
  several algorithms having been independently rediscovered several times. In
  this chapter, we undertake the first extensive review and empirical
  comparison of all proposed techniques. We show that all these algorithms have
  fatal flaws from a data mining perspective. We introduce a novel algorithm
  that we empirically show to be superior to all others in the literature.%
  }
  \verb{doi}
  \verb 10.1.1.12.9924
  \endverb
  \field{isbn}{9812382909}
  \field{pages}{1\bibrangedash 21}
  \field{title}{{Segmenting Time Series: A Survey and Novel Approach}}
  \field{journaltitle}{Data Mining in Time Series Databases}
  \field{year}{2003}
\endentry

\entry{Keogh2011}{misc}{}
  \name{author}{7}{}{%
    {{}%
     {Keogh}{K.}%
     {E.}{E.}%
     {}{}%
     {}{}}%
    {{}%
     {Zhu}{Z.}%
     {Q.}{Q.}%
     {}{}%
     {}{}}%
    {{}%
     {Hu}{H.}%
     {B.}{B.}%
     {}{}%
     {}{}}%
    {{}%
     {Hao}{H.}%
     {Y.}{Y.}%
     {}{}%
     {}{}}%
    {{}%
     {Xi}{X.}%
     {X.}{X.}%
     {}{}%
     {}{}}%
    {{}%
     {Wei}{W.}%
     {L.}{L.}%
     {}{}%
     {}{}}%
    {{}%
     {Ratanamahatana}{R.}%
     {C.A.}{C.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{KE+1}
  \strng{fullhash}{KEZQHBHYXXWLRC1}
  \field{labelalpha}{Keo+11}
  \field{sortinit}{K}
  \field{title}{{The UCR Time Series Classification/Clustering Homepage}}
  \verb{url}
  \verb www.cs.ucr.edu/{~}eamonn/time{\_}series{\_}data/
  \endverb
  \field{year}{2011}
\endentry

\entry{Keller1985}{misc}{}
  \name{author}{3}{}{%
    {{}%
     {Keller}{K.}%
     {J.M.}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Gray}{G.}%
     {M.R.}{M.}%
     {}{}%
     {}{}}%
    {{}%
     {Givens}{G.}%
     {J.A.}{J.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{KJGMGJ1}
  \strng{fullhash}{KJGMGJ1}
  \field{labelalpha}{KGG85}
  \field{sortinit}{K}
  \field{abstract}{%
  Classification of objects is an important area of research and application in
  a variety of fields. In the presence of full knowledge of the underlying
  probabilities, Bayes decision theory gives optimal error rates. In those
  cases where this information is not present, many algorithms make use of
  distance or similarity among samples as a means of classification. The
  K-nearest neighbor decision rule has often been used in these pattern
  recognition problems. One of the difficulties that arises when utilizing this
  technique is that each of the labeled samples is given equal importance in
  deciding the class memberships of the pattern to be classified, regardless of
  their `typicalness'. The theory of fuzzy sets is introduced into the
  K-nearest neighbor technique to develop a fuzzy version of the algorithm.
  Three methods of assigning fuzzy memberships to the labeled samples are
  proposed, and experimental results and comparisons to the crisp version are
  presented.%
  }
  \field{booktitle}{IEEE Transactions on Systems, Man, and Cybernetics}
  \verb{doi}
  \verb 10.1109/TSMC.1985.6313426
  \endverb
  \field{isbn}{0018-9472}
  \field{issn}{0018-9472}
  \field{number}{4}
  \field{pages}{580\bibrangedash 585}
  \field{title}{{A fuzzy K-nearest neighbor algorithm}}
  \field{volume}{SMC-15}
  \field{year}{1985}
\endentry

\entry{Kalpakis}{inproceedings}{}
  \name{author}{3}{}{%
    {{}%
     {Kalpakis}{K.}%
     {K.}{K.}%
     {}{}%
     {}{}}%
    {{}%
     {Gada}{G.}%
     {D.}{D.}%
     {}{}%
     {}{}}%
    {{}%
     {Puttagunta}{P.}%
     {V.}{V.}%
     {}{}%
     {}{}}%
  }
  \keyw{arima models,cepstral coefficients,clustering,series,similarity
  measures,time}
  \strng{namehash}{KKGDPV1}
  \strng{fullhash}{KKGDPV1}
  \field{labelalpha}{KGP01}
  \field{sortinit}{K}
  \field{booktitle}{Proceedings of the 2001 IEEE International Conference on
  Data Mining (ICDM'01)}
  \field{pages}{273\bibrangedash 280}
  \field{title}{{Distance Measures for Effective Clustering of ARIMA
  Time-Series}}
  \list{location}{1}{%
    {San Jose, CA}%
  }
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/Desktop/ICDM01.pdf:pdf
  \endverb
  \field{year}{2001}
\endentry

\entry{Keogh2004}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Keogh}{K.}%
     {E.}{E.}%
     {}{}%
     {}{}}%
    {{}%
     {Ratanamahatana}{R.}%
     {C.A.}{C.}%
     {}{}%
     {}{}}%
  }
  \keyw{dynamic time warping,indexing,lower bounding,time series}
  \strng{namehash}{KERC1}
  \strng{fullhash}{KERC1}
  \field{labelalpha}{KR04}
  \field{sortinit}{K}
  \field{abstract}{%
  The problem of indexing time series has attracted much interest. Most
  algorithms used to index time series utilize the Euclidean distance or some
  variation thereof. However, it has been forcefully shown that the Euclidean
  distance is a very brittle distance measure. Dy- namic time warping (DTW) is
  a much more robust distance measure for time series, allowing similar shapes
  to match even if they are out of phase in the time axis. Because of this
  flexi- bility, DTW is widely used in science, medicine, industry and finance.
  Unfortunately, however, DTW does not obey the triangular inequality and thus
  has resisted attempts at exact indexing. Instead, many researchers have
  introduced approximate indexing techniques or abandoned the idea of indexing
  and concentrated on speeding up sequential searches. In this work, we intro-
  duce a novel technique for the exact indexing of DTW. We prove that our
  method guarantees no false dismissals and we demonstrate its vast superiority
  over all competing approaches in the largest and most comprehensive set of
  time series indexing experiments ever undertaken.%
  }
  \verb{doi}
  \verb 10.1007/s10115-004-0154-9
  \endverb
  \field{issn}{0219-1377}
  \field{number}{3}
  \field{pages}{358\bibrangedash 386}
  \field{title}{{Exact indexing of dynamic time warping}}
  \verb{url}
  \verb http://www.springerlink.com/index/10.1007/s10115-004-0154-9
  \endverb
  \field{volume}{7}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley D
  \verb esktop/Downloaded/Keogh, Ratanamahatana - 2004 - Exact indexing of dyna
  \verb mic time warping.pdf:pdf
  \endverb
  \field{journaltitle}{Knowledge and Information Systems}
  \field{year}{2004}
  \warn{\item Invalid format of field 'month'}
\endentry

\entry{Kijsirikul2002}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Kijsirikul}{K.}%
     {B.}{B.}%
     {}{}%
     {}{}}%
    {{}%
     {Ussivakul}{U.}%
     {N.}{N.}%
     {}{}%
     {}{}}%
  }
  \keyw{decision directed acyclic graph,decision levels,directed
  graphs,learning (artificial intelligence),learning automata,linear support
  vector machines,multiclass support vector machines,pattern
  classification,probability adaptive directed acyclic graph}
  \strng{namehash}{KBUN1}
  \strng{fullhash}{KBUN1}
  \field{labelalpha}{KU02}
  \field{sortinit}{K}
  \field{abstract}{%
  Presents a method of extending support vector machines (SVMs) for dealing
  with multiclass problems. Motivated by the decision directed acyclic graph
  (DDAG), we propose the adaptive DAG (ADAG): a modified structure of the DDAG
  that has a lower number of decision levels and reduces the dependency on the
  sequence of nodes. Thus, the ADAG improves the accuracy of the DDAG while
  maintaining low computational requirement%
  }
  \verb{doi}
  \verb 10.1109/IJCNN.2002.1005608
  \endverb
  \field{pages}{980\bibrangedash 985}
  \field{title}{{Multiclass Support Vector Machines using Adaptive Directed
  Acyclic Graph}}
  \field{volume}{1}
  \field{journaltitle}{Neural Networks, 2002. IJCNN '02. Proceedings of the
  2002 International Joint Conference on}
  \field{year}{2002}
\endentry

\entry{Lee2009}{article}{}
  \name{author}{4}{}{%
    {{}%
     {Lee}{L.}%
     {H.}{H.}%
     {}{}%
     {}{}}%
    {{}%
     {Pham}{P.}%
     {P.}{P.}%
     {}{}%
     {}{}}%
    {{}%
     {Largman}{L.}%
     {Y.}{Y.}%
     {}{}%
     {}{}}%
    {{}%
     {Ng}{N.}%
     {A.}{A.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{LH+1}
  \strng{fullhash}{LHPPLYNA1}
  \field{labelalpha}{Lee+09}
  \field{sortinit}{L}
  \field{abstract}{%
  In recent years, deep learning approaches have gained significant interest as
  a way of building hierarchical representations from unlabeled data. However,
  to our knowledge, these deep learning approaches have not been extensively
  studied for auditory data. In this paper, we apply convolutional deep belief
  networks to audio data and empirically evaluate them on various audio
  classification tasks. In the case of speech data, we show that the learned
  features correspond to phones/phonemes. In addition, our feature
  representations learned from unlabeled audio data show very good performance
  for multiple audio classification tasks. We hope that this paper will inspire
  more research on deep learning approaches applied to a wide range of audio
  recognition tasks.%
  }
  \verb{doi}
  \verb 10.1145/1553374.1553453
  \endverb
  \field{isbn}{9781605585161}
  \field{issn}{02643294}
  \field{pages}{1\bibrangedash 9}
  \field{title}{{Unsupervised feature learning for audio classification using
  convolutional deep belief networks.}}
  \verb{url}
  \verb https://papers.nips.cc/paper/3674-unsupervised-feature-learning-for-aud
  \verb io-classification-using-convolutional-deep-belief-networks.pdf
  \endverb
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/Desktop/3674-unsupervised-feature-lear
  \verb ning-for-audio-classification-using-convolutional-deep-belief-networks.
  \verb pdf:pdf
  \endverb
  \field{journaltitle}{Nips}
  \field{year}{2009}
\endentry

\entry{Lhermitte2011a}{article}{}
  \name{author}{4}{}{%
    {{}%
     {Lhermitte}{L.}%
     {S.}{S.}%
     {}{}%
     {}{}}%
    {{}%
     {Verbesselt}{V.}%
     {J.}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Verstraeten}{V.}%
     {W.W.}{W.}%
     {}{}%
     {}{}}%
    {{}%
     {Coppin}{C.}%
     {P.}{P.}%
     {}{}%
     {}{}}%
  }
  \keyw{Change detection,Classification,Ecosystem dynamics,Time series
  analysis}
  \strng{namehash}{LS+1}
  \strng{fullhash}{LSVJVWCP1}
  \field{labelalpha}{Lhe+11}
  \field{sortinit}{L}
  \field{abstract}{%
  Time series of remote sensing imagery or derived vegetation indices and
  biophysical products have been shown particularly useful to characterize land
  ecosystem dynamics. Various methods have been developed based on temporal
  trajectory analysis to characterize, classify and detect changes in ecosystem
  dynamics. Although time series similarity measures play an important role in
  these methods, a quantitative comparison of the similarity measures is
  lacking. The objective of this study was to provide an overview and
  quantitative comparison of the similarity measures in function of varying
  time series and ecosystem characteristics, such as amplitude, timing and
  noise effects. For this purpose, the performance was evaluated for the
  commonly used similarity measures (D), ranging from Manhattan (DMan),
  Euclidean (DE) and Mahalanobis (DMah) distance measures, to correlation
  (DCC), Principal Component Analysis (PCA; DPCA) and Fourier based
  (DFFT,D$\xi$,DFk) similarities. The quantitative comparison consists of a
  series of Monte-Carlo simulations based on subsets of global MODIS Normalized
  Difference Vegetation index (NDVI) and Enhanced Vegetation Index (EVI) and
  Leaf Area Index (LAI) data. Results of the simulations reveal four main
  groups of time series similarity measures with different sensitivities: (i)
  DMan, DE, DPCA, DFk quantify the difference in time series values, (ii) DMah
  accounts for temporal correlation and non-stationarity of variance, (iii) DCC
  measures the temporal correlation, and (iv) the Fourier based DFFT and D$\xi$
  show their specific sensitivity based on the selected Fourier components. The
  difference measures show relatively the highest sensitivity to amplitude
  effects, whereas the correlation based measures are highly sensitive to
  variations in timing and noise. The Fourier based measures, finally, depend
  highly on the signal to noise ratio and the balance between amplitude and
  phase dominance. The heterogeneity in sensitivity of each D stresses the
  importance of (i) understanding the time series characteristics before
  applying any classification of change detection approach and (ii) defining
  the variability one wants to identify/account for. This requires an
  understanding of the ecosystem dynamics and time series characteristics
  related to the baseline, amplitude, timing, noise and variability of the
  ecosystem time series. This is also illustrated in the quantitative
  comparison, where the different sensitivities of D for the NDVI, EVI, and LAI
  data relate specifically to the temporal characteristics of each data set.
  Additionally, the effect of noise and intra- and interclass variability is
  demonstrated in a case study based on land cover classification.%
  }
  \verb{doi}
  \verb 10.1016/j.rse.2011.06.020
  \endverb
  \field{isbn}{0034-4257}
  \field{issn}{00344257}
  \field{number}{12}
  \field{pages}{3129\bibrangedash 3152}
  \field{title}{{A comparison of time series similarity measures for
  classification and change detection of ecosystem dynamics}}
  \verb{url}
  \verb http://www.sciencedirect.com/science/article/pii/S0034425711002446
  \endverb
  \field{volume}{115}
  \field{journaltitle}{Remote Sensing of Environment}
  \field{year}{2011}
\endentry

\entry{Liang2012}{article}{}
  \name{author}{4}{}{%
    {{}%
     {Liang}{L.}%
     {C.}{C.}%
     {}{}%
     {}{}}%
    {{}%
     {Zhang}{Z.}%
     {Y.}{Y.}%
     {}{}%
     {}{}}%
    {{}%
     {Shi}{S.}%
     {P.}{P.}%
     {}{}%
     {}{}}%
    {{}%
     {Hu}{H.}%
     {Z.}{Z.}%
     {}{}%
     {}{}}%
  }
  \list{publisher}{1}{%
    {Elsevier Inc.}%
  }
  \keyw{Positive unlabeled learning,Uncertain attribute,Uncertain data
  stream,Very fast decision tree}
  \strng{namehash}{LC+1}
  \strng{fullhash}{LCZYSPHZ1}
  \field{labelalpha}{Lia+12}
  \field{sortinit}{L}
  \field{abstract}{%
  Most data stream classification algorithms need to supply input with a large
  amount of precisely labeled data. However, in many data stream applications,
  streaming data contains inherent uncertainty, and labeled samples are
  difficult to be collected, while abundant data are unlabeled. In this paper,
  we focus on classifying uncertain data streams with only positive and
  unlabeled samples available. Based on concept-adapting very fast decision
  tree (CVFDT) algorithm, we propose an algorithm namely puuCVFDT (CVFDT for
  positive and unlabeled uncertain data). Experimental results on both
  synthetic and real-life datasets demonstrate the strong ability and
  efficiency of puuCVFDT to handle concept drift with uncertainty under
  positive and unlabeled learning scenario. Even when 90{\%} of the samples in
  the stream are unlabeled, the classification performance of the proposed
  algorithm is still compared to that of CVFDT, which is learned from fully
  labeled data without uncertainty. ?? 2012 Elsevier Inc. All rights reserved.%
  }
  \verb{doi}
  \verb 10.1016/j.ins.2012.05.023
  \endverb
  \field{issn}{00200255}
  \field{pages}{50\bibrangedash 67}
  \field{title}{{Learning very fast decision tree from uncertain data streams
  with positive and unlabeled samples}}
  \verb{url}
  \verb http://dx.doi.org/10.1016/j.ins.2012.05.023
  \endverb
  \field{volume}{213}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/Google Drive/Th{\`{e}}se/Bibliographie
  \verb /Decision Tree/Liang-Elsevier-2012{\_}Learning very fast decision tree
  \verb from uncertain data streams with positive and unlabeled samples.pdf:pdf
  \endverb
  \field{journaltitle}{Information Sciences}
  \field{year}{2012}
\endentry

\entry{Lin2003b}{article}{}
  \name{author}{4}{}{%
    {{}%
     {Lin}{L.}%
     {J.}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Keogh}{K.}%
     {E.}{E.}%
     {}{}%
     {}{}}%
    {{}%
     {Lonardi}{L.}%
     {S.}{S.}%
     {}{}%
     {}{}}%
    {{}%
     {Chiu}{C.}%
     {B.}{B.}%
     {}{}%
     {}{}}%
  }
  \keyw{data mining,data streams,discretize,symbolic,time series}
  \strng{namehash}{LJ+1}
  \strng{fullhash}{LJKELSCB1}
  \field{labelalpha}{Lin+03}
  \field{sortinit}{L}
  \field{abstract}{%
  The parallel explosions of interest in streaming data, and data mining of
  time series have had surprisingly little intersection. This is in spite of
  the fact that time series data are typically streaming data. The main reason
  for this apparent paradox is the fact that the vast majority of work on
  streaming data explicitly assumes that the data is discrete, whereas the vast
  majority of time series data is real valued.Many researchers have also
  considered transforming real valued time series into symbolic
  representations, nothing that such representations would potentially allow
  researchers to avail of the wealth of data structures and algorithms from the
  text processing and bioinformatics communities, in addition to allowing
  formerly "batch-only" problems to be tackled by the streaming community.
  While many symbolic representations of time series have been introduced over
  the past decades, they all suffer from three fatal flaws. Firstly, the
  dimensionality of the symbolic representation is the same as the original
  data, and virtually all data mining algorithms scale poorly with
  dimensionality. Secondly, although distance measures can be defined on the
  symbolic approaches, these distance measures have little correlation with
  distance measures defined on the original time series. Finally, most of these
  symbolic approaches require one to have access to all the data, before
  creating the symbolic representation. This last feature explicitly thwarts
  efforts to use the representations with streaming algorithms.In this work we
  introduce a new symbolic representation of time series. Our representation is
  unique in that it allows dimensionality/numerosity reduction, and it also
  allows distance measures to be defined on the symbolic approach that lower
  bound corresponding distance measures defined on the original series. As we
  shall demonstrate, this latter feature is particularly exciting because it
  allows one to run certain data mining algorithms on the efficiently
  manipulated symbolic representation, while producing identical results to the
  algorithms that operate on the original data. Finally, our representation
  allows the real valued data to be converted in a streaming fashion, with only
  an infinitesimal time and space overhead.We will demonstrate the utility of
  our representation on the classic data mining tasks of clustering,
  classification, query by content and anomaly detection.%
  }
  \verb{doi}
  \verb 10.1145/882082.882086
  \endverb
  \field{pages}{2\bibrangedash 11}
  \field{title}{{A Symbolic Representation of Time Series, with Implications
  for Streaming Algorithms}}
  \field{journaltitle}{SIGMOD Workshop on Research Issues in Data Mining and
  Knowledge Discovery}
  \field{year}{2003}
\endentry

\entry{Maharaj1996}{article}{}
  \name{author}{1}{}{%
    {{}%
     {Maharaj}{M.}%
     {E.A.}{E.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{ME1}
  \strng{fullhash}{ME1}
  \field{labelalpha}{Mah96}
  \field{sortinit}{M}
  \field{pages}{305\bibrangedash 331}
  \field{title}{{A Significance Test for Classifying ARMA Models}}
  \field{volume}{54}
  \field{journaltitle}{Journal of Statistical Computation and Simulation}
  \field{year}{1996}
\endentry

\entry{Martin2000}{article}{}
  \name{author}{1}{}{%
    {{}%
     {Martin}{M.}%
     {R.J.}{R.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{MR1}
  \strng{fullhash}{MR1}
  \field{labelalpha}{Mar00}
  \field{sortinit}{M}
  \field{abstract}{%
  Autoregressive-moving-average (ARMA) models seek to express
  a$\backslash$nsystem function of a discretely sampled process as a rational
  function$\backslash$nin the z-domain. Treating an ARMA model as a complex
  rational function,$\backslash$nwe discuss a metric defined on the set of
  complex rational functions. We$\backslash$ngive a natural measure of the
  {\&}ldquo;distance{\&}rdquo; between two ARMA$\backslash$nprocesses. The
  paper concentrates on the mathematics behind the problem$\backslash$nand
  shows that the various algebraic structures endow the choice
  of$\backslash$nmetric with some interesting and remarkable properties, which
  we$\backslash$ndiscuss. We suggest that the metric can be used in at least
  two$\backslash$ncircumstances: (i) in which we have signals arising from
  various models$\backslash$nthat are unknown (so we construct the distance
  matrix and perform$\backslash$ncluster analysis) and (ii) where there are
  several possible models$\backslash$nMi, all of which are known, and we wish
  to find which of$\backslash$nthese is closest to an observed data sequence
  modeled as M%
  }
  \verb{doi}
  \verb 10.1109/78.827549
  \endverb
  \field{issn}{1053587X}
  \field{number}{4}
  \field{pages}{1164\bibrangedash 1170}
  \field{title}{{Metric for ARMA processes}}
  \field{volume}{48}
  \field{journaltitle}{IEEE Transactions on Signal Processing}
  \field{year}{2000}
\endentry

\entry{Maurice1906}{article}{}
  \name{author}{1}{}{%
    {{}%
     {Maurice}{M.}%
     {F.M.}{F.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{MF1}
  \strng{fullhash}{MF1}
  \field{labelalpha}{Mau06}
  \field{sortinit}{M}
  \field{number}{1}
  \field{pages}{1\bibrangedash 72}
  \field{title}{{Sur quelques points du calcul fonctionnel}}
  \field{volume}{22}
  \field{journaltitle}{Rendiconti del Circolo Matematico di Palermo
  (1884-1940)}
  \field{year}{1906}
\endentry

\entry{Mohamad2013}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Mohamad}{M.}%
     {I.B.}{I.}%
     {}{}%
     {}{}}%
    {{}%
     {Usman}{U.}%
     {D.}{D.}%
     {}{}%
     {}{}}%
  }
  \keyw{Clustering,Decimal scaling,K-means,Min-max,Standardization,Z-score}
  \strng{namehash}{MIUD1}
  \strng{fullhash}{MIUD1}
  \field{labelalpha}{MU13}
  \field{sortinit}{M}
  \field{abstract}{%
  Data clustering is an important data exploration technique with many
  applications in data mining. K-means is one of the most well known methods of
  data mining that partitions a dataset into groups of patterns, many methods
  have been proposed to improve the performance of the K-means algorithm.
  Standardization is the central preprocessing step in data mining, to
  standardize values of features or attributes from different dynamic range
  into a specific range. In this paper, we have analyzed the performances of
  the three standardization methods on conventional K-means algorithm. By
  comparing the results on infectious diseases datasets, it was found that the
  result obtained by the z-score standardization method is more effective and
  efficient than min-max and decimal scaling standardization methods.%
  }
  \field{issn}{20407459}
  \field{number}{17}
  \field{pages}{3299\bibrangedash 3303}
  \field{title}{{Standardization and its effects on K-means clustering
  algorithm}}
  \field{volume}{6}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/Desktop/v6-3299-3303.pdf:pdf
  \endverb
  \field{journaltitle}{Research Journal of Applied Sciences, Engineering and
  Technology}
  \field{year}{2013}
\endentry

\entry{Montero2014}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Montero}{M.}%
     {P.}{P.}%
     {}{}%
     {}{}}%
    {{}%
     {Vilar}{V.}%
     {J.}{J.}%
     {}{}%
     {}{}}%
  }
  \keyw{clustering,dissimilarity measure,time series data,validation indices}
  \strng{namehash}{MPVJ1}
  \strng{fullhash}{MPVJ1}
  \field{labelalpha}{MV14}
  \field{sortinit}{M}
  \field{abstract}{%
  Time series clustering is an active research area with applications in a wide
  range of fields. One key component in cluster analysis is determining a
  proper dissimilarity mea- sure between two data objects, and many criteria
  have been proposed in the literature to assess dissimilarity between two time
  series. The R package TSclust is aimed to im- plement a large set of
  well-established peer-reviewed time series dissimilarity measures, including
  measures based on raw data, extracted features, underlying parametric models,
  complexity levels, and forecast behaviors. Computation of these measures
  allows the user to perform clustering by using conventional clustering
  algorithms. TSclust also includes a clustering procedure based on p values
  from checking the equality of generating models, and some utilities to
  evaluate cluster solutions. The implemented dissimilarity functions are
  accessible individually for an easier extension and possible use out of the
  clustering context. The main features of TSclust are described and examples
  of its use are presented.%
  }
  \field{number}{1}
  \field{title}{{TSclust : An R Package for Time Series Clustering}}
  \verb{url}
  \verb http://www.jstatsoft.org/v62/i01/paper
  \endverb
  \field{volume}{62}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/Desktop/TS clust.pdf:pdf
  \endverb
  \field{journaltitle}{Journal of Statistical Software November}
  \field{year}{2014}
\endentry

\entry{Nagelkerke1991}{article}{}
  \name{author}{1}{}{%
    {{}%
     {Nagelkerke}{N.}%
     {N.}{N.}%
     {}{}%
     {}{}}%
  }
  \keyw{Discrete probability,Log likelihood,Multiple correlation
  coefficient,Regression model,Residual variation}
  \strng{namehash}{NN1}
  \strng{fullhash}{NN1}
  \field{labelalpha}{Nag91}
  \field{sortinit}{N}
  \field{abstract}{%
  A generalization of the coefficient of determination R2 to general regression
  models is discussed. A modification of an earlier definition to allow for
  discrete models is proposed.%
  }
  \verb{doi}
  \verb 10.1093/biomet/78.3.691
  \endverb
  \field{isbn}{0006-3444}
  \field{issn}{00063444}
  \field{number}{3}
  \field{pages}{691\bibrangedash 692}
  \field{title}{{A note on a general definition of the coefficient of
  determination}}
  \field{volume}{78}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/Desktop/nagelkerke{\_}n.j.d.{\_}1991{\
  \verb _}-{\_}a{\_}note{\_}on{\_}a{\_}general{\_}definition{\_}of{\_}the{\_}co
  \verb efficient{\_}of{\_}determination.pdf:pdf
  \endverb
  \field{journaltitle}{Biometrika}
  \field{year}{1991}
\endentry

\entry{Najmeddine2012}{inproceedings}{}
  \name{author}{4}{}{%
    {{}%
     {Najmeddine}{N.}%
     {H.}{H.}%
     {}{}%
     {}{}}%
    {{}%
     {Jay}{J.}%
     {A.}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Marechal}{M.}%
     {P.}{P.}%
     {}{}%
     {}{}}%
    {{}%
     {Mari{\'{e}}}{M.}%
     {S.}{S.}%
     {}{}%
     {}{}}%
  }
  \keyw{Data mining,INCAS.,Time series,diagnosis and decision
  support,sensors,similarity measures}
  \strng{namehash}{NH+1}
  \strng{fullhash}{NHJAMPMS1}
  \field{labelalpha}{Naj+12}
  \field{sortinit}{N}
  \field{booktitle}{RFIA}
  \field{isbn}{9782953951523}
  \field{title}{{Mesures de similarit{\'{e}} pour l'aide {\`{a}} l'analyse des
  donn{\'{e}}es {\'{e}}nerg{\'{e}}tiques de b{\^{a}}timents}}
  \verb{url}
  \verb https://hal-cea.archives-ouvertes.fr/file/index/docid/661016/filename/a
  \verb rticle53{\_}modif.pdf
  \endverb
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley D
  \verb esktop/Downloaded/Najmeddine et al. - 2012 - Mesures de similarit{\'{e}
  \verb } pour l’aide {\`{a}} l’analyse des donn{\'{e}}es {\'{e}}nerg{\'{e}
  \verb }tiques de b{\^{a}}timents.pdf:pdf
  \endverb
  \field{year}{2012}
\endentry

\entry{Nguyen2012}{inproceedings}{}
  \name{author}{5}{}{%
    {{}%
     {Nguyen}{N.}%
     {L.}{L.}%
     {}{}%
     {}{}}%
    {{}%
     {Wu}{W.}%
     {P.}{P.}%
     {}{}%
     {}{}}%
    {{}%
     {Chan}{C.}%
     {W.}{W.}%
     {}{}%
     {}{}}%
    {{}%
     {Peng}{P.}%
     {W.}{W.}%
     {}{}%
     {}{}}%
    {{}%
     {Zhang}{Z.}%
     {Y.}{Y.}%
     {}{}%
     {}{}}%
  }
  \keyw{sentiment analysis,sentiment prediction,social network analysis}
  \strng{namehash}{NL+1}
  \strng{fullhash}{NLWPCWPWZY1}
  \field{labelalpha}{Ngu+12}
  \field{sortinit}{N}
  \field{abstract}{%
  More and more people express their opinions on social media such as Facebook
  and Twitter. Predictive analysis on social media time-series allows the
  stake-holders to leverage this immediate, accessible and vast reachable
  communication channel to react and proact against the public opinion. In
  particular, understanding and predicting the sentiment change of the public
  opinions will allow business and government agencies to react against
  negative sentiment and design strategies such as dispelling rumors and post
  balanced messages to revert the public opinion. In this paper, we present a
  strategy of building statistical models from the social media dynamics to
  predict collective sentiment dynamics. We model the collective sentiment
  change without delving into micro analysis of individual tweets or users and
  their corresponding low level network structures. Experiments on large-scale
  Twitter data show that the model can achieve above 85{\%} accuracy on
  directional sentiment prediction.%
  }
  \field{booktitle}{WISDOM}
  \verb{doi}
  \verb 10.1145/2346676.2346682
  \endverb
  \field{isbn}{9781450315432}
  \field{title}{{Predicting collective sentiment dynamics from time-series
  social media}}
  \field{year}{2012}
\endentry

\entry{Oncina2006}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Oncina}{O.}%
     {J.}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Sebban}{S.}%
     {M.}{M.}%
     {}{}%
     {}{}}%
  }
  \keyw{Finite-state transducers,Handwritten character recognition,Stochastic
  edit distance}
  \strng{namehash}{OJSM1}
  \strng{fullhash}{OJSM1}
  \field{labelalpha}{OS06}
  \field{sortinit}{O}
  \field{abstract}{%
  Many pattern recognition algorithms are based on the nearest-neighbour search
  and use the well-known edit distance, for which the primitive edit costs are
  usually fixed in advance. In this article, we aim at learning an unbiased
  stochastic edit distance in the form of a finite-state transducer from a
  corpus of (input, output) pairs of strings. Contrary to the other standard
  methods, which generally use the Expectation Maximisation algorithm, our
  algorithm learns a transducer independently on the marginal probability
  distribution of the input strings. Such an unbiased way to proceed requires
  to optimise the parameters of a conditional transducer instead of a joint
  one. We apply our new model in the context of handwritten digit recognition.
  We show, carrying out a large series of experiments, that it always
  outperforms the standard edit distance. ?? 2006 Pattern Recognition Society.%
  }
  \verb{doi}
  \verb 10.1016/j.patcog.2006.03.011
  \endverb
  \field{isbn}{0031-3203}
  \field{issn}{00313203}
  \field{pages}{1575\bibrangedash 1587}
  \field{title}{{Learning stochastic edit distance: Application in handwritten
  character recognition}}
  \field{volume}{39}
  \field{journaltitle}{Pattern Recognition}
  \field{year}{2006}
\endentry

\entry{PANAGIOTAKIS2008}{article}{}
  \name{author}{5}{}{%
    {{}%
     {Panagiotakis}{P.}%
     {C.}{C.}%
     {}{}%
     {}{}}%
    {{}%
     {Ramasso}{R.}%
     {E.}{E.}%
     {}{}%
     {}{}}%
    {{}%
     {Tziritas}{T.}%
     {G.}{G.}%
     {}{}%
     {}{}}%
    {{}%
     {Rombaut}{R.}%
     {M.}{M.}%
     {}{}%
     {}{}}%
    {{}%
     {Pellerin}{P.}%
     {D.}{D.}%
     {}{}%
     {}{}}%
  }
  \keyw{People detection,people counting,team activity recognition,transferable
  belief model,video analysis}
  \strng{namehash}{PC+1}
  \strng{fullhash}{PCRETGRMPD1}
  \field{labelalpha}{Pan+08}
  \field{sortinit}{P}
  \field{abstract}{%
  We present a shape-based method for automatic people detection and counting
  without any assumption concerning camera motion. In order to evaluate the
  robustness of the proposed method, we apply it for classifying athletics
  videos into two classes: videos of individual and videos of team sports. The
  videos used are real and characterized by dynamic and unconstrained
  environment. Moreover, in the case of team sport, we propose a shape
  deformations based method for running/hurdling discrimination (activity
  recognition). Robust, adaptive and independent from color, illumination
  changes and the camera motion, the proposed features are combined in the
  Transferable Belief Model (TBM) framework providing a two-level (frames and
  shot) video categorization. Experimental results of 97{\%} of accuracy for
  individual/team sport categorization using a dataset of 252 real videos of
  athletic meetings, acquired by moving cameras under varying view angles,
  indicate the stability and the good performance of the proposed scheme.%
  }
  \verb{doi}
  \verb 10.1142/S0218001408006752
  \endverb
  \field{issn}{0218-0014}
  \field{number}{06}
  \field{pages}{1187\bibrangedash 1213}
  \field{title}{{Shape-based individual/group detection for sport videos
  categorization}}
  \verb{url}
  \verb http://www.worldscientific.com/doi/abs/10.1142/S0218001408006752?journa
  \verb lCode=ijprai
  \endverb
  \field{volume}{22}
  \field{journaltitle}{International Journal of Pattern Recognition and
  Artificial Intelligence}
  \field{year}{2008}
\endentry

\entry{Piccolo1990}{article}{}
  \name{author}{1}{}{%
    {{}%
     {Piccolo}{P.}%
     {D.}{D.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{PD1}
  \strng{fullhash}{PD1}
  \field{labelalpha}{Pic90}
  \field{sortinit}{P}
  \field{abstract}{%
  Abstract. In a number of practical problems where clustering or choosing from
  a set of dynamic structures is needed, the introduction of a distance between
  the data is an early step in the application of multivariate statistical%
  }
  \verb{doi}
  \verb 10.1111/j.1467-9892.1990.tb00048.x
  \endverb
  \field{issn}{01439782}
  \field{title}{{A distance measure for classifying ARIMA models}}
  \verb{url}
  \verb http://www.blackwell-synergy.com/doi/abs/10.1111/j.1467-9892.1990.tb000
  \verb 48.x$\backslash$npapers3://publication/uuid/7709FD1E-9C95-4A60-8191-0F1
  \verb E0F5C6610
  \endverb
  \field{journaltitle}{Journal of Time Series Analysis}
  \field{year}{1990}
\endentry

\entry{Prekopcsak2012}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Prekopcs{\'{a}}k}{P.}%
     {Z.}{Z.}%
     {}{}%
     {}{}}%
    {{}%
     {Lemire}{L.}%
     {D.}{D.}%
     {}{}%
     {}{}}%
  }
  \keyw{Distance measure learning,Mahalanobis distance measure,Nearest
  Neighbor,Time-series classification}
  \strng{namehash}{PZLD1}
  \strng{fullhash}{PZLD1}
  \field{labelalpha}{PL12}
  \field{sortinit}{P}
  \field{abstract}{%
  To classify time series by nearest neighbors, we need to specify or learn one
  or several distance measures. We consider variations of the Mahalanobis
  distance measures which rely on the inverse covariance matrix of the data.
  Unfortunately --- for time series data --- the covariance matrix has often
  low rank. To alleviate this problem we can either use a pseudoinverse,
  covariance shrinking or limit the matrix to its diagonal. We review these
  alternatives and benchmark them against competitive methods such as the
  related Large Margin Nearest Neighbor Classification (LMNN) and the Dynamic
  Time Warping (DTW) distance. As we expected, we find that the DTW is
  superior, but the Mahalanobis distance measures are one to two orders of
  magnitude faster. To get best results with Mahalanobis distance measures, we
  recommend learning one distance measure per class using either covariance
  shrinking or the diagonal approach.%
  }
  \verb{doi}
  \verb 10.1007/s11634-012-0110-6
  \endverb
  \verb{eprint}
  \verb 1010.1526
  \endverb
  \field{isbn}{1163401201106}
  \field{issn}{18625347}
  \field{number}{3}
  \field{pages}{185\bibrangedash 200}
  \field{title}{{Time series classification by class-specific Mahalanobis
  distance measures}}
  \field{volume}{6}
  \field{journaltitle}{Advances in Data Analysis and Classification}
  \field{eprinttype}{arXiv}
  \field{year}{2012}
\endentry

\entry{Quinlan1986a}{article}{}
  \name{author}{1}{}{%
    {{}%
     {Quinlan}{Q.}%
     {J.R.}{J.}%
     {}{}%
     {}{}}%
  }
  \keyw{classification,decision trees,expert systems,induction,information
  theory,knowledge acquisition}
  \strng{namehash}{QJ1}
  \strng{fullhash}{QJ1}
  \field{labelalpha}{Qui86}
  \field{sortinit}{Q}
  \field{abstract}{%
  Recent literature has demonstrated the applicability of genetic programming
  to induction of decision trees for modelling toxicity endpoints. Compared
  with other decision tree induction techniques that are based upon recursive
  partitioning employing greedy searches to choose the best splitting attribute
  and value at each node that will necessarily miss regions of the search
  space, the genetic programming based approach can overcome the problem.
  However, the method still requires the discretization of the often
  continuous-valued toxicity endpoints prior to the tree induction. A novel
  extension of this method, YAdapt, is introduced in this work which models the
  original continuous endpoint by adaptively finding suitable ranges to
  describe the endpoints during the tree induction process, removing the need
  for discretization prior to tree induction and allowing the ordinal nature of
  the endpoint to be taken into account in the models built.%
  }
  \verb{doi}
  \verb 10.1023/A:1022643204877
  \endverb
  \field{isbn}{9783540283485}
  \field{issn}{15730565}
  \field{number}{1}
  \field{pages}{81\bibrangedash 106}
  \field{title}{{Induction of Decision Trees}}
  \field{volume}{1}
  \field{journaltitle}{Machine Learning}
  \field{year}{1986}
\endentry

\entry{Ramasso2008}{article}{}
  \name{author}{4}{}{%
    {{}%
     {Ramasso}{R.}%
     {E.}{E.}%
     {}{}%
     {}{}}%
    {{}%
     {Panagiotakis}{P.}%
     {C.}{C.}%
     {}{}%
     {}{}}%
    {{}%
     {Pellerin}{P.}%
     {D.}{D.}%
     {}{}%
     {}{}}%
    {{}%
     {Rombaut}{R.}%
     {M.}{M.}%
     {}{}%
     {}{}}%
  }
  \keyw{Human action recognition,Moving camera,Temporal Belief
  Filter,Transferable Belief Model}
  \strng{namehash}{RE+1}
  \strng{fullhash}{REPCPDRM1}
  \field{labelalpha}{Ram+08}
  \field{sortinit}{R}
  \field{abstract}{%
  This paper focuses on human behavior recognition where the main
  problem$\backslash$nis to bridge the semantic gap between the analogue
  observations of the$\backslash$nreal world and the symbolic world of human
  interpretation. For that, a$\backslash$nfusion architecture based on the
  Transferable Belief Model framework is$\backslash$nproposed and applied to
  action recognition of an athlete in video$\backslash$nsequences of athletics
  meeting with moving camera. Relevant features are$\backslash$nextracted from
  videos, based on both the camera motion analysis and the$\backslash$ntracking
  of particular points on the athlete's silhouette. Some models$\backslash$nof
  interpretation are used to link the numerical features to the
  symbols$\backslash$nto be recognized, which are running, jumping and falling
  actions. A$\backslash$nTemporal Belief Filter is then used to improve the
  robustness of action$\backslash$nrecognition. The proposed approach
  demonstrates good performance when$\backslash$ntested on real videos of
  athletics sports videos (high jumps, pole$\backslash$nvaults, triple jumps
  and long jumps) acquired by a moving camera and$\backslash$ndifferent view
  angles. The proposed system is also compared to
  Bayesian$\backslash$nNetworks.%
  }
  \verb{doi}
  \verb 10.1007/s10044-007-0073-y
  \endverb
  \field{issn}{14337541}
  \field{number}{1}
  \field{pages}{1\bibrangedash 19}
  \field{title}{{Human action recognition in videos based on the transferable
  belief model : Application to athletics jumps}}
  \field{volume}{11}
  \field{journaltitle}{Pattern Analysis and Applications}
  \field{year}{2008}
\endentry

\entry{Reyes2011}{article}{}
  \name{author}{1}{}{%
    {{}%
     {Reyes}{R.}%
     {M.}{M.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{RM1}
  \strng{fullhash}{RM1}
  \field{labelalpha}{Rey11}
  \field{sortinit}{R}
  \field{abstract}{%
  We present a gesture recognition approach for depth video data based on a
  novel Feature Weighting approach within the Dynamic Time Warping framework.
  Depth features from human joints are compared through video sequences using
  Dynamic Time Warping, and weights are assigned to features based on
  inter-intra class gesture variability. Feature Weighting in Dynamic Time
  Warping is then applied for recognizing begin-end of gestures in data
  sequences. The obtained results recognizing several gestures in depth data
  show high performance compared with classical Dynamic Time Warping approach.%
  }
  \verb{doi}
  \verb 10.1109/ICCVW.2011.6130384
  \endverb
  \field{isbn}{978-1-4673-0063-6}
  \field{pages}{1182\bibrangedash 1188}
  \field{title}{{Feature weighting in dynamic time warping for gesture
  recognition in depth data}}
  \verb{url}
  \verb http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6130384
  \endverb
  \field{journaltitle}{Computer Vision Workshops (ICCV Workshops), 2011 IEEE
  International Conference on}
  \field{year}{2011}
\endentry

\entry{Rabiner1993}{book}{}
  \name{author}{2}{}{%
    {{}%
     {Rabiner}{R.}%
     {L.R.}{L.}%
     {}{}%
     {}{}}%
    {{}%
     {Juang}{J.}%
     {B.}{B.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{RLJB1}
  \strng{fullhash}{RLJB1}
  \field{labelalpha}{RJ93}
  \field{sortinit}{R}
  \field{abstract}{%
  Provides a theoretically sound, technically accurate, and complete
  description of the basic knowledge and ideas that constitute a modern system
  for speech recognition by machine. Covers production, perception, and
  acoustic-phonetic characterization of the speech signal; signal processing
  and analysis methods for speech recognition; pattern comparison techniques;
  speech recognition system design and implementation; theory and
  implementation of hidden Markov models; speech recognition based on connected
  word models; large vocabulary continuous speech recognition; and task-
  oriented application of automatic speech recognition. For practicing
  engineers, scientists, linguists, and programmers interested in speech
  recognition.%
  }
  \field{booktitle}{Prentice Hall}
  \verb{doi}
  \verb 10.1002/ev.1647
  \endverb
  \field{isbn}{0130151572}
  \field{title}{{Fundamentals of Speech Recognition}}
  \verb{url}
  \verb http://cmp.felk.cvut.cz/cmp/support/phd112.html
  \endverb
  \field{volume}{103}
  \field{year}{1993}
\endentry

\entry{Ramoni2002}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Ramoni}{R.}%
     {M.}{M.}%
     {}{}%
     {}{}}%
    {{}%
     {Sebastiani}{S.}%
     {P.}{P.}%
     {}{}%
     {}{}}%
    {{}%
     {Cohen}{C.}%
     {P.}{P.}%
     {}{}%
     {}{}}%
  }
  \keyw{Bayesian learning,Clustering,Entropy,Heuristic search,Markov
  chains,Time series}
  \strng{namehash}{RMSPCP1}
  \strng{fullhash}{RMSPCP1}
  \field{labelalpha}{RSC02}
  \field{sortinit}{R}
  \field{abstract}{%
  This paper introduces a Bayesian method for clustering dynamic processes. The
  method models dynamics as Markov chains and then applies an agglomerative
  clustering procedure to discover the most probable set of clusters capturing
  different dynamics. To increase efficiency, the method uses an entropy-based
  heuristic search strategy. A controlled experiment suggests that the method
  is very accurate when applied to artificial time series in a broad range of
  conditions and, when applied to clustering sensor data from mobile robots, it
  produces clusters that are meaningful in the domain of application.%
  }
  \verb{doi}
  \verb 10.1023/A:1013635829250
  \endverb
  \field{isbn}{08856125}
  \field{issn}{08856125}
  \field{number}{1}
  \field{pages}{91\bibrangedash 121}
  \field{title}{{Bayesian clustering by dynamics}}
  \field{volume}{47}
  \field{journaltitle}{Machine Learning}
  \field{year}{2002}
\endentry

\entry{Salvador}{inproceedings}{}
  \name{author}{2}{}{%
    {{}%
     {Salvador}{S.}%
     {S.}{S.}%
     {}{}%
     {}{}}%
    {{}%
     {Chan}{C.}%
     {P.}{P.}%
     {}{}%
     {}{}}%
  }
  \keyw{dynamic time warping,time series}
  \strng{namehash}{SSCP1}
  \strng{fullhash}{SSCP1}
  \field{labelalpha}{SC04}
  \field{sortinit}{S}
  \field{abstract}{%
  The dynamic time warping (DTW) algorithm is able to find the optimal
  alignment between two time series. It is often used to determine time series
  similarity, classification, and to find corresponding regions between two
  time series. DTW has a quadratic time and space complexity that limits its
  use to only small time series data sets. In this paper we introduce FastDTW,
  an approximation of DTW that has a linear time and space complexity. FastDTW
  uses a multilevel approach that recursively projects a solution from a coarse
  resolution and refines the projected solution. We prove the linear time and
  space complexity of FastDTW both theoretically and empirically. We also
  analyze the accuracy of FastDTW compared to two other existing approximate
  DTW algorithms: Sakoe-Chuba Bands and Data Abstraction. Our results show a
  large improvement in accuracy over the existing methods.%
  }
  \field{booktitle}{3rd Workshop on Mining Temporal and Sequential Data}
  \field{title}{{FastDTW : Toward Accurate Dynamic Time Warping in Linear Time
  and Space}}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley D
  \verb esktop/Downloaded/Salvador, Chan - Unknown - FastDTW Toward Accurate Dy
  \verb namic Time Warping in Linear Time and Space.pdf:pdf
  \endverb
  \field{year}{2004}
\endentry

\entry{Sakoe1978b}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Sakoe}{S.}%
     {H.}{H.}%
     {}{}%
     {}{}}%
    {{}%
     {Chiba}{C.}%
     {S.}{S.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{SHCS1}
  \strng{fullhash}{SHCS1}
  \field{labelalpha}{SC78}
  \field{sortinit}{S}
  \field{abstract}{%
  This paper reports on an optimum dynamic programming (DP) based
  time-normalization algorithm for spoken word recognition. First, a general
  principle of time-normalization is given using timewarping function. Then,
  two time-normalized distance definitions, d e d symmetric and asymmetric
  forms, are derived from the principle. These two forms are compared with each
  other through theoretical discussions and experimental studies. The symmetric
  form algorithm superiority is established. A new technique, called slope
  constraint, is successfully introduced, iwn hich the warping function slope
  isr estricted so as to improve discrimination between words in different
  categories. The effective slope constraint characteristic is qualitatively
  analyzed, and the optimum slope constraint condition is determined through
  experiments. The optimized algorithm is then extensively subjected to
  experimentat comparison with various DP-algorithms, previously applied to
  spoken word recognition by different research groups. The experiment shows
  that the present algorithm gives no more than about twothirds errors, even
  compared to the best conventional algorithm.%
  }
  \verb{doi}
  \verb 10.1109/TASSP.1978.1163055
  \endverb
  \field{isbn}{1558601244}
  \field{issn}{00963518}
  \field{number}{1}
  \field{pages}{43\bibrangedash 49}
  \field{title}{{Dynamic Programming Algorithm Optimization for Spoken Word
  Recognition}}
  \field{volume}{ASSP-26}
  \field{journaltitle}{IEEE transactions on acoustics, speech, and signal
  processing}
  \field{year}{1978}
\endentry

\entry{Silverman1989}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Silverman}{S.}%
     {B.}{B.}%
     {}{}%
     {}{}}%
    {{}%
     {Jones}{J.}%
     {M.}{M.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{SBJM1}
  \strng{fullhash}{SBJM1}
  \field{labelalpha}{SJ89}
  \field{sortinit}{S}
  \field{abstract}{%
  In 1951, Evelyn Fix and J.L. Hodges, Jr. wrote a technical report which
  contained prophetic work on nonparametric discriminant analysis and
  probability density estimation, and which was never published by the authors.
  The report introduced several important concepts for the first time. It is of
  interest not only for historical reasons but also because it contains much
  material that is still of contemporary relevance. Here, the report is printed
  in full together with a commentary placing the paper in context and
  interpreting its ideas in the light of more modern developments. /// En 1951,
  E. Fix et J.L. Hodges, Jr. ont �crit un rapport technique proph�tique sur
  l'analyse non-param�trique de discrimination et l'estimation de la
  densit� de probabilit�, mais celui-ci ne fut jamais publi� par ses
  auteurs. Ce rapport introduit plusieurs id�es nouvelles et importantes. Il
  nous int�resse non seulement pour des raisons historiques, mais aussi parce
  qu'il contient des concepts qui sont encore importants de nos jours. Nous le
  publions ici en entier, accompagn� d'un commentaire qui l'interpr�te d'un
  point de vue plus moderne.%
  }
  \verb{doi}
  \verb 10.2307/1403796
  \endverb
  \field{isbn}{03067734}
  \field{issn}{03067734}
  \field{number}{3}
  \field{pages}{pp. 233\bibrangedash 238}
  \field{title}{{An Important Contribution to Nonparametric Discriminant
  Analysis and Density Estimation: Commentary on Fix and Hodges (1951)}}
  \verb{url}
  \verb http://www.jstor.org/stable/1403796
  \endverb
  \field{volume}{57}
  \field{journaltitle}{International Statistical Review / Revue Internationale
  de Statistique}
  \field{year}{1989}
\endentry

\entry{Smyth1997}{article}{}
  \name{author}{1}{}{%
    {{}%
     {Smyth}{S.}%
     {P.}{P.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{SP1}
  \strng{fullhash}{SP1}
  \field{labelalpha}{Smy97}
  \field{sortinit}{S}
  \field{abstract}{%
  This paper discusses a probabilistic model-based approach to clustering
  sequences, using hidden Markov models (HMMs). The problem can be framed as a
  generalization of the standard mixture model approach to clustering in
  feature space. Two primary issues are addressed. First, a novel parameter
  initialization procedure is proposed, and second, the more difficult problem
  of determining the number of clusters K, from the data, is investigated.
  Experimental results indicate that the proposed techniques are useful for
  revealing hidden cluster structure in data sets of sequences. 1 Introduction
  Consider a data set D consisting of N sequences, D = fS 1 ; . . . ; SN g. S i
  = (x i 1 ; . . . x i L i is a sequence of length L i composed of potentially
  multivariate feature vectors x. The problem addressed in this paper is the
  discovery from data of a natural grouping of the sequences into K clusters.
  This is analagous to clustering in multivariate feature space which is
  normally handled by m...%
  }
  \field{isbn}{0262100657}
  \field{issn}{10495258}
  \field{pages}{648\bibrangedash 654}
  \field{title}{{Clustering sequences with hidden Markov models}}
  \verb{url}
  \verb http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.44.3648
  \endverb
  \field{volume}{9}
  \field{journaltitle}{Advances in neural information processing systems}
  \field{year}{1997}
\endentry

\entry{Sahidullah2012a}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Sahidullah}{S.}%
     {M.}{M.}%
     {}{}%
     {}{}}%
    {{}%
     {Saha}{S.}%
     {G.}{G.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{SMSG1}
  \strng{fullhash}{SMSG1}
  \field{labelalpha}{SS12}
  \field{extraalpha}{1}
  \field{sortinit}{S}
  \field{number}{4}
  \field{pages}{543\bibrangedash 565}
  \field{title}{{Design, analysis and experimental evaluation of block based
  transformation in mfcc computation for speaker recognition}}
  \field{volume}{54}
  \field{journaltitle}{Speech Communication}
  \field{year}{2012}
\endentry

\entry{Sahidullah2012}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Sahidullah}{S.}%
     {M.}{M.}%
     {}{}%
     {}{}}%
    {{}%
     {Saha}{S.}%
     {G.}{G.}%
     {}{}%
     {}{}}%
  }
  \keyw{Block transform,Correlation matrix,DCT,Decorrelation technique,Linear
  transformation,MFCC,Missing feature theory,Narrow-band noise,Speaker
  recognition}
  \strng{namehash}{SMSG1}
  \strng{fullhash}{SMSG1}
  \field{labelalpha}{SS12}
  \field{extraalpha}{2}
  \field{sortinit}{S}
  \field{abstract}{%
  Standard Mel frequency cepstrum coefficient (MFCC) computation technique
  utilizes discrete cosine transform (DCT) for decorrelating log energies of
  filter bank output. The use of DCT is reasonable here as the covariance
  matrix of Mel filter bank log energy (MFLE) can be compared with that of
  highly correlated Markov-I process. This full-band based MFCC computation
  technique where each of the filter bank output has contribution to all
  coefficients, has two main disadvantages. First, the covariance matrix of the
  log energies does not exactly follow Markov-I property. Second, full-band
  based MFCC feature gets severely degraded when speech signal is corrupted
  with narrow-band channel noise, though few filter bank outputs may remain
  unaffected. In this work, we have studied a class of linear transformation
  techniques based on block wise transformation of MFLE which effectively
  decorrelate the filter bank log energies and also capture speech information
  in an efficient manner. A thorough study has been carried out on the block
  based transformation approach by investigating a new partitioning technique
  that highlights associated advantages. This article also reports a novel
  feature extraction scheme which captures complementary information to wide
  band information; that otherwise remains undetected by standard MFCC and
  proposed block transform (BT) techniques. The proposed features are evaluated
  on NIST SRE databases using Gaussian mixture model-universal background model
  (GMM-UBM) based speaker recognition system. We have obtained significant
  performance improvement over baseline features for both matched and
  mismatched condition, also for standard and narrow-band noises. The proposed
  method achieves significant performance improvement in presence of
  narrow-band noise when clubbed with missing feature theory based score
  computation scheme. ?? 2011 Elsevier B.V. All rights reserved.%
  }
  \verb{doi}
  \verb 10.1016/j.specom.2011.11.004
  \endverb
  \field{isbn}{0167-6393}
  \field{issn}{01676393}
  \field{number}{4}
  \field{pages}{543\bibrangedash 565}
  \field{title}{{Design, analysis and experimental evaluation of block based
  transformation in MFCC computation for speaker recognition}}
  \verb{url}
  \verb http://dx.doi.org/10.1016/j.specom.2011.11.004
  \endverb
  \field{volume}{54}
  \field{journaltitle}{Speech Communication}
  \field{year}{2012}
\endentry

\entry{Schlkopf2013}{book}{}
  \name{author}{2}{}{%
    {{}%
     {Schlkopf}{S.}%
     {B.}{B.}%
     {}{}%
     {}{}}%
    {{}%
     {Smola}{S.}%
     {A.}{A.}%
     {}{}%
     {}{}}%
  }
  \keyw{icle}
  \strng{namehash}{SBSA1}
  \strng{fullhash}{SBSA1}
  \field{labelalpha}{SS13}
  \field{sortinit}{S}
  \field{abstract}{%
  Predicting the binding mode of flexible polypeptides to proteins is an
  important task that falls outside the domain of applicability of most small
  molecule and protein−protein docking tools. Here, we test the small
  molecule flexible ligand docking program Glide on a set of 19
  non-$\alpha$-helical peptides and systematically improve pose prediction
  accuracy by enhancing Glide sampling for flexible polypeptides. In addition,
  scoring of the poses was improved by post-processing with physics-based
  implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10
  scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the
  interface backbone atoms) increased from 21{\%} with default Glide SP
  settings to 58{\%} with the enhanced peptide sampling and scoring protocol in
  the case of redocking to the native protein structure. This approaches the
  accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success
  for these 19 peptides) while being over 100 times faster. Cross-docking was
  performed for a subset of cases where an unbound receptor structure was
  available, and in that case, 40{\%} of peptides were docked successfully. We
  analyze the results and find that the optimized polypeptide protocol is most
  accurate for extended peptides of limited size and number of formal charges,
  defining a domain of applicability for this approach.%
  }
  \field{booktitle}{Journal of Chemical Information and Modeling}
  \verb{doi}
  \verb 10.1017/CBO9781107415324.004
  \endverb
  \verb{eprint}
  \verb arXiv:1011.1669v3
  \endverb
  \field{isbn}{9788578110796}
  \field{issn}{1098-6596}
  \field{pages}{1689\bibrangedash 1699}
  \field{title}{{Learning with Kernels}}
  \field{volume}{53}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/Google Drive/Th{\`{e}}se/Bibliographie
  \verb /Livre/Schokopf, Smola-Learning with Kernels.pdf:pdf
  \endverb
  \field{eprinttype}{arXiv}
  \field{year}{2013}
\endentry

\entry{Sadri2003}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Sadri}{S.}%
     {J.}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Suen}{S.}%
     {C.}{C.}%
     {}{}%
     {}{}}%
    {{}%
     {Bui}{B.}%
     {T.}{T.}%
     {}{}%
     {}{}}%
  }
  \keyw{feature extraction,machine learning,mlp neural network,multiple support
  vector classifiers,ocr,optical character recognition,support,svm,vector
  machine}
  \strng{namehash}{SJSCBT1}
  \strng{fullhash}{SJSCBT1}
  \field{labelalpha}{SSB03}
  \field{sortinit}{S}
  \field{abstract}{%
  : A new method for recognition of isolated handwritten Arabic/Persian digits
  is presented. This method is based on Support Vector Machines (SVMs), and a
  new approach of feature extraction. Each digit is considered from four
  different views, and from each view 16 features are extracted and combined to
  obtain 64 features. Using these features, multiple SVM classifiers are
  trained to separate different classes of digits. CENPARMI Indian
  (Arabic/Persian) handwritten digit database is used for training and testing
  of SVM classifiers. Based on this database, differences between Arabic and
  Persian digits in digit recognition are shown. This database provides 7390
  samples for training and 3035 samples for testing from the real life samples.
  Experiments show that the proposed features can provide a very good
  recognition result using Support Vector Machines at a recognition rate
  94.14{\%}, compared with 91.25 {\%} obtained by MLP neural network classifier
  using the same features and test set.%
  }
  \field{pages}{300\bibrangedash 307}
  \field{title}{{Application of Support Vector Machines for recognition of
  handwritten Arabic/Persian digits}}
  \field{volume}{1}
  \field{journaltitle}{Second Conference on Machine Vision and Image Processing
  {\&} Applications (MVIP 2003)}
  \field{year}{2003}
\endentry

\entry{Torrence1998}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Torrence}{T.}%
     {C.}{C.}%
     {}{}%
     {}{}}%
    {{}%
     {Compo}{C.}%
     {G.}{G.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{TCCG1}
  \strng{fullhash}{TCCG1}
  \field{labelalpha}{TC98}
  \field{sortinit}{T}
  \field{abstract}{%
  A practical step-by-step guide to wavelet analysis is given, with examples
  taken from time series of the El Ni{\~{n}}o–Southern Oscillation (ENSO).
  The guide includes a comparison to the windowed Fourier transform, the choice
  of an appropriate wavelet basis function, edge effects due to finite-length
  time series, and the relationship between wavelet scale and Fourier
  frequency. New statistical significance tests for wavelet power spectra are
  developed by deriving theoretical wavelet spectra for white and red noise
  processes and using these to establish significance levels and confidence
  intervals. It is shown that smoothing in time or scale can be used to
  increase the confidence of the wavelet spectrum. Empirical formulas are given
  for the effect of smoothing on significance levels and confidence intervals.
  Extensions to wavelet analysis such as filtering, the power Hovm{\"{o}}ller,
  cross-wavelet spectra, and coherence are described. The statistical
  significance tests are used to give a quantitative measure of changes in ENSO
  variance on interdecadal timescales. Using new datasets that extend back to
  1871, the Ni{\~{n}}o3 sea surface temperature and the Southern Oscillation
  index show significantly higher power during 1880–1920 and 1960–90, and
  lower power during 1920–60, as well as a possible 15-yr modulation of
  variance. The power Hovm{\"{o}}ller of sea level pressure shows significant
  variations in 2–8-yr wavelet power in both longitude and time.%
  }
  \verb{doi}
  \verb 10.1175/1520-0477(1998)079<0061:APGTWA>2.0.CO;2
  \endverb
  \field{isbn}{0871706881}
  \field{issn}{00030007}
  \field{number}{1}
  \field{pages}{61\bibrangedash 78}
  \field{title}{{A Practical Guide to Wavelet Analysis}}
  \field{volume}{79}
  \field{journaltitle}{Bulletin of the American Meteorological Society}
  \field{year}{1998}
\endentry

\entry{Wang2002}{article}{}
  \name{author}{1}{}{%
    {{}%
     {Wang}{W.}%
     {J.}{J.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{WJ1}
  \strng{fullhash}{WJ1}
  \field{labelalpha}{Wan02}
  \field{sortinit}{W}
  \field{abstract}{%
  Recently a new learning method called support vector machines (SVM) has shown
  comparable or better results than neural networks on some applications. In
  this thesis we exploit the possibility of using SVM for three important
  issues of bioinformatics: the prediction of protein secondary structure,
  multi-class protein fold recognition, and the prediction of human signal
  peptide cleavage sites. By using similar data, we demonstrate that SVM can
  easily achieve comparable accuracy as using neural networks. Therefore, in
  the future it is a promising direction to apply SVM on more bioinformatics
  applications.%
  }
  \field{pages}{1\bibrangedash 56}
  \field{title}{{Support Vector Machines ( SVM ) in bioinformatics
  Bioinformatics applications}}
  \verb{url}
  \verb http://www.csie.ntu.edu.tw/{~}p88012/Bio{\_}SVM.pdf
  \endverb
  \field{journaltitle}{Bioinformatics}
  \field{year}{2002}
\endentry

\entry{WienerN1942}{book}{}
  \name{author}{1}{}{%
    {{}%
     {Wiener}{W.}%
     {N.}{N.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{WN1}
  \strng{fullhash}{WN1}
  \field{labelalpha}{Wie42}
  \field{sortinit}{W}
  \field{title}{{Extrapolation, Interpolation {\&} Smoothing of Stationary Time
  Series - With Engineering Applications}}
  \list{institution}{1}{%
    {Report of the Services 19, Research Project DIC-6037 MIT}%
  }
  \field{year}{1942}
\endentry

\entry{Weinberger2009}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Weinberger}{W.}%
     {K.}{K.}%
     {}{}%
     {}{}}%
    {{}%
     {Saul}{S.}%
     {L.}{L.}%
     {}{}%
     {}{}}%
  }
  \keyw{convex optimization,ing,mahalanobis distance,metric learn-,multi-class
  classification,semi-definite programming,support vector machines}
  \strng{namehash}{WKSL1}
  \strng{fullhash}{WKSL1}
  \field{labelalpha}{WS09}
  \field{sortinit}{W}
  \field{abstract}{%
  The accuracy of k-nearest neighbor (kNN) classification depends significantly
  on the metric used to compute distances between different examples. In this
  paper, we show how to learn a Maha- lanobis distance metric for kNN
  classification from labeled examples. The Mahalanobis metric can equivalently
  be viewed as a global linear transformation of the input space that precedes
  kNN classification using Euclidean distances. In our approach, the metric is
  trained with the goal that the k-nearest neighbors always belong to the same
  class while examples from different classes are separated by a largemargin.
  As in support vectormachines (SVMs), themargin criterion leads to a convex
  optimization based on the hinge loss. Unlike learning in SVMs, however, our
  approach re- quires no modification or extension for problems in multiway (as
  opposed to binary) classification. In our framework, the Mahalanobis distance
  metric is obtained as the solution to a semidefinite program. On several data
  sets of varying size and difficulty, we find that metrics trained in this way
  lead to significant improvements in kNN classification. Sometimes these
  results can be further improved by clustering the training examples and
  learning an individual metric within each cluster. We show how to learn and
  combine these local metrics in a globally integrated manner. Keywords: convex
  optimization, semi-definite programming,Mahalanobis distance,metric learn-
  ing, multi-class classification, support vector machines 1.%
  }
  \field{pages}{207\bibrangedash 244}
  \field{title}{{Distance Metric Learning for Large Margin Nearest Neighbor
  Classification}}
  \verb{url}
  \verb http://www.jmlr.org/papers/volume10/weinberger09a/weinberger09a.pdf
  \endverb
  \field{volume}{10}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley D
  \verb esktop/Downloaded/Weinberger, Saul - 2009 - Distance Metric Learning fo
  \verb r Large Margin Nearest Neighbor Classification(2).pdf:pdf
  \endverb
  \field{journaltitle}{Journal of Machine Learning Research}
  \field{year}{2009}
\endentry

\entry{Wang2012}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Wang}{W.}%
     {J.}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Woznica}{W.}%
     {A.}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Kalousis}{K.}%
     {A.}{A.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{WJWAKA1}
  \strng{fullhash}{WJWAKA1}
  \field{labelalpha}{WWK12}
  \field{sortinit}{W}
  \field{abstract}{%
  We study the problem of learning local metrics for nearest neighbor
  classification. Most previous works on local metric learning learn a number
  of local unrelated metrics. While this ”independence” approach delivers
  an increased flexibility its downside is the considerable risk of
  overfitting. We present a newparametric local metric learning method in which
  we learn a smooth metric matrix function over the data manifold. Using an
  approximation error bound of the metric matrix function we learn local
  metrics as linear combinations of basis metrics defined on anchor points over
  different regions of the instance space. We constrain the metric matrix
  function by imposing on the linear combinations manifold regularization which
  makes the learned metric matrix function vary smoothly along the geodesics of
  the data manifold. Our metric learning method has excellent performance both
  in terms of predictive power and scalability. We experimented with several
  large- scale classification problems, tens of thousands of instances, and
  compared it with several state of the art metric learning methods, both
  global and local, as well as to SVM with automatic kernel selection, all of
  which it outperforms in a significant manner.%
  }
  \field{isbn}{9781627480031}
  \field{issn}{10495258}
  \field{pages}{1\bibrangedash 9}
  \field{title}{{Parametric local metric learning for nearest neighbor
  classification}}
  \verb{url}
  \verb http://arxiv.org/abs/1209.3056
  \endverb
  \field{journaltitle}{arXiv preprint arXiv:1209.3056}
  \field{year}{2012}
\endentry

\entry{Xi2006a}{inproceedings}{}
  \name{author}{5}{}{%
    {{}%
     {Xi}{X.}%
     {X.}{X.}%
     {}{}%
     {}{}}%
    {{}%
     {Keogh}{K.}%
     {E.}{E.}%
     {}{}%
     {}{}}%
    {{}%
     {Shelton}{S.}%
     {C.}{C.}%
     {}{}%
     {}{}}%
    {{}%
     {Wei}{W.}%
     {L.}{L.}%
     {}{}%
     {}{}}%
    {{}%
     {Ratanamahatana}{R.}%
     {C.A.}{C.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{XX+1}
  \strng{fullhash}{XXKESCWLRC1}
  \field{labelalpha}{Xi+06}
  \field{sortinit}{X}
  \field{abstract}{%
  Many algorithms have been proposed for the problem of time series
  classification. However, it is clear that one-nearest-neighbor with Dynamic
  Time Warping (DTW) distance is exceptionally difficult to beat. This approach
  has one weakness, however; it is computationally too demanding for many
  realtime applications. One way to mitigate this problem is to speed up the
  DTW calculations. Nonetheless, there is a limit to how much this can help. In
  this work, we propose an additional technique, numerosity reduction, to speed
  up one-nearest-neighbor DTW. While the idea of numerosity reduction for
  nearest-neighbor classifiers has a long history, we show here that we can
  leverage off an original observation about the relationship between dataset
  size and DTW constraints to produce an extremely compact dataset with little
  or no loss in accuracy. We test our ideas with a comprehensive set of
  experiments, and show that it can efficiently produce extremely fast accurate
  classifiers.%
  }
  \field{booktitle}{Proceedings of the 23rd international conference on Machine
  learning (ICML)}
  \verb{doi}
  \verb 10.1145/1143844.1143974
  \endverb
  \field{isbn}{1595933832}
  \field{pages}{1033\bibrangedash 1040}
  \field{title}{{A Fast time series classification using numerosity reduction}}
  \verb{url}
  \verb http://dl.acm.org/citation.cfm?id=1143974
  \endverb
  \field{year}{2006}
\endentry

\entry{Xu2012}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Xu}{X.}%
     {Z.}{Z.}%
     {}{}%
     {}{}}%
    {{}%
     {Weinberger}{W.}%
     {K.}{K.}%
     {}{}%
     {}{}}%
    {{}%
     {Chapelle}{C.}%
     {O.}{O.}%
     {}{}%
     {}{}}%
  }
  \keyw{distance learning,mahalanobis distance,metric learning,semi-definite
  programming,support vector machines}
  \strng{namehash}{XZWKCO1}
  \strng{fullhash}{XZWKCO1}
  \field{labelalpha}{XWC12}
  \field{sortinit}{X}
  \field{abstract}{%
  Recent work in metric learning has significantly improved the
  state-of-the-art in k-nearest neighbor classification. Support vector
  machines (SVM), particularly with RBF kernels, are amongst the most popular
  classification algorithms that uses distance metrics to compare examples.
  This paper provides an empirical analysis of the efficacy of three of the
  most popular Mahalanobis metric learning algorithms as pre-processing for SVM
  training. We show that none of these algorithms generate metrics that lead to
  particularly satisfying improvements for SVM-RBF classification. As a remedy
  we introduce support vector metric learning (SVML), a novel algorithm that
  seamlessly combines the learning of a Mahalanobis metric with the training of
  the RBF-SVM parameters. We demonstrate the capabilities of SVML on nine
  benchmark data sets of varying sizes and difficulties. In our study, SVML
  outperforms all alternative state-of-the-art metric learning algorithms in
  terms of accuracy and establishes itself as a serious alternative to the
  standard Euclidean metric with model selection by cross validation.%
  }
  \verb{eprint}
  \verb 1208.3422
  \endverb
  \field{pages}{1\bibrangedash 17}
  \field{title}{{Distance Metric Learning for Kernel Machines}}
  \verb{url}
  \verb http://arxiv.org/abs/1208.3422
  \endverb
  \field{journaltitle}{Arxiv}
  \field{eprinttype}{arXiv}
  \field{year}{2012}
\endentry

\entry{Yin2008}{inproceedings}{}
  \name{author}{2}{}{%
    {{}%
     {Yin}{Y.}%
     {J.}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Gaber}{G.}%
     {M.}{M.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{YJGM1}
  \strng{fullhash}{YJGM1}
  \field{labelalpha}{YG08}
  \field{sortinit}{Y}
  \field{abstract}{%
  Event detection is a critical task in sensor networks, especially for
  environmental monitoring applications. Traditional solutions to event
  detection are based on analyzing one-shot data points, which might incur a
  high false alarm rate because sensor data is inherently unreliable and noisy.
  To address this issue, we propose a novel Distributed Single-pass Incremental
  Clustering (DSIC) technique to cluster the time series obtained at sensor
  nodes based on their underlying trends. In order to achieve scalability and
  energy-efficiency, our DSIC technique uses a hierarchical structure of sensor
  networks as the underlying infrastructure. The algorithm first compresses the
  time series produced at individual sensor nodes into a compact representation
  using Haar wavelet transform, and then, based on dynamic time warping
  distances, hierarchically groups the approximate time series into a global
  clustering model in an incremental manner. Experimental results on both real
  data and synthetic data demonstrate that our DSIC algorithm is accurate,
  energy-efficient and robust with respect to network topology changes.%
  }
  \field{booktitle}{ICDM}
  \verb{doi}
  \verb 10.1109/ICDM.2008.58
  \endverb
  \field{isbn}{9780769535029}
  \field{issn}{15504786}
  \field{title}{{Clustering distributed time series in sensor networks}}
  \field{year}{2008}
\endentry

\entry{Yang1999}{incollection}{}
  \name{author}{2}{}{%
    {{}%
     {Yang}{Y.}%
     {Y.}{Y.}%
     {}{}%
     {}{}}%
    {{}%
     {Liu}{L.}%
     {X.}{X.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{YYLX1}
  \strng{fullhash}{YYLX1}
  \field{labelalpha}{YL99}
  \field{sortinit}{Y}
  \field{abstract}{%
  This paper reports a controlled study with statistical signifi cance tests on
  five text categorization methods: the Support Vector Machines (SVM), a
  kNearest Neighbor (kNN) clas sifier, a neural network (NNet) approach, the
  Linear Least squares Fit (LLSF) mapping and a Naive Bayes (NB) classi fier.
  We focus on the robustness of these methods in dealing with a skewed category
  distribution, and their performance as function of the trainingset category
  frequency. Our re sults show that SVM, kNN and LLSF significantly outper form
  NNet and NB when the number of positive training instances per category are
  small (less than ten), and that all the methods perform comparably when the
  categories are sufficiently common (over 300 instances).%
  }
  \field{booktitle}{Proceedings of the 22nd annual international ACM SIGIR
  conference on Research and development in information retrieval SIGIR 99}
  \verb{doi}
  \verb 10.1145/312624.312647
  \endverb
  \field{isbn}{1581130961}
  \field{pages}{42\bibrangedash 49}
  \field{title}{{A re-examination of text categorization methods}}
  \field{year}{1999}
\endentry

\entry{Zhang2013}{article}{}
  \name{author}{4}{}{%
    {{}%
     {Zhang}{Z.}%
     {Y.}{Y.}%
     {}{}%
     {}{}}%
    {{}%
     {Zhang}{Z.}%
     {H.}{H.}%
     {}{}%
     {}{}}%
    {{}%
     {Nasrabadi}{N.}%
     {N.}{N.}%
     {}{}%
     {}{}}%
    {{}%
     {Huang}{H.}%
     {T.}{T.}%
     {}{}%
     {}{}}%
  }
  \keyw{Joint classification,Metric learning,Multi-sensor fusion}
  \strng{namehash}{ZY+1}
  \strng{fullhash}{ZYZHNNHT1}
  \field{labelalpha}{Zha+13}
  \field{sortinit}{Z}
  \field{abstract}{%
  In this paper, we propose a multiple-metric learning algorithm to learn
  jointly a set of optimal homogenous/ heterogeneous metrics in order to fuse
  the data collected from multiple sensors for joint classification. The
  learned metrics have the potential to perform better than the conventional
  Euclidean metric for classification. Moreover, in the case of heterogenous
  sensors, the learned multiple metrics can be quite different, which are
  adapted to each type of sensor. By learning the multiple metrics jointly
  within a single unified optimization framework, we can learn better metrics
  to fuse the multi-sensor data for a joint classification. Furthermore, we
  also exploit multi-metric learning in a kernel induced feature space to
  capture the non-linearity in the original feature space via kernel mapping.
  ?? 2012 Elsevier B.V. All rights reserved.%
  }
  \verb{doi}
  \verb 10.1016/j.inffus.2012.05.002
  \endverb
  \field{issn}{15662535}
  \field{number}{4}
  \field{pages}{431\bibrangedash 440}
  \field{title}{{Multi-metric learning for multi-sensor fusion based
  classification}}
  \field{volume}{14}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/Desktop/Haicho{\_}Inforamtion Fusion{\
  \verb _}2012{\_}MultiMetric.pdf:pdf
  \endverb
  \field{journaltitle}{Information Fusion}
  \field{year}{2013}
\endentry

\entry{Zhu2007}{article}{}
  \name{author}{1}{}{%
    {{}%
     {Zhu}{Z.}%
     {X.}{X.}%
     {}{}%
     {}{}}%
  }
  \keyw{learning}
  \strng{namehash}{ZX1}
  \strng{fullhash}{ZX1}
  \field{labelalpha}{Zhu07}
  \field{sortinit}{Z}
  \field{abstract}{%
  We review the literature on semi-supervised learning, which is an area in
  machine learning and more generally, artificial intelligence. There has been
  a whole spectrum of interesting ideas on how to learn from both labeled and
  unlabeled data, i.e. semi-supervised learning. This document originates as a
  chapter in the authors doctoral thesis (Zhu, 2005). However the author will
  update the online version regularly to incorporate the latest development in
  the field.%
  }
  \verb{doi}
  \verb 10.1.1.146.2352
  \endverb
  \field{isbn}{1530}
  \field{pages}{1\bibrangedash 59}
  \field{title}{{Semi-Supervised Learning Literature Survey}}
  \verb{url}
  \verb http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.146.2352{\&}re
  \verb p=rep1{\&}type=pdf
  \endverb
  \field{journaltitle}{Sciences-New York}
  \field{year}{2007}
\endentry

\entry{ZhangX.-L.Z.-G.Luo2014}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Zhang}{Z.}%
     {X.L.}{X.}%
     {}{}%
     {}{}}%
    {{}%
     {Luo}{L.}%
     {Z.G.}{Z.}%
     {}{}%
     {}{}}%
    {{}%
     {Li}{L.}%
     {M.}{M.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{ZXLZLM1}
  \strng{fullhash}{ZXLZLM1}
  \field{labelalpha}{ZLL14}
  \field{sortinit}{Z}
  \field{number}{6}
  \field{pages}{1072\bibrangedash 1082}
  \field{title}{{Merge-weighted dynamic time warping for speech recognition}}
  \field{volume}{29}
  \field{journaltitle}{Journal of Computer Science and Technology}
  \field{year}{2014}
\endentry

\entry{Zumel}{book}{}
  \name{author}{3}{}{%
    {{}%
     {Zumel}{Z.}%
     {N.}{N.}%
     {}{}%
     {}{}}%
    {{}%
     {Mount}{M.}%
     {J.}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Porzak}{P.}%
     {J.}{J.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{ZNMJPJ1}
  \strng{fullhash}{ZNMJPJ1}
  \field{labelalpha}{ZMP14}
  \field{sortinit}{Z}
  \field{isbn}{ISBN-13: 978-1617291562}
  \field{title}{{Practical Data Science with R}}
  \field{year}{2014}
\endentry

\entry{Zhang2010}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Zhang}{Z.}%
     {Y.}{Y.}%
     {}{}%
     {}{}}%
    {{}%
     {Yeung}{Y.}%
     {D.}{D.}%
     {}{}%
     {}{}}%
  }
  \keyw{metric learning,multi-task learning,transfer learning}
  \strng{namehash}{ZYYD1}
  \strng{fullhash}{ZYYD1}
  \field{labelalpha}{ZY10}
  \field{sortinit}{Z}
  \field{abstract}{%
  Distance metric learning plays a very crucial role in many data mining
  algorithms because the performance of an algorithm relies heavily on choosing
  a good metric. However, the labeled data available in many applications is
  scarce and hence the metrics learned are often unsatisfactory. In this paper,
  we consider a transfer learning setting in which some related source tasks
  with labeled data are available to help the learning of the target task. We
  first propose a convex formulation for multi-task metric learning by modeling
  the task relationships in the form of a task covariance matrix. Then we
  regard transfer learning as a special case of multitask learning and adapt
  the formulation of multi-task metric learning to the transfer learning
  setting for our method, called transfer metric learning (TML). In TML, we
  learn the metric and the task covariances between the source tasks and the
  target task under a unified convex formulation. To solve the convex
  optimization problem, we use an alternating method in which each subproblem
  has an efficient solution. Experimental results on some commonly used
  transfer learning applications demonstrate the effectiveness of our method.
  © 2010 ACM.%
  }
  \verb{doi}
  \verb 10.1145/1835804.1835954
  \endverb
  \field{isbn}{9781450300551}
  \field{pages}{1199}
  \field{title}{{Transfer metric learning by learning task relationships}}
  \verb{url}
  \verb http://dl.acm.org/citation.cfm?doid=1835804.1835954
  \endverb
  \field{journaltitle}{Proceedings of the 16th ACM SIGKDD international
  conference on Knowledge discovery and data mining - KDD '10}
  \field{year}{2010}
\endentry

\lossort
\endlossort

\endinput
