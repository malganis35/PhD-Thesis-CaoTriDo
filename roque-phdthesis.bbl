% $ biblatex auxiliary file $
% $ biblatex version 2.5 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated as
% required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup

\entry{Aizerman1964}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Aizerman}{A.}%
     {M.}{M.}%
     {}{}%
     {}{}}%
    {{}%
     {Braverman}{B.}%
     {E.}{E.}%
     {}{}%
     {}{}}%
    {{}%
     {Rozonoer}{R.}%
     {L.}{L.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{AMBERL1}
  \strng{fullhash}{AMBERL1}
  \field{labelalpha}{ABR64}
  \field{sortinit}{A}
  \field{abstract}{%
  Introduction of kernels%
  }
  \field{pages}{821\bibrangedash 837}
  \field{title}{{Theoretical foundations of the potential function method in
  pattern recognition learning}}
  \field{volume}{25}
  \field{journaltitle}{Automation and Remote Control}
  \field{year}{1964}
\endentry

\entry{Altman1992}{article}{}
  \name{author}{1}{}{%
    {{}%
     {Altman}{A.}%
     {Ns}{N.}%
     {}{}%
     {}{}}%
  }
  \keyw{Confidence intervals,Local linear re- gression,Model building,Model
  checking,Smoothing.}
  \strng{namehash}{AN1}
  \strng{fullhash}{AN1}
  \field{labelalpha}{Alt92}
  \field{sortinit}{A}
  \field{abstract}{%
  Nonparametric regression is a set of techniques for es- timating a regression
  curve without making strong as- sumptions about the shape of the true
  regression func- tion. These techniques are therefore useful for building and
  checking parametric models, as well as for data description. Kernel and
  nearest-neighbor regression es- timators are local versions of univariate
  location esti- mators, and so they can readily be introduced to be- ginning
  students and consulting clients who are familiar with such summaries as the
  sample mean and median.%
  }
  \verb{doi}
  \verb 10.1080/00031305.1992.10475879
  \endverb
  \field{isbn}{0003-1305}
  \field{issn}{0003-1305}
  \field{number}{3}
  \field{pages}{175\bibrangedash 185}
  \field{title}{{An introduction to kernel and nearest-neighbor nonparametric
  regression}}
  \field{volume}{46}
  \field{journaltitle}{The American Statistician}
  \field{year}{1992}
\endentry

\entry{Abraham2010a}{inproceedings}{}
  \name{author}{2}{}{%
    {{}%
     {Abraham}{A.}%
     {Z.}{Z.}%
     {}{}%
     {}{}}%
    {{}%
     {Tan}{T.}%
     {P.N.}{P.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{AZTP1}
  \strng{fullhash}{AZTP1}
  \field{labelalpha}{AT10}
  \field{extraalpha}{1}
  \field{sortinit}{A}
  \field{abstract}{%
  Zero-inflated time series data are commonly encountered in many applications,
  including climate and ecological modeling, disease monitoring, manufacturing
  defect detection, and traffic monitoring. Such data often leads to poor model
  fitting using standard regression methods because they tend to underestimate
  the frequency of zeros and the magnitude of non-zero values. This paper
  presents an integrated framework that simultaneously performs classification
  and regression to accurately predict future values of a zero-inflated time
  series. A regression model is initially applied to predict the value of the
  time series. The regression output is then fed into a classification model to
  determine whether the predicted value should be adjusted to zero. Our
  regression and classification models are trained to optimize a joint
  objective function that considers both classification errors on the time
  series and regression errors on data points that have non-zero values. We
  demonstrate the effectiveness of our framework in the context of its
  application to a precipitation downscaling problem for climate impact
  assessment studies. Read More:
  http://epubs.siam.org/doi/abs/10.1137/1.9781611972801.57%
  }
  \field{booktitle}{ACM SIGKDD}
  \field{title}{{An Integrated Framework for Simultaneous Classification and
  Regression of Time-Series Data}}
  \verb{url}
  \verb https://siam.org/proceedings/datamining/2010/dm10{\_}057{\_}abrahamz.pd
  \verb f
  \endverb
  \field{year}{2010}
\endentry

\entry{Abraham2010b}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Abraham}{A.}%
     {Zubin}{Z.}%
     {}{}%
     {}{}}%
    {{}%
     {Tan}{T.}%
     {PN}{P.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{AZTP2}
  \strng{fullhash}{AZTP2}
  \field{labelalpha}{AT10}
  \field{extraalpha}{2}
  \field{sortinit}{A}
  \field{abstract}{%
  Zero-inflated time series data are commonly encountered in many applications,
  including climate and ecological modeling, disease monitoring, manufacturing
  defect detection, and traffic monitoring. Such data often leads to poor model
  fitting using standard regression methods because they tend to underestimate
  the frequency of zeros and the magnitude of non-zero values. This paper
  presents an integrated framework that simultaneously performs classification
  and regression to accurately predict future values of a zero-inflated time
  series. A regression model is initially applied to predict the value of the
  time series. The regression output is then fed into a classification model to
  determine whether the predicted value should be adjusted to zero. Our
  regression and classification models are trained to optimize a joint
  objective function that considers both classification errors on the time
  series and regression errors on data points that have non-zero values. We
  demonstrate the effectiveness of our framework in the context of its
  application to a precipitation downscaling problem for climate impact
  assessment studies. Read More:
  http://epubs.siam.org/doi/abs/10.1137/1.9781611972801.57%
  }
  \field{isbn}{978-0-89871-703-7}
  \field{pages}{653\bibrangedash 664}
  \field{title}{{An Integrated Framework for Simultaneous Classification and
  Regression of Time-Series Data}}
  \verb{url}
  \verb https://siam.org/proceedings/datamining/2010/dm10{\_}057{\_}abrahamz.pd
  \verb f
  \endverb
  \field{journaltitle}{Proc of the ACM SIGKDD Int'l Conf on Data Mining}
  \field{year}{2010}
\endentry

\entry{Berndt1994a}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Berndt}{B.}%
     {Donald}{D.}%
     {}{}%
     {}{}}%
    {{}%
     {Clifford}{C.}%
     {James}{J.}%
     {}{}%
     {}{}}%
  }
  \keyw{dynamic programming,dynamic time warping,knowledge discovery,pat,tern
  analysis,time series}
  \strng{namehash}{BDCJ1}
  \strng{fullhash}{BDCJ1}
  \field{labelalpha}{BC94}
  \field{extraalpha}{1}
  \field{sortinit}{B}
  \field{abstract}{%
  Knowledge discovery in databases presents many interesting challenges within
  the context of providing computer tools for exploring large data archives.
  Electronic data repositories are growing qulckiy and contain data from
  commercial, scientific, and other domains. Much of this data is inherently
  temporal, such as stock prices or NASA telemetry data. Detecting patterns in
  such data streams or time series is an important knowledge discovery task.
  This paper describes some primary experiments with a dynamic programming
  approach to the problem. The pattern detection algorithm is based on the
  dynamic time warping technique used in the speech recognition field.
  Keywords: dynamic programming, dynamic time warping, knowledge discovery,
  pattern analysis, time series.%
  }
  \field{pages}{359\bibrangedash 370}
  \field{title}{{Using dynamic time warping to find patterns in time series}}
  \verb{url}
  \verb http://www.aaai.org/Papers/Workshops/1994/WS-94-03/WS94-03-031.pdf
  \endverb
  \field{volume}{398}
  \field{journaltitle}{Workshop on Knowledge Knowledge Discovery in Databases}
  \field{year}{1994}
\endentry

\entry{Berndt1994c}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Berndt}{B.}%
     {Donald}{D.}%
     {}{}%
     {}{}}%
    {{}%
     {Clifford}{C.}%
     {James}{J.}%
     {}{}%
     {}{}}%
  }
  \keyw{dynamic programming,dynamic time warping,knowledge discovery,pat,tern
  analysis,time series}
  \strng{namehash}{BDCJ1}
  \strng{fullhash}{BDCJ1}
  \field{labelalpha}{BC94}
  \field{extraalpha}{2}
  \field{sortinit}{B}
  \field{abstract}{%
  Knowledge discovery in databases presents many interesting challenges within
  the context of providing computer tools for exploring large data archives.
  Electronic data repositories are growing qulckiy and contain data from
  commercial, scientific, and other domains. Much of this data is inherently
  temporal, such as stock prices or NASA telemetry data. Detecting patterns in
  such data streams or time series is an important knowledge discovery task.
  This paper describes some primary experiments with a dynamic programming
  approach to the problem. The pattern detection algorithm is based on the
  dynamic time warping technique used in the speech recognition field.
  Keywords: dynamic programming, dynamic time warping, knowledge discovery,
  pattern analysis, time series.%
  }
  \field{pages}{359\bibrangedash 370}
  \field{title}{{Using dynamic time warping to find patterns in time series}}
  \verb{url}
  \verb http://www.aaai.org/Papers/Workshops/1994/WS-94-03/WS94-03-031.pdf
  \endverb
  \field{volume}{398}
  \field{journaltitle}{Workshop on Knowledge Knowledge Discovery in Databases}
  \field{year}{1994}
\endentry

\entry{Benesty2009}{article}{}
  \name{author}{4}{}{%
    {{}%
     {Benesty}{B.}%
     {J.}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Chen}{C.}%
     {J.}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Huang}{H.}%
     {Y.}{Y.}%
     {}{}%
     {}{}}%
    {{}%
     {Cohen}{C.}%
     {I.}{I.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{BJ+1}
  \strng{fullhash}{BJCJHYCI1}
  \field{labelalpha}{Ben+09}
  \field{sortinit}{B}
  \field{abstract}{%
  This chapter develops several forms of the Pearson correlation coefficient in
  the different domains. This coefficient can be used as an optimization
  criterion to derive different optimal noise reduction filters [14], but is
  even more useful for analyzing these optimal filters for their noise
  reduction performance.%
  }
  \verb{doi}
  \verb 10.1007/978-3-642-00296-0
  \endverb
  \field{isbn}{978-3-642-00295-3}
  \field{title}{{Pearson correlation coefficient}}
  \verb{url}
  \verb http://www.springerlink.com/index/10.1007/978-3-642-00296-0$\backslash$
  \verb nhttp://link.springer.com/content/pdf/10.1007/978-3-642-00296-0{\_}5.pd
  \verb f
  \endverb
  \field{journaltitle}{Noise Reduction in Speech Processing}
  \field{year}{2009}
\endentry

\entry{Boser1992}{inproceedings}{}
  \name{author}{3}{}{%
    {{}%
     {Boser}{B.}%
     {Bernhard~E.}{B.~E.}%
     {}{}%
     {}{}}%
    {{}%
     {Guyon}{G.}%
     {Isabelle~M.}{I.~M.}%
     {}{}%
     {}{}}%
    {{}%
     {Vapnik}{V.}%
     {Vladimir~N.}{V.~N.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{BBEGIMVVN1}
  \strng{fullhash}{BBEGIMVVN1}
  \field{labelalpha}{BGV92}
  \field{sortinit}{B}
  \field{abstract}{%
  A training algorithm that maximizes the margin between the training patterns
  and the decision boundary is presented. The technique is applicable to a wide
  variety of classifiaction functions, including Perceptrons, polynomials, and
  Radial Basis Functions. The effective number of parameters is adjusted
  automatically to match the complexity of the problem. The solution is
  expressed as a linear combination of supporting patterns. These are the
  subset of training patterns that are closest to the decision boundary. Bounds
  on the generalization performance based on the leave-one-out method and the
  VC-dimension are given. Experimental results on optical character recognition
  problems demonstrate the good generalization obtained when compared with
  other learning algorithms. 1 INTRODUCTION Good generalization performance of
  pattern classifiers is achieved when the capacity of the classification
  function is matched to the size of the training set. Classifiers with a large
  numb...%
  }
  \field{booktitle}{Proceedings of the 5th Annual ACM Workshop on Computational
  Learning Theory}
  \verb{doi}
  \verb 10.1.1.21.3818
  \endverb
  \field{isbn}{089791497X}
  \field{issn}{0-89791-497-X}
  \field{pages}{144\bibrangedash 152}
  \field{title}{{A Training Algorithm for Optimal Margin Classifiers}}
  \verb{url}
  \verb http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.21.3818
  \endverb
  \field{year}{1992}
\endentry

\entry{Bellet2012}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Bellet}{B.}%
     {Aur{\'{e}}lien}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Habrard}{H.}%
     {Amaury}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Sebban}{S.}%
     {Marc}{M.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{BAHASM1}
  \strng{fullhash}{BAHASM1}
  \field{labelalpha}{BHS12}
  \field{sortinit}{B}
  \field{abstract}{%
  Similarity functions are a fundamental component of many learning algorithms.
  When dealing with string or tree-structured data, measures based on the edit
  distance are widely used, and there exist a few methods for learning them
  from data. However, these methods offer no theoretical guarantee as to the
  generalization ability and discriminative power of the learned similarities.
  In this paper, we propose an approach to edit similarity learning based on
  loss minimization, called GESL. It is driven by the notion of
  (ϵ,$\gamma$,$\tau$)-goodness, a theory that bridges the gap between the
  properties of a similarity function and its performance in classification.
  Using the notion of uniform stability, we derive generalization guarantees
  that hold for a large class of loss functions. We also provide experimental
  results on two real-world datasets which show that edit similarities learned
  with GESL induce more accurate and sparser classifiers than other (standard
  or learned) edit similarities.%
  }
  \verb{doi}
  \verb 10.1007/s10994-012-5293-8
  \endverb
  \field{isbn}{0885-6125}
  \field{issn}{0885-6125, 1573-0565}
  \field{number}{1-2}
  \field{pages}{5\bibrangedash 35}
  \field{title}{{Good edit similarity learning by loss minimization}}
  \verb{url}
  \verb http://link.springer.com/article/10.1007/s10994-012-5293-8$\backslash$n
  \verb http://link.springer.com/article/10.1007{\%}2Fs10994-012-5293-8$\backsl
  \verb ash$nhttp://link.springer.com/content/pdf/10.1007{\%}2Fs10994-012-5293-
  \verb 8.pdf
  \endverb
  \field{volume}{89}
  \field{journaltitle}{Machine Learning}
  \field{year}{2012}
\endentry

\entry{Bellet2013a}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Bellet}{B.}%
     {Aur{\'{e}}lien}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Habrard}{H.}%
     {Amaury}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Sebban}{S.}%
     {Marc}{M.}%
     {}{}%
     {}{}}%
  }
  \keyw{(),edit distance,mahalanobis distance,metric learning,similarity
  learning}
  \strng{namehash}{BAHASM1}
  \strng{fullhash}{BAHASM1}
  \field{labelalpha}{BHS13}
  \field{sortinit}{B}
  \field{abstract}{%
  The need for appropriate ways to measure the distance or similarity between
  data is ubiquitous in machine learning, pattern recognition and data mining,
  but handcrafting such good metrics for specific problems is generally
  difficult. This has led to the emergence of metric learning, which aims at
  automatically learning a metric from data and has attracted a lot of interest
  in machine learning and related fields for the past ten years. This survey
  paper proposes a systematic review of the metric learning literature,
  highlighting the pros and cons of each approach. We pay particular attention
  to Mahalanobis distance metric learning, a well-studied and successful
  framework, but additionally present a wide range of methods that have
  recently emerged as powerful alternatives, including nonlinear metric
  learning, similarity learning and local metric learning. Recent trends and
  extensions, such as semi-supervised metric learning, metric learning for
  histogram data and the derivation of generalization guarantees, are also
  covered. Finally, this survey addresses metric learning for structured data,
  in particular edit distance learning, and attempts to give an overview of the
  remaining challenges in metric learning for the years to come.%
  }
  \verb{doi}
  \verb 10.1073/pnas.0809777106
  \endverb
  \verb{eprint}
  \verb 1306.6709
  \endverb
  \field{issn}{00401951}
  \field{pages}{57}
  \field{title}{{A Survey on Metric Learning for Feature Vectors and Structured
  Data}}
  \verb{url}
  \verb http://arxiv.org/abs/1306.6709
  \endverb
  \field{journaltitle}{arXiv preprint arXiv:1306.6709}
  \field{eprinttype}{arXiv}
  \field{year}{2013}
\endentry

\entry{Bishop2006}{book}{}
  \name{author}{1}{}{%
    {{}%
     {Bishop}{B.}%
     {Christopher~M}{C.~M.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{BCM1}
  \strng{fullhash}{BCM1}
  \field{labelalpha}{Bis06}
  \field{sortinit}{B}
  \field{abstract}{%
  The dramatic growth in practical applications for machine learning over the
  last ten years has been accompanied by many important developments in the
  underlying algorithms and techniques. For example, Bayesian methods have
  grown from a specialist niche to become mainstream, while graphical models
  have emerged as a general framework for describing and applying probabilistic
  techniques. The practical applicability of Bayesian methods has been greatly
  enhanced by the development of a range of approximate inference algorithms
  such as variational Bayes and expectation propagation, while new models based
  on kernels have had a significant impact on both algorithms and applications.
  This completely new textbook reflects these recent developments while
  providing a comprehensive introduction to the fields of pattern recognition
  and machine learning. It is aimed at advanced undergraduates or first-year
  PhD students, as well as researchers and practitioners. No previous knowledge
  of pattern recognition or machine learning concepts is assumed. Familiarity
  with multivariate calculus and basic linear algebra is required, and some
  experience in the use of probabilities would be helpful though not essential
  as the book includes a self-contained introduction to basic probability
  theory. The book is suitable for courses on machine learning, statistics,
  computer science, signal processing, computer vision, data mining, and
  bioinformatics. Extensive support is provided for course instructors,
  including more than 400 exercises, graded according to difficulty. Example
  solutions for a subset of the exercises are available from the book web site,
  while solutions for the remainder can be obtained by instructors from the
  publisher. The book is supported by a great deal of additional material, and
  the reader is encouraged to visit the book web site for the latest
  information. A forthcoming companion volume will deal with practical aspects
  of pattern recognition and machine learning, and will include free software
  implementations of the key algorithms along with example data sets and
  demonstration programs. Christopher Bishop is Assistant Director at Microsoft
  Research Cambridge, and also holds a Chair in Computer Science at the
  University of Edinburgh. He is a Fellow of Darwin College Cambridge, and was
  recently elected Fellow of the Royal Academy of Engineering. The author's
  previous textbook "Neural Networks for Pattern Recognition" has been widely
  adopted.%
  }
  \field{booktitle}{Pattern Recognition}
  \verb{doi}
  \verb 10.1117/1.2819119
  \endverb
  \verb{eprint}
  \verb 0-387-31073-8
  \endverb
  \field{isbn}{9780387310732}
  \field{issn}{10179909}
  \field{number}{4}
  \field{pages}{738}
  \field{title}{{Pattern Recognition and Machine Learning}}
  \verb{url}
  \verb http://www.library.wisc.edu/selectedtocs/bg0137.pdf
  \endverb
  \field{volume}{4}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/Google Drive/Th{\`{e}}se/Bibliographie
  \verb /Livre/Bishop - Pattern Recognition and Machine Learning.pdf:pdf
  \endverb
  \field{eprinttype}{arXiv}
  \field{year}{2006}
\endentry

\entry{Brigham1967}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Brigham}{B.}%
     {E.~O.}{E.~O.}%
     {}{}%
     {}{}}%
    {{}%
     {Morrow}{M.}%
     {R.~E.}{R.~E.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{BEOMRE1}
  \strng{fullhash}{BEOMRE1}
  \field{labelalpha}{BM67}
  \field{sortinit}{B}
  \field{abstract}{%
  The fast Fourier transform (FFT), a computer algorithm that computes the
  discrete Fourier transform much faster than other algorithms, is explained.
  Examples and detailed procedures are provided to assist the reader in
  learning how to use the algorithm. The savings in computer time can be huge;
  for example, an N = 210-point transform can be computed with the FFT 100
  times faster than with the use of a direct approach.%
  }
  \verb{doi}
  \verb 10.1109/MSPEC.1967.5217220
  \endverb
  \field{isbn}{0018-9235}
  \field{issn}{0018-9235}
  \field{number}{12}
  \field{pages}{63 \bibrangedash 70}
  \field{title}{{The fast Fourier transform}}
  \verb{url}
  \verb http://ieeexplore.ieee.org/ielx5/6/5217195/05217220.pdf?tp={\&}arnumber
  \verb =5217220{\&}isnumber=5217195$\backslash$nhttp://ieeexplore.ieee.org/sta
  \verb mp/stamp.jsp?tp={\&}arnumber=5217220
  \endverb
  \field{volume}{4}
  \field{journaltitle}{Spectrum, IEEE}
  \field{year}{1967}
\endentry

\entry{Belongie2002}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Belongie}{B.}%
     {Serge}{S.}%
     {}{}%
     {}{}}%
    {{}%
     {Malik}{M.}%
     {Jitendra}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Puzicha}{P.}%
     {Jan}{J.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{BSMJPJ1}
  \strng{fullhash}{BSMJPJ1}
  \field{labelalpha}{BMP02}
  \field{sortinit}{B}
  \field{abstract}{%
  We present a novel approach to measuring similarity between shapes and
  exploit it for object recognition. In our framework, the measurement of
  similarity is preceded by 1) solving for correspondences between points on
  the two shapes, 2) using the correspondences to estimate an aligning
  transform. In order to solve the correspondence problem, we attach a
  descriptor, the shape context, to each point. The shape context at a
  reference point captures the distribution of the remaining points relative to
  it, thus offering a globally discriminative characterization. Corresponding
  points on two similar shapes will have similar shape contexts, enabling us to
  solve for correspondences as an optimal assignment problem. Given the point
  correspondences, we estimate the transformation that best aligns the two
  shapes; regularized thin-plate splines provide a flexible class of
  transformation maps for this purpose. The dissimilarity between the two
  shapes is computed as a sum of matching errors between corresponding points,
  together with a term measuring the magnitude of the aligning transform. We
  treat recognition in a nearest-neighbor classification framework as the
  problem of finding the stored prototype shape that is maximally similar to
  that in the image. Results are presented for silhouettes, trademarks,
  handwritten digits, and the COIL data set.%
  }
  \verb{doi}
  \verb 10.1.1.18.8852
  \endverb
  \field{isbn}{9781424455409}
  \field{issn}{01628828}
  \field{pages}{509\bibrangedash 522}
  \field{title}{{Shape Matching and Object Recognition Using Shape Contexts}}
  \verb{url}
  \verb http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.18.8852
  \endverb
  \field{volume}{24}
  \field{journaltitle}{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}
  \field{year}{2002}
\endentry

\entry{Caiado2006c}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Caiado}{C.}%
     {Jorge}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Crato}{C.}%
     {Nuno}{N.}%
     {}{}%
     {}{}}%
    {{}%
     {Pe{\~{n}}a}{P.}%
     {Daniel}{D.}%
     {}{}%
     {}{}}%
  }
  \keyw{Autocorrelation function,Classification,Clustering,Euclidean
  distance,Periodogram,Stationary and non-stationary time series}
  \strng{namehash}{CJCNPD1}
  \strng{fullhash}{CJCNPD1}
  \field{labelalpha}{CCP06}
  \field{sortinit}{C}
  \field{abstract}{%
  The statistical discrimination and clustering literature has studied the
  problem of identifying similarities in time series data. Some studies use
  non-parametric approaches for splitting a set of time series into clusters by
  looking at their Euclidean distances in the space of points. A new measure of
  distance between time series based on the normalized periodogram is proposed.
  Simulation results comparing this measure with others parametric and
  non-parametric metrics are provided. In particular, the classification of
  time series as stationary or as non-stationary is discussed. The use of both
  hierarchical and non-hierarchical clustering algorithms is considered. An
  illustrative example with economic time series data is also presented. ©
  2005 Elsevier B.V. All rights reserved.%
  }
  \verb{doi}
  \verb 10.1016/j.csda.2005.04.012
  \endverb
  \field{isbn}{01679473}
  \field{issn}{01679473}
  \field{number}{10}
  \field{pages}{2668\bibrangedash 2684}
  \field{title}{{A periodogram-based metric for time series classification}}
  \field{volume}{50}
  \field{journaltitle}{Computational Statistics and Data Analysis}
  \field{year}{2006}
\endentry

\entry{Cover1967b}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Cover}{C.}%
     {T.}{T.}%
     {}{}%
     {}{}}%
    {{}%
     {Hart}{H.}%
     {P.}{P.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{CTHP1}
  \strng{fullhash}{CTHP1}
  \field{labelalpha}{CH67}
  \field{sortinit}{C}
  \field{abstract}{%
  The nearest neighbor decision rule assigns to an unclassified sample point
  the classification of the nearest of a set of previously classified points.
  This rule is independent of the underlying joint distribution on the sample
  points and their classifications, and hence the probability of
  error<tex>R</tex>of such a rule must be at least as great as the Bayes
  probability of error<tex>R{\^{}}{\{}ast{\}}</tex>--the minimum probability of
  error over all decision rules taking underlying probability structure into
  account. However, in a large sample analysis, we will show in
  the<tex>M</tex>-category case that<tex>R{\^{}}{\{}ast{\}} leq R leq
  R{\^{}}{\{}ast{\}}(2 --MR{\^{}}{\{}ast{\}}/(M-1))</tex>, where these bounds
  are the tightest possible, for all suitably smooth underlying distributions.
  Thus for any number of categories, the probability of error of the nearest
  neighbor rule is bounded above by twice the Bayes probability of error. In
  this sense, it may be said that half the classification information in an
  infinite sample set is contained in the nearest neighbor.%
  }
  \verb{doi}
  \verb 10.1109/TIT.1967.1053964
  \endverb
  \field{isbn}{0018-9448}
  \field{issn}{0018-9448}
  \field{number}{1}
  \field{pages}{21\bibrangedash 27}
  \field{title}{{Nearest neighbor pattern classification}}
  \field{volume}{13}
  \field{journaltitle}{IEEE Transactions on Information Theory}
  \field{year}{1967}
\endentry

\entry{Chatpatanasiri2010}{article}{}
  \name{author}{4}{}{%
    {{}%
     {Chatpatanasiri}{C.}%
     {Ratthachat}{R.}%
     {}{}%
     {}{}}%
    {{}%
     {Korsrilabutr}{K.}%
     {Teesid}{T.}%
     {}{}%
     {}{}}%
    {{}%
     {Tangchanachaianan}{T.}%
     {Pasakorn}{P.}%
     {}{}%
     {}{}}%
    {{}%
     {Kijsirikul}{K.}%
     {Boonserm}{B.}%
     {}{}%
     {}{}}%
  }
  \keyw{Dimensionality reduction,Distance metric learning,Kernel
  alignment,Kernel machines,Representer theorem}
  \strng{namehash}{CR+1}
  \strng{fullhash}{CRKTTPKB1}
  \field{labelalpha}{Cha+10}
  \field{sortinit}{C}
  \field{abstract}{%
  This paper focuses on developing a new framework of kernelizing Mahalanobis
  distance learners. The new KPCA trick framework offers several practical
  advantages over the classical kernel trick framework, e.g. no mathematical
  formulas and no reprogramming are required for a kernel implementation, a way
  to speed up an algorithm is provided with no extra work, the framework avoids
  troublesome problems such as singularity. Rigorous representer theorems in
  countably infinite dimensional spaces are given to validate our framework.
  Furthermore, unlike previous works which always apply brute force methods to
  select a kernel, we derive a kernel alignment formula based on quadratic
  programming which can efficiently construct an appropriate kernel for a given
  dataset. ?? 2010.%
  }
  \verb{doi}
  \verb 10.1016/j.neucom.2009.11.037
  \endverb
  \field{issn}{09252312}
  \field{number}{10-12}
  \field{pages}{1570\bibrangedash 1579}
  \field{title}{{A new kernelization framework for Mahalanobis distance
  learning algorithms}}
  \verb{url}
  \verb http://dx.doi.org/10.1016/j.neucom.2009.11.037$\backslash$nhttp://ac.el
  \verb s-cdn.com/S0925231210001165/1-s2.0-S0925231210001165-main.pdf?{\_}tid=9
  \verb 23c9f62-a756-11e4-970c-00000aab0f6b{\&}acdnat=1422495279{\_}136843023ec
  \verb d8d5dc1ba9d83e2539013
  \endverb
  \field{volume}{73}
  \field{journaltitle}{Neurocomputing}
  \field{year}{2010}
\endentry

\entry{Chatfield2004}{book}{}
  \name{author}{1}{}{%
    {{}%
     {Chatfield}{C.}%
     {Christopher}{C.}%
     {}{}%
     {}{}}%
  }
  \keyw{Time-series analysis.}
  \strng{namehash}{CC1}
  \strng{fullhash}{CC1}
  \field{labelalpha}{Cha04}
  \field{sortinit}{C}
  \field{abstract}{%
  "Since 1975, The Analysis of Time Series: An Introduction has introduced
  legions of statistics students and researchers to the theory and practice of
  time series analysis. The sixth edition provides an accessible, comprehensive
  introduction to the theory and practice of time series analysis. The
  treatment covers a wide range of topics, including ARIMA probability models,
  forecasting methods, spectral analysis, linear systems, state-space models,
  and the Kalman filter. It also addresses nonlinear, multivariate, and
  long-memory models. The author has carefully updated each chapter, added new
  discussions, incorporated new datasets, and made those datasets available at
  www.crcpress.com."--BOOK JACKET.%
  }
  \field{booktitle}{Texts in statistical science}
  \field{isbn}{1584883170}
  \field{pages}{xiii, 333 p.}
  \field{title}{{The analysis of time series : an introduction}}
  \field{year}{2004}
\endentry

\entry{Chopra2005}{inproceedings}{}
  \name{author}{3}{}{%
    {{}%
     {Chopra}{C.}%
     {Sumit}{S.}%
     {}{}%
     {}{}}%
    {{}%
     {Hadsell}{H.}%
     {Raia}{R.}%
     {}{}%
     {}{}}%
    {{}%
     {LeCun}{L.}%
     {Yann}{Y.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{CSHRLY1}
  \strng{fullhash}{CSHRLY1}
  \field{labelalpha}{CHL05}
  \field{sortinit}{C}
  \field{abstract}{%
  We present a method for training a similarity metric from data. The method
  can be used for recognition or verification applications where the number of
  categories is very large and not known during training, and where the number
  of training samples for a single category is very small. The idea is to learn
  a function that maps input patterns into a target space such that the L1 norm
  in the target space approximates the "semantic" distance in the input space.
  The method is applied to a face verification task. The learning process
  minimizes a discriminative loss function that drives the similarity metric to
  be small for pairs of faces from the same person, and large for pairs from
  different persons. The mapping from raw to the target space is a
  convolutional network whose architecture is designed for robustness to
  geometric distortions. The system is tested on the Purdue/AR face database
  which has a very high degree of variability in the pose, lighting,
  expression, position, and artificial occlusions such as dark glasses and
  obscuring scarves.%
  }
  \field{booktitle}{CVPR}
  \verb{doi}
  \verb 10.1109/CVPR.2005.202
  \endverb
  \field{isbn}{0769523722}
  \field{issn}{10636919}
  \field{pages}{539\bibrangedash 546}
  \field{title}{{Learning a similarity metric discriminatively, with
  application to face verification}}
  \field{volume}{1}
  \field{year}{2005}
\endentry

\entry{Chen1996}{misc}{}
  \name{author}{3}{}{%
    {{}%
     {Chen}{C.}%
     {Ming~Syan}{M.~S.}%
     {}{}%
     {}{}}%
    {{}%
     {Han}{H.}%
     {Jiawei}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Yu}{Y.}%
     {Philip~S.}{P.~S.}%
     {}{}%
     {}{}}%
  }
  \keyw{Association rules,Classification,Data clustering,Data cubes,Data
  generalization and characterization,Data mining,Knowledge
  discovery,Multiple-dimensional databases,Pattern matching algorithms}
  \strng{namehash}{CMSHJYPS1}
  \strng{fullhash}{CMSHJYPS1}
  \field{labelalpha}{CHY96}
  \field{sortinit}{C}
  \field{abstract}{%
  Mining information and knowledge from large databases has been recognized by
  many researchers as a key research topic in database systems and machine
  learning, and by many industrial companies as an important area with an
  opportunity of major revenues. Researchers in many different fields have
  shown great interest in data mining. Several emerging applications in
  information-providing services, such as data warehousing and online services
  over the Internet, also call for various data mining techniques to better
  understand user behavior, to improve the service provided and to increase
  business opportunities. In response to such a demand, this article provides a
  survey, from a database researcher's point of view, on the data mining
  techniques developed recently. A classification of the available data mining
  techniques is provided and a comparative study of such techniques is
  presented%
  }
  \field{booktitle}{IEEE Transactions on Knowledge and Data Engineering}
  \verb{doi}
  \verb 10.1109/69.553155
  \endverb
  \field{isbn}{1041-4347}
  \field{issn}{10414347}
  \field{number}{6}
  \field{pages}{866\bibrangedash 883}
  \field{title}{{Data mining: An Overview from a Database Perspective}}
  \field{volume}{8}
  \field{year}{1996}
\endentry

\entry{Cochran1977}{article}{}
  \name{author}{1}{}{%
    {{}%
     {Cochran}{C.}%
     {William~C}{W.~C.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{CWC1}
  \strng{fullhash}{CWC1}
  \field{labelalpha}{Coc77}
  \field{sortinit}{C}
  \field{abstract}{%
  Commentary by : Cochran William C. Current Contents : {\#}19, May 9, 1977%
  }
  \field{pages}{1}
  \field{title}{{Snedecor G W {\&} Cochran W G. Statistical methods applied to
  experiments in agriculture and biology. 5th ed. Ames, Iowa: Iowa State
  University Press, 1956.}}
  \verb{url}
  \verb papers3://publication/uuid/8C5C843E-F853-4CB4-82BC-1141F0C01CB4
  \endverb
  \field{volume}{19}
  \field{journaltitle}{Citation Classics}
  \field{year}{1977}
\endentry

\entry{Crammer2001}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Crammer}{C.}%
     {Koby}{K.}%
     {}{}%
     {}{}}%
    {{}%
     {Singer}{S.}%
     {Yoram}{Y.}%
     {}{}%
     {}{}}%
  }
  \keyw{kernel machines,multiclass problems,svm}
  \strng{namehash}{CKSY1}
  \strng{fullhash}{CKSY1}
  \field{labelalpha}{CS01}
  \field{sortinit}{C}
  \field{abstract}{%
  In this paper we describe the algorithmic implementation of multiclass
  kernel-based vector machines. Our starting point is a generalized notion of
  the margin to multiclass problems. Using this notion we cast multiclass
  categorization problems as a constrained optimization problem with a
  quadratic objective function. Unlike most of previous approaches which
  typically decompose a multiclass problem into multiple independent binary
  classification tasks, our notion of margin yields a direct method for
  training multiclass predictors. By using the dual of the optimization problem
  we are able to incorporate kernels with a compact set of constraints and
  decompose the dual problem into multiple optimization problems of reduced
  size. We describe an efficient fixed-point algorithm for solving the reduced
  optimization problems and prove its convergence. We then discuss technical
  details that yield significant running time improvements for large datasets.
  Finally, we describe various experiments with our approach comparing it to
  previously studied kernel-based methods. Our experiments indicate that for
  multiclass problems we attain state-of-the-art accuracy%
  }
  \verb{doi}
  \verb 10.1162/15324430260185628
  \endverb
  \field{isbn}{1532-4435}
  \field{issn}{15324435}
  \field{pages}{265\bibrangedash 292}
  \field{title}{{On the Algorithmic Implementation of Multiclass Kernel-based
  Vector Machines}}
  \verb{url}
  \verb http://machinelearning.wustl.edu/mlpapers/paper{\_}files/CrammerS01.pdf
  \verb $\backslash$nhttp://www.jmlr.org/papers/volume2/crammer01a/crammer01a.p
  \verb df
  \endverb
  \field{volume}{2}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley D
  \verb esktop/Downloaded/Crammer, Singer - 2001 - On the Algorithmic Implement
  \verb ation of Multiclass Kernel-based Vector Machines.pdf:pdf
  \endverb
  \field{journaltitle}{Journal of Machine Learning Research}
  \field{year}{2001}
\endentry

\entry{Cao2001}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Cao}{C.}%
     {Lijuan}{L.}%
     {}{}%
     {}{}}%
    {{}%
     {Tay}{T.}%
     {Francis E~H}{F.~E.~H.}%
     {}{}%
     {}{}}%
  }
  \keyw{back propagation
  algorithm,financial,generalisation,multi-layer,perceptron,support vector
  machines,time series forecasting}
  \strng{namehash}{CLTFEH1}
  \strng{fullhash}{CLTFEH1}
  \field{labelalpha}{CT01}
  \field{sortinit}{C}
  \field{pages}{184\bibrangedash 192}
  \field{title}{{Financial Forecasting Using Support Vector Machines}}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley D
  \verb esktop/Downloaded/Cao, Tay - 2001 - Financial Forecasting Using Support
  \verb  Vector Machines.pdf:pdf
  \endverb
  \field{journaltitle}{Neural Computing {\&} Applications}
  \field{year}{2001}
\endentry

\entry{Cortes1995}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Cortes}{C.}%
     {Corinna}{C.}%
     {}{}%
     {}{}}%
    {{}%
     {Vapnik}{V.}%
     {Vladimir}{V.}%
     {}{}%
     {}{}}%
  }
  \keyw{efficient learning algorithms,neural networks,pattern
  recognition,polynomial classifiers,radial basis function classifiers}
  \strng{namehash}{CCVV1}
  \strng{fullhash}{CCVV1}
  \field{labelalpha}{CV95}
  \field{sortinit}{C}
  \field{abstract}{%
  The support-vector network is a new leaming machine for two-group
  classification problems. The machine conceptually implements the following
  idea: input vectors are non-linearly mapped to a very high- dimension feature
  space. In this feature space a linear decision surface is constructed.
  Special properties of the decision surface ensures high generalization
  ability of the learning machine. The idea behind the support-vector network
  was previously implemented for the restricted case where the training data
  can be separated without errors. We here extend this result to non-separable
  training data. High generalization ability of support-vector networks
  utilizing polynomial input transformations is demon- strated. We also compare
  the performance of the support-vector network to various classical learning
  algorithms that all took part in a benchmark study of Optical Character
  Recognition.%
  }
  \verb{doi}
  \verb 10.1007/BF00994018
  \endverb
  \verb{eprint}
  \verb arXiv:1011.1669v3
  \endverb
  \field{isbn}{0885-6125}
  \field{issn}{08856125}
  \field{number}{3}
  \field{pages}{273\bibrangedash 297}
  \field{title}{{Support-vector networks}}
  \field{volume}{20}
  \field{journaltitle}{Machine Learning}
  \field{eprinttype}{arXiv}
  \field{year}{1995}
\endentry

\entry{Campbell2011}{book}{}
  \name{author}{2}{}{%
    {{}%
     {Campbell}{C.}%
     {Colin}{C.}%
     {}{}%
     {}{}}%
    {{}%
     {Ying}{Y.}%
     {Yiming}{Y.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{CCYY1}
  \strng{fullhash}{CCYY1}
  \field{labelalpha}{CY11}
  \field{sortinit}{C}
  \field{booktitle}{Synthesis Lectures on Artificial Intelligence and Machine
  Learning}
  \verb{doi}
  \verb 10.2200/S00324ED1V01Y201102AIM010
  \endverb
  \field{isbn}{9781608456161}
  \field{issn}{1939-4608}
  \field{number}{1}
  \field{pages}{1\bibrangedash 95}
  \field{title}{{Learning with Support Vector Machines}}
  \verb{url}
  \verb http://www.morganclaypool.com/doi/abs/10.2200/S00324ED1V01Y201102AIM010
  \endverb
  \field{volume}{5}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley D
  \verb esktop/Downloaded/Campbell, Ying - 2011 - Learning with Support Vector
  \verb Machines.pdf:pdf
  \endverb
  \field{year}{2011}
  \warn{\item Invalid format of field 'month'}
\endentry

\entry{AhlameDouzal-Chouakria2011}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Douzal-Chouakria}{D.-C.}%
     {A.}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Amblard}{A.}%
     {C.}{C.}%
     {}{}%
     {}{}}%
  }
  \list{publisher}{1}{%
    {VLDB Endowment}%
  }
  \keyw{Classification,Supervised classification,Time series proximity
  measures,trees Learning metric}
  \strng{namehash}{DCAAC1}
  \strng{fullhash}{DCAAC1}
  \field{labelalpha}{DCA11}
  \field{sortinit}{D}
  \field{abstract}{%
  This paper proposes an extension of classification trees to time series input
  variables. A new split criterion based on time series proximities is
  introduced. First, the criterion relies on an adaptive (i.e., parameterized)
  time series metric to cover both behaviors and values proximities. The
  metrics parameters may change from one internal node to another to achieve
  the best bisection of the set of time series. Second, the criterion involves
  the automatic extraction of the most discriminating subsequences. The
  proposed time series classification tree is applied to a wide range of
  datasets: public and new, real and synthetic, univariate and multivariate
  data. We show, through the experiments performed in this study, that the
  proposed tree outperforms temporal trees using standard time series distances
  and performs well compared to other competitive time series classifiers%
  }
  \field{isbn}{0000000000000}
  \field{issn}{2150-8097}
  \field{title}{{Classification trees for time series}}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley D
  \verb esktop/Downloaded/Douzal-Chouakria, Amblard - 2011 - Classification tre
  \verb es for time series.pdf:pdf
  \endverb
  \field{journaltitle}{Pattern Recognition journal}
  \field{year}{2011}
\endentry

\entry{Douzal-Chouakria2012a}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Douzal-Chouakria}{D.-C.}%
     {Ahlame}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Amblard}{A.}%
     {C{\'{e}}cile}{C.}%
     {}{}%
     {}{}}%
  }
  \keyw{Classification trees,Learning metric,Supervised classification,Time
  series proximity measures}
  \strng{namehash}{DCAAC2}
  \strng{fullhash}{DCAAC2}
  \field{labelalpha}{DCA12}
  \field{sortinit}{D}
  \field{abstract}{%
  This paper proposes an extension of classification trees to time series input
  variables. A new split criterion based on time series proximities is
  introduced. First, the criterion relies on an adaptive (i.e., parameterized)
  time series metric to cover both behaviors and values proximities. The
  metrics parameters may change from one internal node to another to achieve
  the best bisection of the set of time series. Second, the criterion involves
  the automatic extraction of the most discriminating subsequences. The
  proposed time series classification tree is applied to a wide range of
  datasets: public and new, real and synthetic, univariate and multivariate
  data. We show, through the experiments performed in this study, that the
  proposed tree outperforms temporal trees using standard time series distances
  and performs well compared to other competitive time series classifiers. ©
  2011 Elsevier Ltd. All rights reserved.%
  }
  \verb{doi}
  \verb 10.1016/j.patcog.2011.08.018
  \endverb
  \field{issn}{00313203}
  \field{number}{3}
  \field{pages}{1076\bibrangedash 1091}
  \field{title}{{Classification trees for time series}}
  \field{volume}{45}
  \field{journaltitle}{Pattern Recognition}
  \field{year}{2012}
\endentry

\entry{Douzal-Chouakria2010}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Douzal-Chouakria}{D.-C.}%
     {A.}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Diallo}{D.}%
     {A.}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Giroud}{G.}%
     {F.}{F.}%
     {}{}%
     {}{}}%
  }
  \keyw{Classification,Clustering,Distance,Genes expression profiles,Time
  series}
  \strng{namehash}{DCADAGF1}
  \strng{fullhash}{DCADAGF1}
  \field{labelalpha}{DCDG10}
  \field{sortinit}{D}
  \field{abstract}{%
  This paper addresses the clustering and classification of active genes during
  the process of cell division. Cell division ensures the proliferation of
  cells, but it becomes increasingly abnormal in cancer cells. The genes
  studied here are described by their expression profiles (i.e. time series)
  during the cell division cycle. This work focuses on evaluating the
  efficiency of four major metrics for clustering and classifying genes
  expression profiles and is based on a random-periods model for the expression
  of cell-cycle genes. The model accounts for the observed attenuation in cycle
  amplitude or duration, variations in the initial amplitude, and drift in the
  expression profiles. ?? 2009 Elsevier B.V. All rights reserved.%
  }
  \verb{doi}
  \verb 10.1016/j.patrec.2010.05.008
  \endverb
  \field{issn}{01678655}
  \field{title}{{A random-periods model for the comparison of a metrics
  efficiency to classify cell-cycle expressed genes}}
  \field{journaltitle}{Pattern Recognition Letters}
  \field{year}{2010}
\endentry

\entry{Chouakria2007}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Douzal-Chouakria}{D.-C.}%
     {A.}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Nagabhushan}{N.}%
     {P.}{P.}%
     {}{}%
     {}{}}%
  }
  \keyw{Classification,Dynamic time warping,Fr??chet distance,Time Series}
  \strng{namehash}{DCANP1}
  \strng{fullhash}{DCANP1}
  \field{labelalpha}{DCN07}
  \field{sortinit}{D}
  \field{abstract}{%
  Abstract The most widely used measures of time series proximity are the
  Euclidean distance and dynamic time warping. The latter can be derived from
  the distance introduced by Maurice Frchet in 1906 to account for the
  proximity between curves. The major limitation of these proximity measures is
  that they are based on the closeness of the values regardless of the
  similarity w.r.t. the growth behavior of the time series. To alleviate this
  drawback we propose a new dissimilarity index, based on an automatic adaptive
  tuning function, to include both proximity measures w.r.t. values and w.r.t.
  behavior. A comparative numerical analysis between the proposed index and the
  classical distance measures is performed on the basis of two datasets: a
  synthetic dataset and a dataset from a public health study.%
  }
  \verb{doi}
  \verb 10.1007/s11634-006-0004-6
  \endverb
  \field{issn}{18625347}
  \field{title}{{Adaptive dissimilarity index for measuring time series
  proximity}}
  \field{journaltitle}{Advances in Data Analysis and Classification}
  \field{year}{2007}
\endentry

\entry{Denoeux1995}{article}{}
  \name{author}{1}{}{%
    {{}%
     {Denoeux}{D.}%
     {T.}{T.}%
     {}{}%
     {}{}}%
  }
  \keyw{Dempster's rule of combination,Dempster-Shafer theory,Density
  functional theory,Error analysis,H infinity control,Medical services,Nearest
  neighbor searches,Neural networks,Voting,ambiguity,class membership,distance
  rejection,distance-weighted k-NN procedures,evidence,imperfect
  knowledge,inference mechanisms,k-nearest neighbor classification rule,pattern
  classification,statistical analysis,unseen pattern classification,voting}
  \strng{namehash}{DT1}
  \strng{fullhash}{DT1}
  \field{labelalpha}{Den95}
  \field{sortinit}{D}
  \field{abstract}{%
  In this paper, the problem of classifying an unseen pattern on the basis of
  its nearest neighbors in a recorded data set is addressed from the point of
  view of Dempster-Shafer theory. Each neighbor of a sample to be classified is
  considered as an item of evidence that supports certain hypotheses regarding
  the class membership of that pattern. The degree of support is defined as a
  function of the distance between the two vectors. The evidence of the k
  nearest neighbors is then pooled by means of Dempster's rule of combination.
  This approach provides a global treatment of such issues as ambiguity and
  distance rejection, and imperfect knowledge regarding the class membership of
  training patterns. The effectiveness of this classification scheme as
  compared to the voting and distance-weighted k-NN procedures is demonstrated
  using several sets of simulated and real-world data%
  }
  \verb{doi}
  \verb 10.1109/21.376493
  \endverb
  \field{issn}{00189472}
  \field{number}{5}
  \field{pages}{804\bibrangedash 813}
  \field{title}{{A k-nearest neighbor classification rule based on
  Dempster-Shafer theory}}
  \verb{url}
  \verb http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=376493
  \endverb
  \field{volume}{25}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/Desktop/smc95.pdf:pdf
  \endverb
  \field{journaltitle}{IEEE Transactions on Systems, Man, and Cybernetics}
  \field{year}{1995}
\endentry

\entry{Dietterich1995}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Dietterich}{D.}%
     {Thomas~G.}{T.~G.}%
     {}{}%
     {}{}}%
    {{}%
     {Hild}{H.}%
     {Hermann}{H.}%
     {}{}%
     {}{}}%
    {{}%
     {Bakiri}{B.}%
     {Ghulum}{G.}%
     {}{}%
     {}{}}%
  }
  \keyw{ID3,backpropagation,experimental comparisons,text-to-speech}
  \strng{namehash}{DTGHHBG1}
  \strng{fullhash}{DTGHHBG1}
  \field{labelalpha}{DHB95}
  \field{sortinit}{D}
  \field{abstract}{%
  The performance of the error backpropagation (BP) and ID3 learning algorithms
  was compared on the task of mapping English text to phonemes and stresses.
  Under the distributed output code developed by Sejnowski and Rosenberg, it is
  shown that BP consistently out-performs ID3 on this task by several
  percentage points. Three hypotheses explaining this difference were explored:
  (a) ID3 is overfitting the training data, (b) BP is able to share hidden
  units across several output units and hence can learn the output units
  better, and (c) BP captures statistical information that ID3 does not. We
  conclude that only hypothesis (c) is correct. By augmenting ID3 with a simple
  statistical learning procedure, the performance of BP can be closely matched.
  More complex statistical procedures can improve the performance of both BP
  and ID3 substantially in this domain.%
  }
  \verb{doi}
  \verb 10.1007/BF00993821
  \endverb
  \field{isbn}{0885-6125}
  \field{issn}{08856125}
  \field{number}{1}
  \field{pages}{51\bibrangedash 80}
  \field{title}{{A comparison of ID3 and backpropagation for English
  text-to-speech mapping}}
  \field{volume}{18}
  \field{journaltitle}{Machine Learning}
  \field{year}{1995}
\endentry

\entry{Dietterich1997}{article}{}
  \name{author}{1}{}{%
    {{}%
     {Dietterich}{D.}%
     {T.}{T.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{DT2}
  \strng{fullhash}{DT2}
  \field{labelalpha}{Die97}
  \field{sortinit}{D}
  \field{title}{{Approximate Statistical Tests for Comparing Supervised
  Classification Learning Algorithms}}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley D
  \verb esktop/Downloaded/Dietterich - 1997 - Approximate Statistical Tests for
  \verb  Comparing Supervised Classification Learning Algorithms.pdf:pdf
  \endverb
  \field{year}{1997}
\endentry

\entry{Ding2008}{article}{}
  \name{author}{5}{}{%
    {{}%
     {Ding}{D.}%
     {Hui}{H.}%
     {}{}%
     {}{}}%
    {{}%
     {Trajcevski}{T.}%
     {Goce}{G.}%
     {}{}%
     {}{}}%
    {{}%
     {Scheuermann}{S.}%
     {Peter}{P.}%
     {}{}%
     {}{}}%
    {{}%
     {Wang}{W.}%
     {Xiaoyue}{X.}%
     {}{}%
     {}{}}%
    {{}%
     {Keogh}{K.}%
     {Eamonn}{E.}%
     {}{}%
     {}{}}%
  }
  \list{publisher}{1}{%
    {VLDB Endowment}%
  }
  \strng{namehash}{DH+1}
  \strng{fullhash}{DHTGSPWXKE1}
  \field{labelalpha}{Din+08}
  \field{sortinit}{D}
  \field{abstract}{%
  The last decade has witnessed a tremendous growths of interests in
  applications that deal with querying and mining of time series data. Numerous
  representation methods for dimensionality reduction and similarity measures
  geared towards time series have been introduced. Each individual work
  introducing a particular method has made specific claims and, aside from the
  occasional theoretical justifications, provided quantitative experimental
  observations. However, for the most part, the comparative aspects of these
  experiments were too narrowly focused on demonstrating the benefits of the
  proposed methods over some of the previously introduced ones. In order to
  provide a comprehensive validation, we conducted an extensive set of time
  series experiments re-implementing 8 different representation methods and 9
  similarity measures and their variants, and testing their effectiveness on 38
  time series data sets from a wide variety of application domains. In this
  paper, we give an overview of these different techniques and present our
  comparative experimental findings regarding their effectiveness. Our
  experiments have provided both a unified validation of some of the existing
  achievements, and in some cases, suggested that certain claims in the
  literature may be unduly optimistic. 1.%
  }
  \verb{doi}
  \verb 10.1145/1454159.1454226
  \endverb
  \verb{eprint}
  \verb 1012.2789v1
  \endverb
  \field{isbn}{0000000000000}
  \field{issn}{2150-8097}
  \field{number}{2}
  \field{pages}{1542\bibrangedash 1552}
  \field{title}{{Querying and Mining of Time Series Data : Experimental
  Comparison of Representations and Distance Measures}}
  \verb{url}
  \verb http://dl.acm.org/citation.cfm?id=1454226
  \endverb
  \field{volume}{1}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley D
  \verb esktop/Downloaded/Ding, Trajcevski, Scheuermann - 2008 - Querying and M
  \verb ining of Time Series Data Experimental Comparison of Representations an
  \verb d Distance.pdf:pdf
  \endverb
  \field{journaltitle}{Proceedings of the VLDB Endowment}
  \field{eprinttype}{arXiv}
  \field{year}{2008}
\endentry

\entry{DUrso2009}{article}{}
  \name{author}{2}{}{%
    {{}%
     {D'Urso}{D.}%
     {Pierpaolo}{P.}%
     {}{}%
     {}{}}%
    {{}%
     {Maharaj}{M.}%
     {Elizabeth~Ann}{E.~A.}%
     {}{}%
     {}{}}%
  }
  \keyw{Autocorrelation function,Crisp C-means clustering,Fuzzy C-means
  clustering,Switching time series,Time series}
  \strng{namehash}{DPMEA1}
  \strng{fullhash}{DPMEA1}
  \field{labelalpha}{DM09}
  \field{sortinit}{D}
  \field{abstract}{%
  The traditional approaches to clustering a set of time series are generally
  applicable if there is a fixed underlying structure to the time series so
  that each will belong to one cluster or the other. However, time series often
  display dynamic behaviour in their evolution over time. This dynamic
  behaviour should be taken into account when attempting to cluster time
  series. For instance, during a certain period, a time series might belong to
  a certain cluster; afterwards its dynamics might be closer to that of another
  cluster. In this case, the traditional clustering approaches are unlikely to
  find and represent the underlying structure in the given time series. This
  switch from one time state to another, which is typically vague, can be
  naturally treated following a fuzzy approach. This paper proposes a fuzzy
  clustering approach based on the autocorrelation functions of time series, in
  which each time series is not assigned exclusively to only one cluster, but
  it is allowed to belong to different clusters with various membership
  degrees. ?? 2009 Elsevier B.V. All rights reserved.%
  }
  \verb{doi}
  \verb 10.1016/j.fss.2009.04.013
  \endverb
  \field{isbn}{0165-0114}
  \field{issn}{01650114}
  \field{number}{24}
  \field{pages}{3565\bibrangedash 3589}
  \field{title}{{Autocorrelation-based fuzzy clustering of time series}}
  \field{volume}{160}
  \field{journaltitle}{Fuzzy Sets and Systems}
  \field{year}{2009}
\endentry

\entry{Do2012}{article}{}
  \name{author}{4}{}{%
    {{}%
     {Do}{D.}%
     {Huyen}{H.}%
     {}{}%
     {}{}}%
    {{}%
     {Kalousis}{K.}%
     {Alexandros}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Wang}{W.}%
     {Jun}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Woznica}{W.}%
     {Adam}{A.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{DH+2}
  \strng{fullhash}{DHKAWJWA1}
  \field{labelalpha}{Do+12}
  \field{sortinit}{D}
  \verb{eprint}
  \verb arXiv:1201.4714v1
  \endverb
  \field{pages}{308\bibrangedash 317}
  \field{title}{{A metric learning perspective of SVM: on the relation of LMNN
  and SVM}}
  \verb{url}
  \verb http://cui.unige.ch/{~}wangjun/papers/svm{\_}lmnn{\_}aistats12.pdf
  \endverb
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/Desktop/ML and SVM.pdf:pdf
  \endverb
  \field{journaltitle}{Proceedings of the 15th International Con- ference on
  Artificial Intelligence and Statistics (AISTAS '12)}
  \field{eprinttype}{arXiv}
  \field{year}{2012}
\endentry

\entry{Duin2012}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Duin}{D.}%
     {Robert P~W}{R.~P.~W.}%
     {}{}%
     {}{}}%
    {{}%
     {P{\c{c}}kalska}{P.}%
     {Elbieta}{E.}%
     {}{}%
     {}{}}%
  }
  \keyw{Dissimilarity representation,Dissimilarity space,Representation
  set,Structural pattern recognition,Vector space}
  \strng{namehash}{DRPWPE1}
  \strng{fullhash}{DRPWPE1}
  \field{labelalpha}{DP12}
  \field{sortinit}{D}
  \field{abstract}{%
  Human experts constitute pattern classes of natural objects based on their
  observed appearance. Automatic systems for pattern recognition may be
  designed on a structural description derived from sensor observations.
  Alternatively, training sets of examples can be used in statistical learning
  procedures. They are most powerful for vectorial object representations.
  Unfortunately, structural descriptions do not match well with vectorial
  representations. Consequently it is difficult to combine the structural and
  statistical approaches to pattern recognition. Structural descriptions may be
  used to compare objects. This leads to a set of pairwise dissimilarities from
  which vectors can be derived for the purpose of statistical learning. The
  resulting dissimilarity representation bridges thereby the structural and
  statistical approaches. The dissimilarity space is one of the possible spaces
  resulting from this representation. It is very general and easy to implement.
  This paper gives a historical review and discusses the properties of the
  dissimilarity space approaches illustrated by a set of examples on real world
  datasets. © 2011 Elsevier B.V. All rights reserved.%
  }
  \verb{doi}
  \verb 10.1016/j.patrec.2011.04.019
  \endverb
  \field{isbn}{0167-8655}
  \field{issn}{01678655}
  \field{number}{7}
  \field{pages}{826\bibrangedash 832}
  \field{title}{{The dissimilarity space: Bridging structural and statistical
  pattern recognition}}
  \field{volume}{33}
  \field{journaltitle}{Pattern Recognition Letters}
  \field{year}{2012}
\endentry

\entry{Dudani1976}{article}{}
  \name{author}{1}{}{%
    {{}%
     {Dudani}{D.}%
     {Sahibsingh~a.}{S.~a.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{DSa1}
  \strng{fullhash}{DSa1}
  \field{labelalpha}{Dud76}
  \field{sortinit}{D}
  \field{abstract}{%
  Among the simplest and most intuitively appealing classes of nonprobabilistic
  classification procedures are those that weight the evidence of nearby sample
  observations most heavily. More specifically, one might wish to weight the
  evidence of a neighbor close to an unclassified observation more heavily than
  the evidence of another neighbor which is at a greater distance from the
  unclassified observation. One such classification rule is described which
  makes use of a neighbor weighting function for the purpose of assigning a
  class to an unclassified sample. The admissibility of such a rule is also
  considered.%
  }
  \verb{doi}
  \verb 10.1109/TSMC.1976.5408784
  \endverb
  \field{isbn}{0018-9472}
  \field{issn}{00189472}
  \field{number}{4}
  \field{pages}{325\bibrangedash 327}
  \field{title}{{DISTANCE-WEIGHTED k-NEAREST-NEIGHBOR RULE.}}
  \field{volume}{SMC-6}
  \field{journaltitle}{IEEE Transactions on Systems, Man and Cybernetics}
  \field{year}{1976}
\endentry

\entry{Diaz2010}{article}{}
  \name{author}{2}{}{%
    {{}%
     {D{\'{\i}}az}{D.}%
     {Sonia~P{\'{e}}rtega}{S.~P.}%
     {}{}%
     {}{}}%
    {{}%
     {Vilar}{V.}%
     {Jos{\'{e}}~A.}{J.~A.}%
     {}{}%
     {}{}}%
  }
  \keyw{ARMA processes,Dissimilarity measures,Local linear
  regression,Non-linear processes,Stationary and non-stationary processes,Time
  series clustering}
  \strng{namehash}{DSPVJA1}
  \strng{fullhash}{DSPVJA1}
  \field{labelalpha}{DV10}
  \field{sortinit}{D}
  \field{abstract}{%
  One key point in cluster analysis is to determine a similarity or
  dissimilarity measure between data objects. When working with time series,
  the concept of similarity can be established in different ways. In this
  paper, several non-parametric statistics originally designed to test the
  equality of the log-spectra of two stochastic processes are proposed as
  dissimilarity measures between time series data. Their behavior in time
  series clustering is analyzed throughout a simulation study, and compared
  with the performance of several model-free and model-based dissimilarity
  measures. Up to three different classification settings were considered: (i)
  to distinguish between stationary and non-stationary time series, (ii) to
  classify different ARMA processes and (iii) to classify several non-linear
  time series models. As it was expected, the performance of a particular
  dissimilarity metric strongly depended on the type of processes subjected to
  clustering. Among all the measures studied, the non-parametric distances
  showed the most robust behavior.%
  }
  \verb{doi}
  \verb 10.1007/s00357-010-9064-6
  \endverb
  \field{issn}{0176-4268}
  \field{number}{3}
  \field{pages}{333\bibrangedash 362}
  \field{title}{{Comparing Several Parametric and Nonparametric Approaches to
  Time Series Clustering: A Simulation Study}}
  \verb{url}
  \verb http://www.scopus.com/inward/record.url?eid=2-s2.0-84871904170{\&}partn
  \verb erID=tZOtx3y1
  \endverb
  \field{volume}{27}
  \field{journaltitle}{Journal of Classification}
  \field{year}{2010}
\endentry

\entry{Fan2008}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Fan}{F.}%
     {RE}{R.}%
     {}{}%
     {}{}}%
    {{}%
     {Chang}{C.}%
     {KW}{K.}%
     {}{}%
     {}{}}%
    {{}%
     {Hsieh}{H.}%
     {CJ}{C.}%
     {}{}%
     {}{}}%
  }
  \keyw{large-scale linear classification,logistic regression,machine
  learning,open,source,support vector machines}
  \strng{namehash}{FRCKHC1}
  \strng{fullhash}{FRCKHC1}
  \field{labelalpha}{FCH08}
  \field{sortinit}{F}
  \field{abstract}{%
  LIBLINEAR is an open source library for large-scale linear classification. It
  supports logistic regression and linear support vector machines. We provide
  easy-to-use command-line tools and library calls for users and developers.
  Comprehensive documents are available for both beginners and advanced users.
  Experiments demonstrate that LIBLINEAR is very efficient on large sparse data
  sets.%
  }
  \verb{doi}
  \verb 10.1038/oby.2011.351
  \endverb
  \field{isbn}{089791497X}
  \field{issn}{15324435}
  \field{title}{{LIBLINEAR: A library for large linear classification}}
  \verb{url}
  \verb http://dl.acm.org/citation.cfm?id=1442794
  \endverb
  \field{journaltitle}{The Journal of Machine Learning}
  \field{year}{2008}
\endentry

\entry{Frambourg2013a}{inproceedings}{}
  \name{author}{3}{}{%
    {{}%
     {Frambourg}{F.}%
     {C.}{C.}%
     {}{}%
     {}{}}%
    {{}%
     {Douzal-Chouakria}{D.-C.}%
     {A.}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Gaussier}{G.}%
     {E.}{E.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{FCDCAGE1}
  \strng{fullhash}{FCDCAGE1}
  \field{labelalpha}{FDCG13}
  \field{sortinit}{F}
  \field{booktitle}{Intelligent Data Analysis}
  \field{pages}{198\bibrangedash 209}
  \field{title}{{Learning multiple temporal matching for time series
  classification}}
  \field{year}{2013}
\endentry

\entry{Faloutsos1994}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Faloutsos}{F.}%
     {Christos}{C.}%
     {}{}%
     {}{}}%
    {{}%
     {Ranganathan}{R.}%
     {M.}{M.}%
     {}{}%
     {}{}}%
    {{}%
     {Manolopoulos}{M.}%
     {Yannis}{Y.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{FCRMMY1}
  \strng{fullhash}{FCRMMY1}
  \field{labelalpha}{FRM94}
  \field{sortinit}{F}
  \field{abstract}{%
  We present an ecient indexing method to locate 1-dimensional subsequences
  within a collection of sequences, such that the subsequences match a given
  (query) pattern within a specified tolerance. The idea is to map each data
  sequence into a small set of multidimensional rectangles in feature space.
  Then, these rectangles can be readily indexed using traditional spatial
  access methods, like the R*-tree [9]. In more detail, we use a sliding window
  over the data sequence and extract its features; the result is a trail in
  feature space. We propose an efficient and effective algorithm to divide such
  trails into sub-trails, which are subsequently represented by their Minimum
  Bounding Rectangles (MBRs). We also examine queries of varying lengths, and
  we show how to handle each case eciently. We implemented our method and
  carried out experiments on synthetic and real data (stock price movements).
  We compared the method to sequential scanning, which is the only obvious
  competitor. The results were excellent: our method accelerated the search
  time from 3 times up to 100 times.%
  }
  \verb{doi}
  \verb 10.1145/191843.191925
  \endverb
  \field{isbn}{0897916395}
  \field{issn}{01635808}
  \field{number}{2}
  \field{pages}{419\bibrangedash 429}
  \field{title}{{Fast subsequence matching in time-series databases}}
  \verb{url}
  \verb http://portal.acm.org/citation.cfm?doid=191843.191925
  \endverb
  \field{volume}{23}
  \field{journaltitle}{ACM SIGMOD Record}
  \field{year}{1994}
\endentry

\entry{Goldberger2004}{article}{}
  \name{author}{6}{}{%
    {{}%
     {Goldberger}{G.}%
     {Jacob}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Roweis}{R.}%
     {Sam~T}{S.~T.}%
     {}{}%
     {}{}}%
    {{}%
     {Hinton}{H.}%
     {Geoffrey~E}{G.~E.}%
     {}{}%
     {}{}}%
    {{}%
     {Salakhutdinov}{S.}%
     {Ruslan}{R.}%
     {}{}%
     {}{}}%
    {{}%
     {Roweis}{R.}%
     {Sam~T}{S.~T.}%
     {}{}%
     {}{}}%
    {{}%
     {Salakhutdinov}{S.}%
     {Ruslan}{R.}%
     {}{}%
     {}{}}%
  }
  \keyw{Learning/Statistics {\&} Optimisation,Theory {\&} Algorithms}
  \strng{namehash}{GJ+1}
  \strng{fullhash}{GJRSTHGESRRSTSR1}
  \field{labelalpha}{Gol+04}
  \field{sortinit}{G}
  \field{abstract}{%
  In this paper we propose a novel method for learning
  a$\backslash$r$\backslash$nMahalanobis distance measure to be used in the KNN
  classification$\backslash$r$\backslash$nalgorithm. The algorithm directly
  maximizes a stochastic variant of$\backslash$r$\backslash$nthe leave-one-out
  KNN score on the training set. It can also$\backslash$r$\backslash$nlearn a
  low-dimensional linear embedding of labeled data that
  can$\backslash$r$\backslash$nbe used for data visualization and fast
  classification.$\backslash$r$\backslash$nUnlike other methods, our
  classification model is non-parametric,$\backslash$r$\backslash$nmaking no
  assumptions about the shape of the class distributions
  or$\backslash$r$\backslash$nthe boundaries between them. The performance of
  the method$\backslash$r$\backslash$nis demonstrated on several data sets,
  both for metric learning and$\backslash$r$\backslash$nlinear dimensionality
  reduction.%
  }
  \verb{doi}
  \verb 10.1.1.108.7841
  \endverb
  \field{pages}{513\bibrangedash 520}
  \field{title}{{Neighbourhood Components Analysis}}
  \verb{url}
  \verb http://eprints.pascal-network.org/archive/00001570/
  \endverb
  \field{journaltitle}{Advances in Neural Information Processing Systems}
  \field{year}{2004}
\endentry

\entry{Hsu2008}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Hsu}{H.}%
     {Chih-Wei}{C.-W.}%
     {}{}%
     {}{}}%
    {{}%
     {Chang}{C.}%
     {Chih-Chung}{C.-C.}%
     {}{}%
     {}{}}%
    {{}%
     {Lin}{L.}%
     {Chih-Jen}{C.-J.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{HCWCCCLCJ1}
  \strng{fullhash}{HCWCCCLCJ1}
  \field{labelalpha}{HCL08}
  \field{sortinit}{H}
  \field{abstract}{%
  The support vector machine (SVM) is a popular classi cation technique.
  However, beginners who are not familiar with SVM often get unsatisfactory
  results since they miss some easy but signi cant steps. In this guide, we
  propose a simple procedure which usually gives reasonable results. developed
  well-differentiated superficial transitional cell bladder cancer.
  CONCLUSIONS: Patients with SCI often prefer SPC than other methods offered to
  them, because of quality-of-life issues. The incidence of significant
  complications might not be as high as previously reported, and with a
  commitment to careful follow-up, SPC can be a safe option for carefully
  selected patients if adequate surveillance can be ensured.%
  }
  \verb{doi}
  \verb 10.1177/02632760022050997
  \endverb
  \verb{eprint}
  \verb 0-387-31073-8
  \endverb
  \field{isbn}{013805326X}
  \field{issn}{1464-410X}
  \field{number}{1}
  \field{pages}{1396\bibrangedash 400}
  \field{title}{{A Practical Guide to Support Vector Classification}}
  \verb{url}
  \verb http://www.csie.ntu.edu.tw/{~}cjlin/papers/guide/guide.pdf
  \endverb
  \field{volume}{101}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/Google Drive/Th{\`{e}}se/Bibliographie
  \verb /SVM Librairie/A Practical Guide to Support Vector Classification.pdf:p
  \verb df
  \endverb
  \field{journaltitle}{BJU international}
  \field{eprinttype}{arXiv}
  \field{year}{2008}
\endentry

\entry{Hwang2012}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Hwang}{H.}%
     {Seok~Hwan}{S.~H.}%
     {}{}%
     {}{}}%
    {{}%
     {Ham}{H.}%
     {Dae~Heon}{D.~H.}%
     {}{}%
     {}{}}%
    {{}%
     {Kim}{K.}%
     {Joong~Hoon}{J.~H.}%
     {}{}%
     {}{}}%
  }
  \keyw{forecasting,forecasting performance,support vector machine}
  \strng{namehash}{HSHHDHKJH1}
  \strng{fullhash}{HSHHDHKJH1}
  \field{labelalpha}{HHK12}
  \field{sortinit}{H}
  \verb{doi}
  \verb 10.1007/s12205-012-1519-3
  \endverb
  \field{issn}{1226-7988}
  \field{number}{5}
  \field{pages}{870\bibrangedash 882}
  \field{title}{{Forecasting performance of LS-SVM for nonlinear hydrological
  time series}}
  \verb{url}
  \verb http://link.springer.com/10.1007/s12205-012-1519-3
  \endverb
  \field{volume}{16}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/Google Drive/Th{\`{e}}se/Bibliographie
  \verb /SVM + Time Series/Hwang-KSCE Journal of Civil Engineering-2012{\_}Fore
  \verb casting performance of LS-SVM for nonlinear hydrological time series.pd
  \verb f:pdf
  \endverb
  \field{journaltitle}{KSCE Journal of Civil Engineering}
  \field{year}{2012}
\endentry

\entry{Heisele2001}{inproceedings}{}
  \name{author}{3}{}{%
    {{}%
     {Heisele}{H.}%
     {B}{B}%
     {}{}%
     {}{}}%
    {{}%
     {Ho}{H.}%
     {P}{P}%
     {}{}%
     {}{}}%
    {{}%
     {Poggio}{P.}%
     {T}{T}%
     {}{}%
     {}{}}%
  }
  \keyw{Active shape model,Biology computing,Face recognition,Image
  databases,Image recognition,Mouth,Robustness,SVM classifier,Solid
  modeling,Support vector machine classification,Support vector
  machines,clustering,component-based approach,facial components,feature
  extraction,feature vector,global methods,learning automata}
  \strng{namehash}{HBHPPT1}
  \strng{fullhash}{HBHPPT1}
  \field{labelalpha}{HHP01}
  \field{sortinit}{H}
  \field{abstract}{%
  We present a component-based method and two global methods for face
  recognition and evaluate them with respect to robustness against pose
  changes. In the component system we first locate facial components, extract
  them and combine them into a single feature vector which is classified by a
  Support Vector Machine (SVM). The two global systems recognize faces by
  classifying a single feature vector consisting of the gray values of the
  whole face image. In the first global system we trained a single SVM
  classifier for each person in the database. The second system consists of
  sets of viewpoint-specific SVM classifiers and involves clustering during
  training. We performed extensive tests on a database which included faces
  rotated up to about 40° in depth. The component system clearly outperformed
  both global systems on all tests.%
  }
  \field{booktitle}{IEEE International Conference on Computer Vision, ICCV}
  \verb{doi}
  \verb 10.1109/ICCV.2001.937693
  \endverb
  \field{isbn}{0-7695-1143-0}
  \field{issn}{1089-7801}
  \field{number}{July}
  \field{pages}{688\bibrangedash 694}
  \field{title}{{Face recognition with support vector machines: global versus
  component-based approach}}
  \verb{url}
  \verb http://dx.doi.org/10.1109/ICCV.2001.937693
  \endverb
  \field{volume}{2}
  \field{year}{2001}
\endentry

\entry{Hu2013}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Hu}{H.}%
     {Jianming}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Wang}{W.}%
     {Jianzhou}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Zeng}{Z.}%
     {Guowei}{G.}%
     {}{}%
     {}{}}%
  }
  \list{publisher}{1}{%
    {Elsevier Ltd}%
  }
  \keyw{Ensemble Empirical Mode Decomposition (EEMD),Support Vector Machine
  (SVM),Wind farm,Wind speed forecasting}
  \strng{namehash}{HJWJZG1}
  \strng{fullhash}{HJWJZG1}
  \field{labelalpha}{HWZ13}
  \field{sortinit}{H}
  \field{abstract}{%
  In this paper, a hybrid forecasting approach, which combines the Ensemble
  Empirical Mode Decomposition (EEMD) and the Support Vector Machine (SVM), is
  proposed to improve the quality of wind speed forecasting. The essence of the
  methodology incorporates three phases. First, the original data of wind speed
  are decomposed into a number of independent Intrinsic Mode Functions (IMFs)
  and one residual series by EEMD using the principle of decomposition. In
  order to forecast these IMFs, excepting the highest frequency acquired by
  EEMD, the respective estimates are yielded using the SVM algorithm. Finally,
  these respective estimates are combined into the final wind speed forecasts
  using the principle of ensemble. The proposed hybrid method is examined by
  forecasting the mean monthly wind speed of three wind farms located in
  northwest China. The obtained results confirm an observable improvement for
  the forecasting validity of the proposed hybrid approach. This tool shows
  great promise for the forecasting of intricate time series which are
  intrinsically highly volatile and irregular. ?? 2013 Elsevier Ltd.%
  }
  \verb{doi}
  \verb 10.1016/j.renene.2013.05.012
  \endverb
  \field{isbn}{0960-1481}
  \field{issn}{09601481}
  \field{pages}{185\bibrangedash 194}
  \field{title}{{A hybrid forecasting approach applied to wind speed time
  series}}
  \verb{url}
  \verb http://dx.doi.org/10.1016/j.renene.2013.05.012
  \endverb
  \field{volume}{60}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/Google Drive/Th{\`{e}}se/Bibliographie
  \verb /SVM + Time Series/Hu-Renewable Energy-2013{\_}A hybrid forecasting app
  \verb roach applied to wind speed time series.pdf:pdf
  \endverb
  \field{journaltitle}{Renewable Energy}
  \field{year}{2013}
\endentry

\entry{Jeong2011}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Jeong}{J.}%
     {Young-Seon}{Y.-S.}%
     {}{}%
     {}{}}%
    {{}%
     {Jeong}{J.}%
     {Myong~K.}{M.~K.}%
     {}{}%
     {}{}}%
    {{}%
     {Omitaomu}{O.}%
     {Olufemi~A.}{O.~A.}%
     {}{}%
     {}{}}%
  }
  \keyw{Adaptive weights,Dynamic time warping,Modified logistic weight
  function,Time series classification,Time series clustering,Weighted dynamic
  time warping}
  \strng{namehash}{JYSJMKOOA1}
  \strng{fullhash}{JYSJMKOOA1}
  \field{labelalpha}{JJO11}
  \field{sortinit}{J}
  \field{abstract}{%
  Dynamic time warping (DTW), which finds the minimum path by providing
  non-linear alignments between two time series, has been widely used as a
  distance measure for time series classification and clustering. However, DTW
  does not account for the relative importance regarding the phase difference
  between a reference point and a testing point. This may lead to
  misclassification especially in applications where the shape similarity
  between two sequences is a major consideration for an accurate recognition.
  Therefore, we propose a novel distance measure, called a weighted DTW (WDTW),
  which is a penalty-based DTW. Our approach penalizes points with higher phase
  difference between a reference point and a testing point in order to prevent
  minimum distance distortion caused by outliers. The rationale underlying the
  proposed distance measure is demonstrated with some illustrative examples. A
  new weight function, called the modified logistic weight function (MLWF), is
  also proposed to systematically assign weights as a function of the phase
  difference between a reference point and a testing point. By applying
  different weights to adjacent points, the proposed algorithm can enhance the
  detection of similarity between two time series. We show that some popular
  distance measures such as DTW and Euclidean distance are special cases of our
  proposed WDTW measure. We extend the proposed idea to other variants of DTW
  such as derivative dynamic time warping (DDTW) and propose the weighted
  version of DDTW. We have compared the performances of our proposed procedures
  with other popular approaches using public data sets available through the
  UCR Time Series Data Mining Archive for both time series classification and
  clustering problems. The experimental results indicate that the proposed
  approaches can achieve improved accuracy for time series classification and
  clustering problems.%
  }
  \verb{doi}
  \verb 10.1016/j.patcog.2010.09.022
  \endverb
  \field{isbn}{0031-3203}
  \field{issn}{00313203}
  \field{number}{9}
  \field{pages}{2231\bibrangedash 2240}
  \field{title}{{Weighted dynamic time warping for time series classification}}
  \verb{url}
  \verb http://www.sciencedirect.com/science/article/pii/S003132031000484X
  \endverb
  \field{volume}{44}
  \field{journaltitle}{Pattern Recognition}
  \field{year}{2011}
\endentry

\entry{Jain1999}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Jain}{J.}%
     {a.~K.}{a.~K.}%
     {}{}%
     {}{}}%
    {{}%
     {Murty}{M.}%
     {M.~N.}{M.~N.}%
     {}{}%
     {}{}}%
    {{}%
     {Flynn}{F.}%
     {P.~J.}{P.~J.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{JaKMMNFPJ1}
  \strng{fullhash}{JaKMMNFPJ1}
  \field{labelalpha}{JMF99}
  \field{sortinit}{J}
  \field{abstract}{%
  Clustering is the unsupervised classification of patterns (observations, data
  items, or feature vectors) into groups (clusters). The clustering problem has
  been addressed in many contexts and by researchers in many disciplines; this
  reflects its broad appeal and usefulness as one of the steps in exploratory
  data analysis. However, clustering is a difficult problem combinatorially,
  and differences in assumptions and contexts in different communities has made
  the transfer of useful generic concepts and methodologies slow to occur. This
  paper presents an overview of pattern clustering methods from a statistical
  pattern recognition perspective, with a goal of providing useful advice and
  references to fundamental concepts accessible to the broad community of
  clustering practitioners. We present a taxonomy of clustering techniques, and
  identify cross-cutting themes and recent advances. We also describe some
  important applications of clustering algorithms such as image segmentation,
  object recognition, and information retrieval.%
  }
  \verb{doi}
  \verb 10.1145/331499.331504
  \endverb
  \verb{eprint}
  \verb arXiv:1101.1881v2
  \endverb
  \field{isbn}{0360-0300}
  \field{issn}{03600300}
  \field{number}{3}
  \field{pages}{264\bibrangedash 323}
  \field{title}{{Data clustering: a review}}
  \verb{url}
  \verb http://portal.acm.org/citation.cfm?doid=331499.331504
  \endverb
  \field{volume}{31}
  \field{journaltitle}{ACM Computing Surveys}
  \field{eprinttype}{arXiv}
  \field{year}{1999}
\endentry

\entry{Kalman1960}{article}{}
  \name{author}{1}{}{%
    {{}%
     {Kalman}{K.}%
     {R~E}{R.~E.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{KRE1}
  \strng{fullhash}{KRE1}
  \field{labelalpha}{Kal60}
  \field{sortinit}{K}
  \field{abstract}{%
  The classical filtering and prediction problem is re-examined using the Bode-
  Shannon representation of random processes and the ``state
  transition{\&}apos;{\&}apos; method of analysis of dynamic systems. New
  results are: (1) The formulation and methods of solution of the problem apply
  without modifica- tion to stationary and nonstationary statistics and to
  growing-memory and infinite- memory filters. (2) A nonlinear difference (or
  differential) equation is derived for the covariance matrix of the optimal
  estimation error. From the solution of this equation the co- efficients of
  the difference (or differential) equation of the optimal linear filter are
  ob- tained without further calculations. (3) The filtering problem is shown
  to be the dual of the noise-free regulator problem. The new method developed
  here is applied to two well-known problems, confirming and extending earlier
  results. The discussion is largely self-contained and proceeds from first
  principles; basic concepts of the theory of random processes are reviewed in
  the Appendix.%
  }
  \verb{doi}
  \verb 10.1115/1.3662552
  \endverb
  \field{isbn}{9783540769897}
  \field{issn}{0021-9223}
  \field{number}{Series D}
  \field{pages}{35\bibrangedash 45}
  \field{title}{{A New Approach to Linear Filtering and Prediction Problems}}
  \field{volume}{82}
  \field{journaltitle}{Transactions of the ASME Journal of Basic Engineering}
  \field{year}{1960}
\endentry

\entry{Keogh2011}{misc}{}
  \name{author}{7}{}{%
    {{}%
     {Keogh}{K.}%
     {E.}{E.}%
     {}{}%
     {}{}}%
    {{}%
     {Zhu}{Z.}%
     {Q.}{Q.}%
     {}{}%
     {}{}}%
    {{}%
     {Hu}{H.}%
     {B.}{B.}%
     {}{}%
     {}{}}%
    {{}%
     {Hao}{H.}%
     {Y.}{Y.}%
     {}{}%
     {}{}}%
    {{}%
     {Xi}{X.}%
     {X.}{X.}%
     {}{}%
     {}{}}%
    {{}%
     {Wei}{W.}%
     {L.}{L.}%
     {}{}%
     {}{}}%
    {{}%
     {Ratanamahatana}{R.}%
     {C.A.}{C.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{KE+1}
  \strng{fullhash}{KEZQHBHYXXWLRC1}
  \field{labelalpha}{Keo+11}
  \field{sortinit}{K}
  \field{title}{{The UCR Time Series Classification/Clustering Homepage}}
  \verb{url}
  \verb www.cs.ucr.edu/{~}eamonn/time{\_}series{\_}data/
  \endverb
  \field{year}{2011}
\endentry

\entry{Keller1985}{misc}{}
  \name{author}{3}{}{%
    {{}%
     {Keller}{K.}%
     {James~M.}{J.~M.}%
     {}{}%
     {}{}}%
    {{}%
     {Gray}{G.}%
     {Michael~R.}{M.~R.}%
     {}{}%
     {}{}}%
    {{}%
     {Givens}{G.}%
     {James~a.}{J.~a.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{KJMGMRGJa1}
  \strng{fullhash}{KJMGMRGJa1}
  \field{labelalpha}{KGG85}
  \field{sortinit}{K}
  \field{abstract}{%
  Classification of objects is an important area of research and application in
  a variety of fields. In the presence of full knowledge of the underlying
  probabilities, Bayes decision theory gives optimal error rates. In those
  cases where this information is not present, many algorithms make use of
  distance or similarity among samples as a means of classification. The
  K-nearest neighbor decision rule has often been used in these pattern
  recognition problems. One of the difficulties that arises when utilizing this
  technique is that each of the labeled samples is given equal importance in
  deciding the class memberships of the pattern to be classified, regardless of
  their `typicalness'. The theory of fuzzy sets is introduced into the
  K-nearest neighbor technique to develop a fuzzy version of the algorithm.
  Three methods of assigning fuzzy memberships to the labeled samples are
  proposed, and experimental results and comparisons to the crisp version are
  presented.%
  }
  \field{booktitle}{IEEE Transactions on Systems, Man, and Cybernetics}
  \verb{doi}
  \verb 10.1109/TSMC.1985.6313426
  \endverb
  \field{isbn}{0018-9472}
  \field{issn}{0018-9472}
  \field{number}{4}
  \field{pages}{580\bibrangedash 585}
  \field{title}{{A fuzzy K-nearest neighbor algorithm}}
  \field{volume}{SMC-15}
  \field{year}{1985}
\endentry

\entry{Kruskall1983}{book}{}
  \name{author}{2}{}{%
    {{}%
     {Kruskall}{K.}%
     {J.}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Liberman}{L.}%
     {M.}{M.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{KJLM1}
  \strng{fullhash}{KJLM1}
  \field{labelalpha}{KL83}
  \field{sortinit}{K}
  \field{booktitle}{TimeWarps, String Edits and Macromolecules}
  \field{edition}{Addison-We}
  \field{title}{{The symmetric time warping algorithm: From continuous to
  discrete. In TimeWarps}}
  \field{year}{1983}
\endentry

\entry{Keogh2001a}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Keogh}{K.}%
     {E~J}{E.~J.}%
     {}{}%
     {}{}}%
    {{}%
     {Pazzani}{P.}%
     {M~J}{M.~J.}%
     {}{}%
     {}{}}%
  }
  \keyw{dynamic time warping}
  \strng{namehash}{KEJPMJ1}
  \strng{fullhash}{KEJPMJ1}
  \field{labelalpha}{KP01}
  \field{sortinit}{K}
  \field{abstract}{%
  this paper we address both these problems by introducing a
  modification$\backslash$nof DTW. The crucial difference is in the features we
  consider when$\backslash$nattempting to find the correct warping. Rather than
  use the raw data,$\backslash$nwe consider only the (estimated) local
  derivatives of the data%
  }
  \verb{doi}
  \verb 10.1137/1.9781611972719.1
  \endverb
  \field{pages}{1\bibrangedash 11}
  \field{title}{{Derivative Dynamic Time Warping}}
  \field{journaltitle}{Proceedings of the 1st SIAM International Conference on
  Data Mining}
  \field{year}{2001}
\endentry

\entry{Keogh2004}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Keogh}{K.}%
     {Eamonn}{E.}%
     {}{}%
     {}{}}%
    {{}%
     {Ratanamahatana}{R.}%
     {Chotirat~Ann}{C.~A.}%
     {}{}%
     {}{}}%
  }
  \keyw{dynamic time warping,indexing,lower bounding,time series}
  \strng{namehash}{KERCA1}
  \strng{fullhash}{KERCA1}
  \field{labelalpha}{KR04}
  \field{sortinit}{K}
  \field{abstract}{%
  The problem of indexing time series has attracted much interest. Most
  algorithms used to index time series utilize the Euclidean distance or some
  variation thereof. However, it has been forcefully shown that the Euclidean
  distance is a very brittle distance measure. Dy- namic time warping (DTW) is
  a much more robust distance measure for time series, allowing similar shapes
  to match even if they are out of phase in the time axis. Because of this
  flexi- bility, DTW is widely used in science, medicine, industry and finance.
  Unfortunately, however, DTW does not obey the triangular inequality and thus
  has resisted attempts at exact indexing. Instead, many researchers have
  introduced approximate indexing techniques or abandoned the idea of indexing
  and concentrated on speeding up sequential searches. In this work, we intro-
  duce a novel technique for the exact indexing of DTW. We prove that our
  method guarantees no false dismissals and we demonstrate its vast superiority
  over all competing approaches in the largest and most comprehensive set of
  time series indexing experiments ever undertaken.%
  }
  \verb{doi}
  \verb 10.1007/s10115-004-0154-9
  \endverb
  \field{issn}{0219-1377}
  \field{number}{3}
  \field{pages}{358\bibrangedash 386}
  \field{title}{{Exact indexing of dynamic time warping}}
  \verb{url}
  \verb http://www.springerlink.com/index/10.1007/s10115-004-0154-9
  \endverb
  \field{volume}{7}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley D
  \verb esktop/Downloaded/Keogh, Ratanamahatana - 2004 - Exact indexing of dyna
  \verb mic time warping.pdf:pdf
  \endverb
  \field{journaltitle}{Knowledge and Information Systems}
  \field{year}{2004}
  \warn{\item Invalid format of field 'month'}
\endentry

\entry{Kakizawa1998}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Kakizawa}{K.}%
     {Yoshihide}{Y.}%
     {}{}%
     {}{}}%
    {{}%
     {Shumway}{S.}%
     {Robert~H}{R.~H.}%
     {}{}%
     {}{}}%
    {{}%
     {Taniguchi}{T.}%
     {Masanobu}{M.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{KYSRHTM1}
  \strng{fullhash}{KYSRHTM1}
  \field{labelalpha}{KST98}
  \field{sortinit}{K}
  \field{abstract}{%
  ... Applications to problems of clustering and classifying earthquakes and
  mining explosions are given. KEY WORDS: Chernoff; Divergence;
  Kullback-Leibler; Minimum discrimination information; Robustness; Seismology
  ; Spectral analysis. ... $\backslash$n%
  }
  \verb{doi}
  \verb 10.1080/01621459.1998.10474114
  \endverb
  \field{isbn}{0162-1459$\backslash$n1537-274X}
  \field{issn}{0162-1459}
  \field{number}{441}
  \field{pages}{328\bibrangedash 340}
  \field{title}{{Discrimination and Clustering for Multivariate Time Series}}
  \verb{url}
  \verb http://www.tandfonline.com/doi/abs/10.1080/01621459.1998.10474114$\back
  \verb slash$npapers2://publication/doi/10.1080/01621459.1998.10474114
  \endverb
  \field{volume}{93}
  \field{journaltitle}{Journal of the American Statistical Association}
  \field{year}{1998}
\endentry

\entry{Kijsirikul2002}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Kijsirikul}{K.}%
     {B}{B}%
     {}{}%
     {}{}}%
    {{}%
     {Ussivakul}{U.}%
     {N}{N}%
     {}{}%
     {}{}}%
  }
  \keyw{decision directed acyclic graph,decision levels,directed
  graphs,learning (artificial intelligence),learning automata,linear support
  vector machines,multiclass support vector machines,pattern
  classification,probability adaptive directed acyclic graph}
  \strng{namehash}{KBUN1}
  \strng{fullhash}{KBUN1}
  \field{labelalpha}{KU02}
  \field{sortinit}{K}
  \field{abstract}{%
  Presents a method of extending support vector machines (SVMs) for dealing
  with multiclass problems. Motivated by the decision directed acyclic graph
  (DDAG), we propose the adaptive DAG (ADAG): a modified structure of the DDAG
  that has a lower number of decision levels and reduces the dependency on the
  sequence of nodes. Thus, the ADAG improves the accuracy of the DDAG while
  maintaining low computational requirement%
  }
  \verb{doi}
  \verb 10.1109/IJCNN.2002.1005608
  \endverb
  \field{pages}{980\bibrangedash 985}
  \field{title}{{Multiclass Support Vector Machines using Adaptive Directed
  Acyclic Graph}}
  \field{volume}{1}
  \field{journaltitle}{Neural Networks, 2002. IJCNN '02. Proceedings of the
  2002 International Joint Conference on}
  \field{year}{2002}
\endentry

\entry{Lhermitte2011a}{article}{}
  \name{author}{4}{}{%
    {{}%
     {Lhermitte}{L.}%
     {S.}{S.}%
     {}{}%
     {}{}}%
    {{}%
     {Verbesselt}{V.}%
     {J.}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Verstraeten}{V.}%
     {W.W.}{W.}%
     {}{}%
     {}{}}%
    {{}%
     {Coppin}{C.}%
     {P.}{P.}%
     {}{}%
     {}{}}%
  }
  \keyw{Change detection,Classification,Ecosystem dynamics,Time series
  analysis}
  \strng{namehash}{LS+1}
  \strng{fullhash}{LSVJVWCP1}
  \field{labelalpha}{Lhe+11}
  \field{sortinit}{L}
  \field{abstract}{%
  Time series of remote sensing imagery or derived vegetation indices and
  biophysical products have been shown particularly useful to characterize land
  ecosystem dynamics. Various methods have been developed based on temporal
  trajectory analysis to characterize, classify and detect changes in ecosystem
  dynamics. Although time series similarity measures play an important role in
  these methods, a quantitative comparison of the similarity measures is
  lacking. The objective of this study was to provide an overview and
  quantitative comparison of the similarity measures in function of varying
  time series and ecosystem characteristics, such as amplitude, timing and
  noise effects. For this purpose, the performance was evaluated for the
  commonly used similarity measures (D), ranging from Manhattan (DMan),
  Euclidean (DE) and Mahalanobis (DMah) distance measures, to correlation
  (DCC), Principal Component Analysis (PCA; DPCA) and Fourier based
  (DFFT,D$\xi$,DFk) similarities. The quantitative comparison consists of a
  series of Monte-Carlo simulations based on subsets of global MODIS Normalized
  Difference Vegetation index (NDVI) and Enhanced Vegetation Index (EVI) and
  Leaf Area Index (LAI) data. Results of the simulations reveal four main
  groups of time series similarity measures with different sensitivities: (i)
  DMan, DE, DPCA, DFk quantify the difference in time series values, (ii) DMah
  accounts for temporal correlation and non-stationarity of variance, (iii) DCC
  measures the temporal correlation, and (iv) the Fourier based DFFT and D$\xi$
  show their specific sensitivity based on the selected Fourier components. The
  difference measures show relatively the highest sensitivity to amplitude
  effects, whereas the correlation based measures are highly sensitive to
  variations in timing and noise. The Fourier based measures, finally, depend
  highly on the signal to noise ratio and the balance between amplitude and
  phase dominance. The heterogeneity in sensitivity of each D stresses the
  importance of (i) understanding the time series characteristics before
  applying any classification of change detection approach and (ii) defining
  the variability one wants to identify/account for. This requires an
  understanding of the ecosystem dynamics and time series characteristics
  related to the baseline, amplitude, timing, noise and variability of the
  ecosystem time series. This is also illustrated in the quantitative
  comparison, where the different sensitivities of D for the NDVI, EVI, and LAI
  data relate specifically to the temporal characteristics of each data set.
  Additionally, the effect of noise and intra- and interclass variability is
  demonstrated in a case study based on land cover classification.%
  }
  \verb{doi}
  \verb 10.1016/j.rse.2011.06.020
  \endverb
  \field{isbn}{0034-4257}
  \field{issn}{00344257}
  \field{number}{12}
  \field{pages}{3129\bibrangedash 3152}
  \field{title}{{A comparison of time series similarity measures for
  classification and change detection of ecosystem dynamics}}
  \verb{url}
  \verb http://www.sciencedirect.com/science/article/pii/S0034425711002446
  \endverb
  \field{volume}{115}
  \field{journaltitle}{Remote Sensing of Environment}
  \field{year}{2011}
\endentry

\entry{Liang2012}{article}{}
  \name{author}{4}{}{%
    {{}%
     {Liang}{L.}%
     {Chunquan}{C.}%
     {}{}%
     {}{}}%
    {{}%
     {Zhang}{Z.}%
     {Yang}{Y.}%
     {}{}%
     {}{}}%
    {{}%
     {Shi}{S.}%
     {Peng}{P.}%
     {}{}%
     {}{}}%
    {{}%
     {Hu}{H.}%
     {Zhengguo}{Z.}%
     {}{}%
     {}{}}%
  }
  \list{publisher}{1}{%
    {Elsevier Inc.}%
  }
  \keyw{Positive unlabeled learning,Uncertain attribute,Uncertain data
  stream,Very fast decision tree}
  \strng{namehash}{LC+1}
  \strng{fullhash}{LCZYSPHZ1}
  \field{labelalpha}{Lia+12}
  \field{sortinit}{L}
  \field{abstract}{%
  Most data stream classification algorithms need to supply input with a large
  amount of precisely labeled data. However, in many data stream applications,
  streaming data contains inherent uncertainty, and labeled samples are
  difficult to be collected, while abundant data are unlabeled. In this paper,
  we focus on classifying uncertain data streams with only positive and
  unlabeled samples available. Based on concept-adapting very fast decision
  tree (CVFDT) algorithm, we propose an algorithm namely puuCVFDT (CVFDT for
  positive and unlabeled uncertain data). Experimental results on both
  synthetic and real-life datasets demonstrate the strong ability and
  efficiency of puuCVFDT to handle concept drift with uncertainty under
  positive and unlabeled learning scenario. Even when 90{\%} of the samples in
  the stream are unlabeled, the classification performance of the proposed
  algorithm is still compared to that of CVFDT, which is learned from fully
  labeled data without uncertainty. ?? 2012 Elsevier Inc. All rights reserved.%
  }
  \verb{doi}
  \verb 10.1016/j.ins.2012.05.023
  \endverb
  \field{issn}{00200255}
  \field{pages}{50\bibrangedash 67}
  \field{title}{{Learning very fast decision tree from uncertain data streams
  with positive and unlabeled samples}}
  \verb{url}
  \verb http://dx.doi.org/10.1016/j.ins.2012.05.023
  \endverb
  \field{volume}{213}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/Google Drive/Th{\`{e}}se/Bibliographie
  \verb /Decision Tree/Liang-Elsevier-2012{\_}Learning very fast decision tree
  \verb from uncertain data streams with positive and unlabeled samples.pdf:pdf
  \endverb
  \field{journaltitle}{Information Sciences}
  \field{year}{2012}
\endentry

\entry{Montero2014}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Montero}{M.}%
     {Pablo}{P.}%
     {}{}%
     {}{}}%
    {{}%
     {Vilar}{V.}%
     {Jos{\'{e}}}{J.}%
     {}{}%
     {}{}}%
  }
  \keyw{clustering,dissimilarity measure,time series data,validation indices}
  \strng{namehash}{MPVJ1}
  \strng{fullhash}{MPVJ1}
  \field{labelalpha}{MV14}
  \field{sortinit}{M}
  \field{abstract}{%
  Time series clustering is an active research area with applications in a wide
  range of fields. One key component in cluster analysis is determining a
  proper dissimilarity mea- sure between two data objects, and many criteria
  have been proposed in the literature to assess dissimilarity between two time
  series. The R package TSclust is aimed to im- plement a large set of
  well-established peer-reviewed time series dissimilarity measures, including
  measures based on raw data, extracted features, underlying parametric models,
  complexity levels, and forecast behaviors. Computation of these measures
  allows the user to perform clustering by using conventional clustering
  algorithms. TSclust also includes a clustering procedure based on p values
  from checking the equality of generating models, and some utilities to
  evaluate cluster solutions. The implemented dissimilarity functions are
  accessible individually for an easier extension and possible use out of the
  clustering context. The main features of TSclust are described and examples
  of its use are presented.%
  }
  \field{number}{1}
  \field{title}{{TSclust : An R Package for Time Series Clustering}}
  \verb{url}
  \verb http://www.jstatsoft.org/v62/i01/paper
  \endverb
  \field{volume}{62}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/Desktop/TS clust.pdf:pdf
  \endverb
  \field{journaltitle}{Journal of Statistical Software November}
  \field{year}{2014}
\endentry

\entry{Najmeddine2012}{inproceedings}{}
  \name{author}{4}{}{%
    {{}%
     {Najmeddine}{N.}%
     {H.}{H.}%
     {}{}%
     {}{}}%
    {{}%
     {Jay}{J.}%
     {A.}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Marechal}{M.}%
     {P.}{P.}%
     {}{}%
     {}{}}%
    {{}%
     {Mari{\'{e}}}{M.}%
     {S.}{S.}%
     {}{}%
     {}{}}%
  }
  \keyw{Data mining,INCAS.,Time series,diagnosis and decision
  support,sensors,similarity measures}
  \strng{namehash}{NH+1}
  \strng{fullhash}{NHJAMPMS1}
  \field{labelalpha}{Naj+12}
  \field{sortinit}{N}
  \field{booktitle}{RFIA}
  \field{isbn}{9782953951523}
  \field{title}{{Mesures de similarit{\'{e}} pour l'aide {\`{a}} l'analyse des
  donn{\'{e}}es {\'{e}}nerg{\'{e}}tiques de b{\^{a}}timents}}
  \verb{url}
  \verb https://hal-cea.archives-ouvertes.fr/file/index/docid/661016/filename/a
  \verb rticle53{\_}modif.pdf
  \endverb
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley D
  \verb esktop/Downloaded/Najmeddine et al. - 2012 - Mesures de similarit{\'{e}
  \verb } pour l’aide {\`{a}} l’analyse des donn{\'{e}}es {\'{e}}nerg{\'{e}
  \verb }tiques de b{\^{a}}timents.pdf:pdf
  \endverb
  \field{year}{2012}
\endentry

\entry{Nguyen2012}{inproceedings}{}
  \name{author}{5}{}{%
    {{}%
     {Nguyen}{N.}%
     {L.}{L.}%
     {}{}%
     {}{}}%
    {{}%
     {Wu}{W.}%
     {P.}{P.}%
     {}{}%
     {}{}}%
    {{}%
     {Chan}{C.}%
     {W.}{W.}%
     {}{}%
     {}{}}%
    {{}%
     {Peng}{P.}%
     {W.}{W.}%
     {}{}%
     {}{}}%
    {{}%
     {Zhang}{Z.}%
     {Y.}{Y.}%
     {}{}%
     {}{}}%
  }
  \keyw{sentiment analysis,sentiment prediction,social network analysis}
  \strng{namehash}{NL+1}
  \strng{fullhash}{NLWPCWPWZY1}
  \field{labelalpha}{Ngu+12}
  \field{sortinit}{N}
  \field{abstract}{%
  More and more people express their opinions on social media such as Facebook
  and Twitter. Predictive analysis on social media time-series allows the
  stake-holders to leverage this immediate, accessible and vast reachable
  communication channel to react and proact against the public opinion. In
  particular, understanding and predicting the sentiment change of the public
  opinions will allow business and government agencies to react against
  negative sentiment and design strategies such as dispelling rumors and post
  balanced messages to revert the public opinion. In this paper, we present a
  strategy of building statistical models from the social media dynamics to
  predict collective sentiment dynamics. We model the collective sentiment
  change without delving into micro analysis of individual tweets or users and
  their corresponding low level network structures. Experiments on large-scale
  Twitter data show that the model can achieve above 85{\%} accuracy on
  directional sentiment prediction.%
  }
  \field{booktitle}{WISDOM}
  \verb{doi}
  \verb 10.1145/2346676.2346682
  \endverb
  \field{isbn}{9781450315432}
  \field{title}{{Predicting collective sentiment dynamics from time-series
  social media}}
  \field{year}{2012}
\endentry

\entry{Duda1973}{book}{}
  \name{author}{2}{}{%
    {{}%
     {{O Duda}}{O.}%
     {Richard}{R.}%
     {}{}%
     {}{}}%
    {{}%
     {{E Hart}}{E.}%
     {Peter}{P.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{OREP1}
  \strng{fullhash}{OREP1}
  \field{labelalpha}{OE73}
  \field{sortinit}{O}
  \field{abstract}{%
  Classic book on pattern recognition. Interesting points: 1) p. 66, and p.
  114: Mentions the problems with dimensionality curse. 2) p. 243-246: Mentions
  Multidimensional scaling (MDS), Karhunen-Loeve and dimensionality reduction.
  Also, has the spiral data-set as a sample. 3) p. 333: mentions
  SVD/eigenvalues for linear fitting.%
  }
  \field{booktitle}{Leonardo}
  \verb{doi}
  \verb 10.2307/1573081
  \endverb
  \field{isbn}{0471223611}
  \field{issn}{0024094X}
  \field{pages}{482}
  \field{title}{{Pattern Classification and Scene Analysis}}
  \verb{url}
  \verb http://www.jstor.org/stable/1573081?origin=crossref
  \endverb
  \field{volume}{7}
  \field{year}{1973}
\endentry

\entry{Oncina2006}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Oncina}{O.}%
     {Jose}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Sebban}{S.}%
     {Marc}{M.}%
     {}{}%
     {}{}}%
  }
  \keyw{Finite-state transducers,Handwritten character recognition,Stochastic
  edit distance}
  \strng{namehash}{OJSM1}
  \strng{fullhash}{OJSM1}
  \field{labelalpha}{OS06}
  \field{sortinit}{O}
  \field{abstract}{%
  Many pattern recognition algorithms are based on the nearest-neighbour search
  and use the well-known edit distance, for which the primitive edit costs are
  usually fixed in advance. In this article, we aim at learning an unbiased
  stochastic edit distance in the form of a finite-state transducer from a
  corpus of (input, output) pairs of strings. Contrary to the other standard
  methods, which generally use the Expectation Maximisation algorithm, our
  algorithm learns a transducer independently on the marginal probability
  distribution of the input strings. Such an unbiased way to proceed requires
  to optimise the parameters of a conditional transducer instead of a joint
  one. We apply our new model in the context of handwritten digit recognition.
  We show, carrying out a large series of experiments, that it always
  outperforms the standard edit distance. ?? 2006 Pattern Recognition Society.%
  }
  \verb{doi}
  \verb 10.1016/j.patcog.2006.03.011
  \endverb
  \field{isbn}{0031-3203}
  \field{issn}{00313203}
  \field{pages}{1575\bibrangedash 1587}
  \field{title}{{Learning stochastic edit distance: Application in handwritten
  character recognition}}
  \field{volume}{39}
  \field{journaltitle}{Pattern Recognition}
  \field{year}{2006}
\endentry

\entry{PANAGIOTAKIS2008}{article}{}
  \name{author}{5}{}{%
    {{}%
     {PANAGIOTAKIS}{P.}%
     {COSTAS}{C.}%
     {}{}%
     {}{}}%
    {{}%
     {RAMASSO}{R.}%
     {EMMANUEL}{E.}%
     {}{}%
     {}{}}%
    {{}%
     {TZIRITAS}{T.}%
     {GEORGIOS}{G.}%
     {}{}%
     {}{}}%
    {{}%
     {ROMBAUT}{R.}%
     {MICH{\`{E}}LE}{M.}%
     {}{}%
     {}{}}%
    {{}%
     {PELLERIN}{P.}%
     {DENIS}{D.}%
     {}{}%
     {}{}}%
  }
  \keyw{People detection,people counting,team activity recognition,transferable
  belief model,video analysis}
  \strng{namehash}{PC+1}
  \strng{fullhash}{PCRETGRMPD1}
  \field{labelalpha}{PAN+08}
  \field{sortinit}{P}
  \field{abstract}{%
  We present a shape-based method for automatic people detection and counting
  without any assumption concerning camera motion. In order to evaluate the
  robustness of the proposed method, we apply it for classifying athletics
  videos into two classes: videos of individual and videos of team sports. The
  videos used are real and characterized by dynamic and unconstrained
  environment. Moreover, in the case of team sport, we propose a shape
  deformations based method for running/hurdling discrimination (activity
  recognition). Robust, adaptive and independent from color, illumination
  changes and the camera motion, the proposed features are combined in the
  Transferable Belief Model (TBM) framework providing a two-level (frames and
  shot) video categorization. Experimental results of 97{\%} of accuracy for
  individual/team sport categorization using a dataset of 252 real videos of
  athletic meetings, acquired by moving cameras under varying view angles,
  indicate the stability and the good performance of the proposed scheme.%
  }
  \verb{doi}
  \verb 10.1142/S0218001408006752
  \endverb
  \field{issn}{0218-0014}
  \field{number}{06}
  \field{pages}{1187\bibrangedash 1213}
  \field{title}{{SHAPE-BASED INDIVIDUAL/GROUP DETECTION FOR SPORT VIDEOS
  CATEGORIZATION}}
  \verb{url}
  \verb http://www.worldscientific.com/doi/abs/10.1142/S0218001408006752?journa
  \verb lCode=ijprai
  \endverb
  \field{volume}{22}
  \field{journaltitle}{International Journal of Pattern Recognition and
  Artificial Intelligence}
  \field{year}{2008}
\endentry

\entry{Prekopcsak2012}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Prekopcs{\'{a}}k}{P.}%
     {Zolt{\'{a}}n}{Z.}%
     {}{}%
     {}{}}%
    {{}%
     {Lemire}{L.}%
     {Daniel}{D.}%
     {}{}%
     {}{}}%
  }
  \keyw{Distance measure learning,Mahalanobis distance measure,Nearest
  Neighbor,Time-series classification}
  \strng{namehash}{PZLD1}
  \strng{fullhash}{PZLD1}
  \field{labelalpha}{PL12}
  \field{sortinit}{P}
  \field{abstract}{%
  To classify time series by nearest neighbors, we need to specify or learn one
  or several distance measures. We consider variations of the Mahalanobis
  distance measures which rely on the inverse covariance matrix of the data.
  Unfortunately --- for time series data --- the covariance matrix has often
  low rank. To alleviate this problem we can either use a pseudoinverse,
  covariance shrinking or limit the matrix to its diagonal. We review these
  alternatives and benchmark them against competitive methods such as the
  related Large Margin Nearest Neighbor Classification (LMNN) and the Dynamic
  Time Warping (DTW) distance. As we expected, we find that the DTW is
  superior, but the Mahalanobis distance measures are one to two orders of
  magnitude faster. To get best results with Mahalanobis distance measures, we
  recommend learning one distance measure per class using either covariance
  shrinking or the diagonal approach.%
  }
  \verb{doi}
  \verb 10.1007/s11634-012-0110-6
  \endverb
  \verb{eprint}
  \verb 1010.1526
  \endverb
  \field{isbn}{1163401201106}
  \field{issn}{18625347}
  \field{number}{3}
  \field{pages}{185\bibrangedash 200}
  \field{title}{{Time series classification by class-specific Mahalanobis
  distance measures}}
  \field{volume}{6}
  \field{journaltitle}{Advances in Data Analysis and Classification}
  \field{eprinttype}{arXiv}
  \field{year}{2012}
\endentry

\entry{Pcekalska2002}{article}{}
  \name{author}{3}{}{%
    {{}%
     {P$\backslash$c{\{}e{\}}kalska}{P.}%
     {El$\backslash$.{\{}z{\}}bieta}{E.}%
     {}{}%
     {}{}}%
    {{}%
     {Paclik}{P.}%
     {Pavel}{P.}%
     {}{}%
     {}{}}%
    {{}%
     {Duin}{D.}%
     {Robert P~W}{R.~P.~W.}%
     {}{}%
     {}{}}%
  }
  \keyw{dissimilarity,embedding,fisher linear discriminant,nearest mean
  classifier,pseudo euclidean space,support vector classifier}
  \strng{namehash}{PEPPDRPW1}
  \strng{fullhash}{PEPPDRPW1}
  \field{labelalpha}{PPD02}
  \field{sortinit}{P}
  \field{abstract}{%
  Usually, objects to be classified are represented by features. In this paper,
  we discuss an alternative object representation based on dissimilarity
  values. If such distances separate the classes well, the nearest neighbor
  method offers a good solution. However, dissimilarities used in practice are
  usually far from ideal and the performance of the nearest neighbor rule
  suffers from its sensitivity to noisy examples. We show that other, more
  global classification techniques are preferable to the nearest neighbor rule,
  in such cases.For classification purposes, two different ways of using
  generalized dissimilarity kernels are considered. In the first one, distances
  are isometrically embedded in a pseudo-Euclidean space and the classification
  task is performed there. In the second approach, classifiers are built
  directly on distance kernels. Both approaches are described theoretically and
  then compared using experiments with different dissimilarity measures and
  datasets including degraded data simulating the problem of missing values.
  http://www.crossref.org/jmlr{\_}DOI.html%
  }
  \verb{doi}
  \verb 10.1162/15324430260185592
  \endverb
  \field{issn}{15324435}
  \field{number}{2}
  \field{pages}{175\bibrangedash 211}
  \field{title}{{A Generalized Kernel Approach to Dissimilarity-based
  Classification}}
  \field{volume}{2}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley D
  \verb esktop/Downloaded/Pc{\{}e{\}}kalska, Paclik, Duin - 2002 - A Generalize
  \verb d Kernel Approach to Dissimilarity-based Classification.pdf:pdf
  \endverb
  \field{journaltitle}{Journal of Machine Learning Research}
  \field{year}{2002}
\endentry

\entry{Rabiner1989}{article}{}
  \name{author}{1}{}{%
    {{}%
     {Rabiner}{R.}%
     {L.R.}{L.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{RL1}
  \strng{fullhash}{RL1}
  \field{labelalpha}{Rab89}
  \field{sortinit}{R}
  \field{abstract}{%
  This tutorial provides an overview of the basic theory of
  hidden$\backslash$nMarkov models (HMMs) as originated by L.E. Baum and T.
  Petrie (1966) and$\backslash$ngives practical details on methods of
  implementation of the theory along$\backslash$nwith a description of selected
  applications of the theory to distinct$\backslash$nproblems in speech
  recognition. Results from a number of original$\backslash$nsources are
  combined to provide a single source of acquiring the$\backslash$nbackground
  required to pursue further this area of research. The
  author$\backslash$nfirst reviews the theory of discrete Markov chains and
  shows how the$\backslash$nconcept of hidden states, where the observation is
  a probabilistic$\backslash$nfunction of the state, can be used effectively.
  The theory is$\backslash$nillustrated with two simple examples, namely
  coin-tossing, and the$\backslash$nclassic balls-in-urns system. Three
  fundamental problems of HMMs are$\backslash$nnoted and several practical
  techniques for solving these problems are$\backslash$ngiven. The various
  types of HMMs that have been studied, including$\backslash$nergodic as well
  as left-right models, are described%
  }
  \verb{doi}
  \verb 10.1109/5.18626
  \endverb
  \field{isbn}{1558601244}
  \field{issn}{00189219}
  \field{number}{2}
  \field{pages}{257\bibrangedash 286}
  \field{title}{{A tutorial on hidden Markov models and selected applications
  in speech recognition}}
  \field{volume}{77}
  \field{journaltitle}{Proceedings of the IEEE}
  \field{year}{1989}
\endentry

\entry{Ramasso2008}{article}{}
  \name{author}{4}{}{%
    {{}%
     {Ramasso}{R.}%
     {E.}{E.}%
     {}{}%
     {}{}}%
    {{}%
     {Panagiotakis}{P.}%
     {C.}{C.}%
     {}{}%
     {}{}}%
    {{}%
     {Pellerin}{P.}%
     {D.}{D.}%
     {}{}%
     {}{}}%
    {{}%
     {Rombaut}{R.}%
     {M.}{M.}%
     {}{}%
     {}{}}%
  }
  \keyw{Human action recognition,Moving camera,Temporal Belief
  Filter,Transferable Belief Model}
  \strng{namehash}{RE+1}
  \strng{fullhash}{REPCPDRM1}
  \field{labelalpha}{Ram+08}
  \field{sortinit}{R}
  \field{abstract}{%
  This paper focuses on human behavior recognition where the main
  problem$\backslash$nis to bridge the semantic gap between the analogue
  observations of the$\backslash$nreal world and the symbolic world of human
  interpretation. For that, a$\backslash$nfusion architecture based on the
  Transferable Belief Model framework is$\backslash$nproposed and applied to
  action recognition of an athlete in video$\backslash$nsequences of athletics
  meeting with moving camera. Relevant features are$\backslash$nextracted from
  videos, based on both the camera motion analysis and the$\backslash$ntracking
  of particular points on the athlete's silhouette. Some models$\backslash$nof
  interpretation are used to link the numerical features to the
  symbols$\backslash$nto be recognized, which are running, jumping and falling
  actions. A$\backslash$nTemporal Belief Filter is then used to improve the
  robustness of action$\backslash$nrecognition. The proposed approach
  demonstrates good performance when$\backslash$ntested on real videos of
  athletics sports videos (high jumps, pole$\backslash$nvaults, triple jumps
  and long jumps) acquired by a moving camera and$\backslash$ndifferent view
  angles. The proposed system is also compared to
  Bayesian$\backslash$nNetworks.%
  }
  \verb{doi}
  \verb 10.1007/s10044-007-0073-y
  \endverb
  \field{issn}{14337541}
  \field{number}{1}
  \field{pages}{1\bibrangedash 19}
  \field{title}{{Human action recognition in videos based on the transferable
  belief model : AAAApplication to athletics jumps}}
  \field{volume}{11}
  \field{journaltitle}{Pattern Analysis and Applications}
  \field{year}{2008}
\endentry

\entry{Rydell2008a}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Rydell}{R.}%
     {Joakim}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Borga}{B.}%
     {Magnus}{M.}%
     {}{}%
     {}{}}%
    {{}%
     {Knutsson}{K.}%
     {Hans}{H.}%
     {}{}%
     {}{}}%
  }
  \keyw{biomedical MRI,canonical correlation analysis,correlation
  estimator,correlation methods,general linear model,locally corrupted
  signals,medical signal processing,multivariate correlation analysis,robust
  correlation analysis,signal detection,synthetic functional MRI data}
  \strng{namehash}{RJBMKH1}
  \strng{fullhash}{RJBMKH1}
  \field{labelalpha}{RBK08}
  \field{sortinit}{R}
  \field{abstract}{%
  Correlation is often used to measure the similarity between signals and is an
  important tool in signal and image processing. In some applications it is
  common that signals are corrupted by local bursts of noise. This adversely
  affects the performance of signal recognition algorithms. This paper presents
  a novel correlation estimator, which is robust to locally corrupted signals.
  The estimator is generalized to multivariate correlation analysis (general
  linear model, GLM, and canonical correlation analysis, CCA). Synthetic
  functional MRI data is used to demonstrate the estimator, and its robustness
  is shown to increase the performance of signal detection.%
  }
  \verb{doi}
  \verb 10.1109/ICASSP.2008.4517644
  \endverb
  \field{isbn}{1424414849}
  \field{issn}{15206149}
  \field{pages}{453\bibrangedash 456}
  \field{title}{{Robust correlation analysis with an application to functional
  MRI}}
  \verb{url}
  \verb http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4517644
  \endverb
  \field{journaltitle}{Acoustics, Speech and Signal {\ldots}}
  \field{year}{2008}
\endentry

\entry{Reyes2011}{article}{}
  \name{author}{1}{}{%
    {{}%
     {Reyes}{R.}%
     {Miguel}{M.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{RM1}
  \strng{fullhash}{RM1}
  \field{labelalpha}{Rey11}
  \field{sortinit}{R}
  \field{abstract}{%
  We present a gesture recognition approach for depth video data based on a
  novel Feature Weighting approach within the Dynamic Time Warping framework.
  Depth features from human joints are compared through video sequences using
  Dynamic Time Warping, and weights are assigned to features based on
  inter-intra class gesture variability. Feature Weighting in Dynamic Time
  Warping is then applied for recognizing begin-end of gestures in data
  sequences. The obtained results recognizing several gestures in depth data
  show high performance compared with classical Dynamic Time Warping approach.%
  }
  \verb{doi}
  \verb 10.1109/ICCVW.2011.6130384
  \endverb
  \field{isbn}{978-1-4673-0063-6}
  \field{pages}{1182\bibrangedash 1188}
  \field{title}{{Feature weighting in dynamic time warping for gesture
  recognition in depth data}}
  \verb{url}
  \verb http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6130384
  \endverb
  \field{journaltitle}{Computer Vision Workshops (ICCV Workshops), 2011 IEEE
  International Conference on}
  \field{year}{2011}
\endentry

\entry{Rabiner1993}{book}{}
  \name{author}{2}{}{%
    {{}%
     {Rabiner}{R.}%
     {L.}{L.}%
     {}{}%
     {}{}}%
    {{}%
     {Juang}{J.}%
     {B.}{B.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{RLJB1}
  \strng{fullhash}{RLJB1}
  \field{labelalpha}{RJ93}
  \field{sortinit}{R}
  \field{abstract}{%
  Provides a theoretically sound, technically accurate, and complete
  description of the basic knowledge and ideas that constitute a modern system
  for speech recognition by machine. Covers production, perception, and
  acoustic-phonetic characterization of the speech signal; signal processing
  and analysis methods for speech recognition; pattern comparison techniques;
  speech recognition system design and implementation; theory and
  implementation of hidden Markov models; speech recognition based on connected
  word models; large vocabulary continuous speech recognition; and task-
  oriented application of automatic speech recognition. For practicing
  engineers, scientists, linguists, and programmers interested in speech
  recognition.%
  }
  \field{booktitle}{Prentice Hall}
  \verb{doi}
  \verb 10.1002/ev.1647
  \endverb
  \field{isbn}{0130151572}
  \field{title}{{Fundamentals of Speech Recognition}}
  \verb{url}
  \verb http://cmp.felk.cvut.cz/cmp/support/phd112.html
  \endverb
  \field{volume}{103}
  \field{year}{1993}
\endentry

\entry{Son2008}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Son}{S.}%
     {Young~Sook}{Y.~S.}%
     {}{}%
     {}{}}%
    {{}%
     {Baek}{B.}%
     {Jangsun}{J.}%
     {}{}%
     {}{}}%
  }
  \keyw{Clustering,Modified correlation coefficient,Pearson’s correlation
  coefficient,Similarity,Spearmann’s correlation coefficient,Time-course gene
  expression data}
  \strng{namehash}{SYSBJ1}
  \strng{fullhash}{SYSBJ1}
  \field{labelalpha}{SB08}
  \field{sortinit}{S}
  \field{abstract}{%
  Gene expression levels are often measured consecutively in time through
  microarray experiments to detect cellular processes underlying regulatory
  effects observed and to assign functionality to genes whose function is yet
  unknown. Clustering methods allow us to group genes that show similar
  time-course expression profiles and that are thus likely to be co-regulated.
  The correlation coefficient, the most well-liked similarity measure in the
  context of gene expression data, is not very reliable in representing the
  association of two temporal profile patterns. Moreover, the clustering
  methods with the correlation coefficient generate the same clustering result
  even when the time points are permuted arbitrarily. We propose a new
  similarity measure for clustering time-course gene expression data. The
  proposed measure is based on the correlation coefficient and the two indices
  representing the concordance of temporal profile patterns and that of the
  time points at which maximum and minimum expression levels are measured
  between two profiles, respectively. We applied the hierarchical clustering
  method with the proposed similarity measure to both synthetic and breast
  cancer cell line data. We observed favorable results compared to the
  correlation coefficient based method. The proposed similarity measure is
  simple to implement, and it is much more consistent for clustering than the
  correlation coefficient based method according to the cross-validation
  criterion.%
  }
  \verb{doi}
  \verb 10.1016/j.patrec.2007.09.015
  \endverb
  \field{isbn}{0167-8655}
  \field{issn}{01678655}
  \field{number}{3}
  \field{pages}{232\bibrangedash 242}
  \field{title}{{A modified correlation coefficient based similarity measure
  for clustering time-course gene expression data}}
  \verb{url}
  \verb http://www.sciencedirect.com/science/article/pii/S0167865507003005
  \endverb
  \field{volume}{29}
  \field{journaltitle}{Pattern Recognition Letters}
  \field{year}{2008}
\endentry

\entry{Salvador}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Salvador}{S.}%
     {Stan}{S.}%
     {}{}%
     {}{}}%
    {{}%
     {Chan}{C.}%
     {Philip}{P.}%
     {}{}%
     {}{}}%
  }
  \keyw{dynamic time warping,time series}
  \strng{namehash}{SSCP1}
  \strng{fullhash}{SSCP1}
  \field{labelalpha}{SC}
  \field{sortinit}{S}
  \field{abstract}{%
  The dynamic time warping (DTW) algorithm is able to find the optimal
  alignment between two time series. It is often used to determine time series
  similarity, classification, and to find corresponding regions between two
  time series. DTW has a quadratic time and space complexity that limits its
  use to only small time series data sets. In this paper we introduce FastDTW,
  an approximation of DTW that has a linear time and space complexity. FastDTW
  uses a multilevel approach that recursively projects a solution from a coarse
  resolution and refines the projected solution. We prove the linear time and
  space complexity of FastDTW both theoretically and empirically. We also
  analyze the accuracy of FastDTW compared to two other existing approximate
  DTW algorithms: Sakoe-Chuba Bands and Data Abstraction. Our results show a
  large improvement in accuracy over the existing methods.%
  }
  \field{title}{{FastDTW : Toward Accurate Dynamic Time Warping in Linear Time
  and Space}}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley D
  \verb esktop/Downloaded/Salvador, Chan - Unknown - FastDTW Toward Accurate Dy
  \verb namic Time Warping in Linear Time and Space.pdf:pdf
  \endverb
\endentry

\entry{Sakoe1978a}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Sakoe}{S.}%
     {H.}{H.}%
     {}{}%
     {}{}}%
    {{}%
     {Chiba}{C.}%
     {S.}{S.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{SHCS1}
  \strng{fullhash}{SHCS1}
  \field{labelalpha}{SC78}
  \field{extraalpha}{1}
  \field{sortinit}{S}
  \field{abstract}{%
  This paper reports on an optimum dynamic programming (DP) based
  time-normalization algorithm for spoken word recognition. First, a general
  principle of time-normalization is given using timewarping function. Then,
  two time-normalized distance definitions, d e d symmetric and asymmetric
  forms, are derived from the principle. These two forms are compared with each
  other through theoretical discussions and experimental studies. The symmetric
  form algorithm superiority is established. A new technique, called slope
  constraint, is successfully introduced, iwn hich the warping function slope
  isr estricted so as to improve discrimination between words in different
  categories. The effective slope constraint characteristic is qualitatively
  analyzed, and the optimum slope constraint condition is determined through
  experiments. The optimized algorithm is then extensively subjected to
  experimentat comparison with various DP-algorithms, previously applied to
  spoken word recognition by different research groups. The experiment shows
  that the present algorithm gives no more than about twothirds errors, even
  compared to the best conventional algorithm.%
  }
  \verb{doi}
  \verb 10.1109/TASSP.1978.1163055
  \endverb
  \field{isbn}{1558601244}
  \field{issn}{00963518}
  \field{title}{{Dynamic Programming Algorithm Optimization for Spoken Word
  Recognition}}
  \field{journaltitle}{IEEE transactions on acoustics, speech, and signal
  processing}
  \field{year}{1978}
\endentry

\entry{Sakoe1978b}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Sakoe}{S.}%
     {Hiroaki}{H.}%
     {}{}%
     {}{}}%
    {{}%
     {Chiba}{C.}%
     {Seibi}{S.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{SHCS2}
  \strng{fullhash}{SHCS2}
  \field{labelalpha}{SC78}
  \field{extraalpha}{2}
  \field{sortinit}{S}
  \field{abstract}{%
  This paper reports on an optimum dynamic programming (DP) based
  time-normalization algorithm for spoken word recognition. First, a general
  principle of time-normalization is given using timewarping function. Then,
  two time-normalized distance definitions, d e d symmetric and asymmetric
  forms, are derived from the principle. These two forms are compared with each
  other through theoretical discussions and experimental studies. The symmetric
  form algorithm superiority is established. A new technique, called slope
  constraint, is successfully introduced, iwn hich the warping function slope
  isr estricted so as to improve discrimination between words in different
  categories. The effective slope constraint characteristic is qualitatively
  analyzed, and the optimum slope constraint condition is determined through
  experiments. The optimized algorithm is then extensively subjected to
  experimentat comparison with various DP-algorithms, previously applied to
  spoken word recognition by different research groups. The experiment shows
  that the present algorithm gives no more than about twothirds errors, even
  compared to the best conventional algorithm.%
  }
  \verb{doi}
  \verb 10.1109/TASSP.1978.1163055
  \endverb
  \field{isbn}{1558601244}
  \field{issn}{00963518}
  \field{number}{1}
  \field{pages}{43\bibrangedash 49}
  \field{title}{{Dynamic Programming Algorithm Optimization for Spoken Word
  Recognition}}
  \field{volume}{ASSP-26}
  \field{journaltitle}{IEEE transactions on acoustics, speech, and signal
  processing}
  \field{year}{1978}
\endentry

\entry{Shental2002}{article}{}
  \name{author}{4}{}{%
    {{}%
     {Shental}{S.}%
     {Noam}{N.}%
     {}{}%
     {}{}}%
    {{}%
     {Hertz}{H.}%
     {Tomer}{T.}%
     {}{}%
     {}{}}%
    {{}%
     {Weinshall}{W.}%
     {Daphna}{D.}%
     {}{}%
     {}{}}%
    {{}%
     {Pavel}{P.}%
     {Misha}{M.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{SN+1}
  \strng{fullhash}{SNHTWDPM1}
  \field{labelalpha}{She+02}
  \field{sortinit}{S}
  \field{abstract}{%
  We present a method for training a similarity metric from data. The method
  can be used for recognition or verification applications where the number of
  categories is very large and not known during training, and where the number
  of training samples for a single category is very small. The idea is to learn
  a function that maps input patterns into a target space such that the L1 norm
  in the target space approximates the "semantic" distance in the input space.
  The method is applied to a face verification task. The learning process
  minimizes a discriminative loss function that drives the similarity metric to
  be small for pairs of faces from the same person, and large for pairs from
  different persons. The mapping from raw to the target space is a
  convolutional network whose architecture is designed for robustness to
  geometric distortions. The system is tested on the Purdue/AR face database
  which has a very high degree of variability in the pose, lighting,
  expression, position, and artificial occlusions such as dark glasses and
  obscuring scarves.%
  }
  \verb{doi}
  \verb 10.1007/3-540-47979-1{\_}52
  \endverb
  \field{isbn}{978-3-540-43748-2}
  \field{pages}{776\bibrangedash 790}
  \field{title}{{Adjustment Learning and Relevant Component Analysis}}
  \verb{url}
  \verb http://dx.doi.org/10.1007/3-540-47979-1{\_}52
  \endverb
  \field{volume}{2353}
  \field{journaltitle}{European Conference on Computer Vision (ECCV)}
  \field{year}{2002}
\endentry

\entry{Silverman1989}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Silverman}{S.}%
     {B~W}{B.~W.}%
     {}{}%
     {}{}}%
    {{}%
     {Jones}{J.}%
     {M~C}{M.~C.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{SBWJMC1}
  \strng{fullhash}{SBWJMC1}
  \field{labelalpha}{SJ89}
  \field{sortinit}{S}
  \field{abstract}{%
  In 1951, Evelyn Fix and J.L. Hodges, Jr. wrote a technical report which
  contained prophetic work on nonparametric discriminant analysis and
  probability density estimation, and which was never published by the authors.
  The report introduced several important concepts for the first time. It is of
  interest not only for historical reasons but also because it contains much
  material that is still of contemporary relevance. Here, the report is printed
  in full together with a commentary placing the paper in context and
  interpreting its ideas in the light of more modern developments. /// En 1951,
  E. Fix et J.L. Hodges, Jr. ont �crit un rapport technique proph�tique sur
  l'analyse non-param�trique de discrimination et l'estimation de la
  densit� de probabilit�, mais celui-ci ne fut jamais publi� par ses
  auteurs. Ce rapport introduit plusieurs id�es nouvelles et importantes. Il
  nous int�resse non seulement pour des raisons historiques, mais aussi parce
  qu'il contient des concepts qui sont encore importants de nos jours. Nous le
  publions ici en entier, accompagn� d'un commentaire qui l'interpr�te d'un
  point de vue plus moderne.%
  }
  \verb{doi}
  \verb 10.2307/1403796
  \endverb
  \field{isbn}{03067734}
  \field{issn}{03067734}
  \field{number}{3}
  \field{pages}{pp. 233\bibrangedash 238}
  \field{title}{{E. Fix and J.L. Hodges (1951): An Important Contribution to
  Nonparametric Discriminant Analysis and Density Estimation: Commentary on Fix
  and Hodges (1951)}}
  \verb{url}
  \verb http://www.jstor.org/stable/1403796
  \endverb
  \field{volume}{57}
  \field{journaltitle}{International Statistical Review / Revue Internationale
  de Statistique}
  \field{year}{1989}
\endentry

\entry{Sahidullah2012a}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Sahidullah}{S.}%
     {M.}{M.}%
     {}{}%
     {}{}}%
    {{}%
     {Saha}{S.}%
     {G.}{G.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{SMSG1}
  \strng{fullhash}{SMSG1}
  \field{labelalpha}{SS12}
  \field{extraalpha}{1}
  \field{sortinit}{S}
  \field{number}{4}
  \field{pages}{543\bibrangedash 565}
  \field{title}{{Design, analysis and experimental evaluation of block based
  transformation in mfcc computation for speaker recognition}}
  \field{volume}{54}
  \field{journaltitle}{Speech Communication}
  \field{year}{2012}
\endentry

\entry{Sahidullah2012}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Sahidullah}{S.}%
     {Md}{M.}%
     {}{}%
     {}{}}%
    {{}%
     {Saha}{S.}%
     {Goutam}{G.}%
     {}{}%
     {}{}}%
  }
  \keyw{Block transform,Correlation matrix,DCT,Decorrelation technique,Linear
  transformation,MFCC,Missing feature theory,Narrow-band noise,Speaker
  recognition}
  \strng{namehash}{SMSG2}
  \strng{fullhash}{SMSG2}
  \field{labelalpha}{SS12}
  \field{extraalpha}{2}
  \field{sortinit}{S}
  \field{abstract}{%
  Standard Mel frequency cepstrum coefficient (MFCC) computation technique
  utilizes discrete cosine transform (DCT) for decorrelating log energies of
  filter bank output. The use of DCT is reasonable here as the covariance
  matrix of Mel filter bank log energy (MFLE) can be compared with that of
  highly correlated Markov-I process. This full-band based MFCC computation
  technique where each of the filter bank output has contribution to all
  coefficients, has two main disadvantages. First, the covariance matrix of the
  log energies does not exactly follow Markov-I property. Second, full-band
  based MFCC feature gets severely degraded when speech signal is corrupted
  with narrow-band channel noise, though few filter bank outputs may remain
  unaffected. In this work, we have studied a class of linear transformation
  techniques based on block wise transformation of MFLE which effectively
  decorrelate the filter bank log energies and also capture speech information
  in an efficient manner. A thorough study has been carried out on the block
  based transformation approach by investigating a new partitioning technique
  that highlights associated advantages. This article also reports a novel
  feature extraction scheme which captures complementary information to wide
  band information; that otherwise remains undetected by standard MFCC and
  proposed block transform (BT) techniques. The proposed features are evaluated
  on NIST SRE databases using Gaussian mixture model-universal background model
  (GMM-UBM) based speaker recognition system. We have obtained significant
  performance improvement over baseline features for both matched and
  mismatched condition, also for standard and narrow-band noises. The proposed
  method achieves significant performance improvement in presence of
  narrow-band noise when clubbed with missing feature theory based score
  computation scheme. ?? 2011 Elsevier B.V. All rights reserved.%
  }
  \verb{doi}
  \verb 10.1016/j.specom.2011.11.004
  \endverb
  \field{isbn}{0167-6393}
  \field{issn}{01676393}
  \field{number}{4}
  \field{pages}{543\bibrangedash 565}
  \field{title}{{Design, analysis and experimental evaluation of block based
  transformation in MFCC computation for speaker recognition}}
  \verb{url}
  \verb http://dx.doi.org/10.1016/j.specom.2011.11.004
  \endverb
  \field{volume}{54}
  \field{journaltitle}{Speech Communication}
  \field{year}{2012}
\endentry

\entry{Schlkopf2013}{book}{}
  \name{author}{2}{}{%
    {{}%
     {Schlkopf}{S.}%
     {Bernhard}{B.}%
     {}{}%
     {}{}}%
    {{}%
     {Smola}{S.}%
     {Alexander~J.}{A.~J.}%
     {}{}%
     {}{}}%
  }
  \keyw{icle}
  \strng{namehash}{SBSAJ1}
  \strng{fullhash}{SBSAJ1}
  \field{labelalpha}{SS13}
  \field{sortinit}{S}
  \field{abstract}{%
  Predicting the binding mode of flexible polypeptides to proteins is an
  important task that falls outside the domain of applicability of most small
  molecule and protein−protein docking tools. Here, we test the small
  molecule flexible ligand docking program Glide on a set of 19
  non-$\alpha$-helical peptides and systematically improve pose prediction
  accuracy by enhancing Glide sampling for flexible polypeptides. In addition,
  scoring of the poses was improved by post-processing with physics-based
  implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10
  scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the
  interface backbone atoms) increased from 21{\%} with default Glide SP
  settings to 58{\%} with the enhanced peptide sampling and scoring protocol in
  the case of redocking to the native protein structure. This approaches the
  accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success
  for these 19 peptides) while being over 100 times faster. Cross-docking was
  performed for a subset of cases where an unbound receptor structure was
  available, and in that case, 40{\%} of peptides were docked successfully. We
  analyze the results and find that the optimized polypeptide protocol is most
  accurate for extended peptides of limited size and number of formal charges,
  defining a domain of applicability for this approach.%
  }
  \field{booktitle}{Journal of Chemical Information and Modeling}
  \verb{doi}
  \verb 10.1017/CBO9781107415324.004
  \endverb
  \verb{eprint}
  \verb arXiv:1011.1669v3
  \endverb
  \field{isbn}{9788578110796}
  \field{issn}{1098-6596}
  \field{pages}{1689\bibrangedash 1699}
  \field{title}{{Learning with Kernels}}
  \field{volume}{53}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/Google Drive/Th{\`{e}}se/Bibliographie
  \verb /Livre/Schokopf, Smola-Learning with Kernels.pdf:pdf
  \endverb
  \field{eprinttype}{arXiv}
  \field{year}{2013}
\endentry

\entry{Sadri2003}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Sadri}{S.}%
     {Javad}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Suen}{S.}%
     {Ching~Y}{C.~Y.}%
     {}{}%
     {}{}}%
    {{}%
     {Bui}{B.}%
     {Tien~D.}{T.~D.}%
     {}{}%
     {}{}}%
  }
  \keyw{feature extraction,machine learning,mlp neural network,multiple support
  vector classifiers,ocr,optical character recognition,support,svm,vector
  machine}
  \strng{namehash}{SJSCYBTD1}
  \strng{fullhash}{SJSCYBTD1}
  \field{labelalpha}{SSB03}
  \field{sortinit}{S}
  \field{abstract}{%
  : A new method for recognition of isolated handwritten Arabic/Persian digits
  is presented. This method is based on Support Vector Machines (SVMs), and a
  new approach of feature extraction. Each digit is considered from four
  different views, and from each view 16 features are extracted and combined to
  obtain 64 features. Using these features, multiple SVM classifiers are
  trained to separate different classes of digits. CENPARMI Indian
  (Arabic/Persian) handwritten digit database is used for training and testing
  of SVM classifiers. Based on this database, differences between Arabic and
  Persian digits in digit recognition are shown. This database provides 7390
  samples for training and 3035 samples for testing from the real life samples.
  Experiments show that the proposed features can provide a very good
  recognition result using Support Vector Machines at a recognition rate
  94.14{\%}, compared with 91.25 {\%} obtained by MLP neural network classifier
  using the same features and test set.%
  }
  \field{pages}{300\bibrangedash 307}
  \field{title}{{Application of Support Vector Machines for recognition of
  handwritten Arabic/Persian digits}}
  \field{volume}{1}
  \field{journaltitle}{Second Conference on Machine Vision and Image Processing
  {\&} Applications (MVIP 2003)}
  \field{year}{2003}
\endentry

\entry{Torrence1998}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Torrence}{T.}%
     {Christopher}{C.}%
     {}{}%
     {}{}}%
    {{}%
     {Compo}{C.}%
     {Gilbert~P.}{G.~P.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{TCCGP1}
  \strng{fullhash}{TCCGP1}
  \field{labelalpha}{TC98}
  \field{sortinit}{T}
  \field{abstract}{%
  A practical step-by-step guide to wavelet analysis is given, with examples
  taken from time series of the El Ni{\~{n}}o–Southern Oscillation (ENSO).
  The guide includes a comparison to the windowed Fourier transform, the choice
  of an appropriate wavelet basis function, edge effects due to finite-length
  time series, and the relationship between wavelet scale and Fourier
  frequency. New statistical significance tests for wavelet power spectra are
  developed by deriving theoretical wavelet spectra for white and red noise
  processes and using these to establish significance levels and confidence
  intervals. It is shown that smoothing in time or scale can be used to
  increase the confidence of the wavelet spectrum. Empirical formulas are given
  for the effect of smoothing on significance levels and confidence intervals.
  Extensions to wavelet analysis such as filtering, the power Hovm{\"{o}}ller,
  cross-wavelet spectra, and coherence are described. The statistical
  significance tests are used to give a quantitative measure of changes in ENSO
  variance on interdecadal timescales. Using new datasets that extend back to
  1871, the Ni{\~{n}}o3 sea surface temperature and the Southern Oscillation
  index show significantly higher power during 1880–1920 and 1960–90, and
  lower power during 1920–60, as well as a possible 15-yr modulation of
  variance. The power Hovm{\"{o}}ller of sea level pressure shows significant
  variations in 2–8-yr wavelet power in both longitude and time.%
  }
  \verb{doi}
  \verb 10.1175/1520-0477(1998)079<0061:APGTWA>2.0.CO;2
  \endverb
  \field{isbn}{0871706881}
  \field{issn}{00030007}
  \field{number}{1}
  \field{pages}{61\bibrangedash 78}
  \field{title}{{A Practical Guide to Wavelet Analysis}}
  \field{volume}{79}
  \field{journaltitle}{Bulletin of the American Meteorological Society}
  \field{year}{1998}
\endentry

\entry{Wang2002}{article}{}
  \name{author}{1}{}{%
    {{}%
     {Wang}{W.}%
     {Jung-Ying}{J.-Y.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{WJY1}
  \strng{fullhash}{WJY1}
  \field{labelalpha}{Wan02}
  \field{sortinit}{W}
  \field{abstract}{%
  Recently a new learning method called support vector machines (SVM) has shown
  comparable or better results than neural networks on some applications. In
  this thesis we exploit the possibility of using SVM for three important
  issues of bioinformatics: the prediction of protein secondary structure,
  multi-class protein fold recognition, and the prediction of human signal
  peptide cleavage sites. By using similar data, we demonstrate that SVM can
  easily achieve comparable accuracy as using neural networks. Therefore, in
  the future it is a promising direction to apply SVM on more bioinformatics
  applications.%
  }
  \field{pages}{1\bibrangedash 56}
  \field{title}{{Support Vector Machines ( SVM ) in bioinformatics
  Bioinformatics applications}}
  \verb{url}
  \verb http://www.csie.ntu.edu.tw/{~}p88012/Bio{\_}SVM.pdf
  \endverb
  \field{journaltitle}{Bioinformatics}
  \field{year}{2002}
\endentry

\entry{Weinberger2009}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Weinberger}{W.}%
     {K.}{K.}%
     {}{}%
     {}{}}%
    {{}%
     {Saul}{S.}%
     {L.}{L.}%
     {}{}%
     {}{}}%
  }
  \keyw{convex optimization,ing,mahalanobis distance,metric learn-,multi-class
  classification,semi-definite programming,support vector machines}
  \strng{namehash}{WKSL1}
  \strng{fullhash}{WKSL1}
  \field{labelalpha}{WS09}
  \field{extraalpha}{1}
  \field{sortinit}{W}
  \field{abstract}{%
  The accuracy of k-nearest neighbor (kNN) classification depends significantly
  on the metric used to compute distances between different examples. In this
  paper, we show how to learn a Maha- lanobis distance metric for kNN
  classification from labeled examples. The Mahalanobis metric can equivalently
  be viewed as a global linear transformation of the input space that precedes
  kNN classification using Euclidean distances. In our approach, the metric is
  trained with the goal that the k-nearest neighbors always belong to the same
  class while examples from different classes are separated by a largemargin.
  As in support vectormachines (SVMs), themargin criterion leads to a convex
  optimization based on the hinge loss. Unlike learning in SVMs, however, our
  approach re- quires no modification or extension for problems in multiway (as
  opposed to binary) classification. In our framework, the Mahalanobis distance
  metric is obtained as the solution to a semidefinite program. On several data
  sets of varying size and difficulty, we find that metrics trained in this way
  lead to significant improvements in kNN classification. Sometimes these
  results can be further improved by clustering the training examples and
  learning an individual metric within each cluster. We show how to learn and
  combine these local metrics in a globally integrated manner. Keywords: convex
  optimization, semi-definite programming,Mahalanobis distance,metric learn-
  ing, multi-class classification, support vector machines 1.%
  }
  \field{pages}{207\bibrangedash 244}
  \field{title}{{Distance Metric Learning for Large Margin Nearest Neighbor
  Classification}}
  \verb{url}
  \verb http://www.jmlr.org/papers/volume10/weinberger09a/weinberger09a.pdf
  \endverb
  \field{volume}{10}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley D
  \verb esktop/Downloaded/Weinberger, Saul - 2009 - Distance Metric Learning fo
  \verb r Large Margin Nearest Neighbor Classification(2).pdf:pdf
  \endverb
  \field{journaltitle}{Journal of Machine Learning Research}
  \field{year}{2009}
\endentry

\entry{Weinberger2009a}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Weinberger}{W.}%
     {Kilian~Q}{K.~Q.}%
     {}{}%
     {}{}}%
    {{}%
     {Saul}{S.}%
     {Lawrence~K}{L.~K.}%
     {}{}%
     {}{}}%
  }
  \keyw{Metric learning,convex optimization,ing,mahalanobis distance,metric
  learn-,multi-class classification,semi-definite programming,support vector
  machines}
  \strng{namehash}{WKQSLK1}
  \strng{fullhash}{WKQSLK1}
  \field{labelalpha}{WS09}
  \field{extraalpha}{2}
  \field{sortinit}{W}
  \field{abstract}{%
  The accuracy of k-nearest neighbor (kNN) classification depends significantly
  on the metric used to compute distances between different examples. In this
  paper, we show how to learn a Mahalanobis distance metric for kNN
  classification from labeled examples. The Mahalanobis metric can equivalently
  be viewed as a global linear transformation of the input space that precedes
  kNN classification using Euclidean distances. In our approach, the metric is
  trained with the goal that the k-nearest neighbors always belong to the same
  class while examples from different classes are separated by a large margin.
  As in support vector machines (SVMs), the margin criterion leads to a convex
  optimization based on the hinge loss. Unlike learning in SVMs, however, our
  approach requires no modification or extension for problems in multiway (as
  opposed to binary) classification. In our framework, the Mahalanobis distance
  metric is obtained as the solution to a semidefinite program. On several data
  sets of varying size and difficulty, we find that metrics trained in this way
  lead to significant improvements in kNN classification. Sometimes these
  results can be further improved by clustering the training examples and
  learning an individual metric within each cluster. We show how to learn and
  combine these local metrics in a globally integrated manner.%
  }
  \verb{doi}
  \verb 10.1126/science.277.5323.215
  \endverb
  \field{isbn}{1532-4435}
  \field{issn}{1532-4435}
  \field{pages}{207\bibrangedash 244}
  \field{title}{{Distance Metric Learning for Large Margin Nearest Neighbor
  Classification}}
  \field{volume}{10}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley D
  \verb esktop/Downloaded/Weinberger, Saul - 2009 - Distance Metric Learning fo
  \verb r Large Margin Nearest Neighbor Classification(2).pdf:pdf
  \endverb
  \field{journaltitle}{Journal of Machine Learning Research}
  \field{year}{2009}
\endentry

\entry{Xi2006a}{inproceedings}{}
  \name{author}{5}{}{%
    {{}%
     {Xi}{X.}%
     {Xiaopeng}{X.}%
     {}{}%
     {}{}}%
    {{}%
     {Keogh}{K.}%
     {Eamonn}{E.}%
     {}{}%
     {}{}}%
    {{}%
     {Shelton}{S.}%
     {Christian}{C.}%
     {}{}%
     {}{}}%
    {{}%
     {Wei}{W.}%
     {Li}{L.}%
     {}{}%
     {}{}}%
    {{}%
     {Ratanamahatana}{R.}%
     {Chotirat~Ann}{C.~A.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{XX+1}
  \strng{fullhash}{XXKESCWLRCA1}
  \field{labelalpha}{Xi+06}
  \field{sortinit}{X}
  \field{abstract}{%
  Many algorithms have been proposed for the problem of time series
  classification. However, it is clear that one-nearest-neighbor with Dynamic
  Time Warping (DTW) distance is exceptionally difficult to beat. This approach
  has one weakness, however; it is computationally too demanding for many
  realtime applications. One way to mitigate this problem is to speed up the
  DTW calculations. Nonetheless, there is a limit to how much this can help. In
  this work, we propose an additional technique, numerosity reduction, to speed
  up one-nearest-neighbor DTW. While the idea of numerosity reduction for
  nearest-neighbor classifiers has a long history, we show here that we can
  leverage off an original observation about the relationship between dataset
  size and DTW constraints to produce an extremely compact dataset with little
  or no loss in accuracy. We test our ideas with a comprehensive set of
  experiments, and show that it can efficiently produce extremely fast accurate
  classifiers.%
  }
  \field{booktitle}{Proceedings of the 23rd international conference on Machine
  learning (ICML)}
  \verb{doi}
  \verb 10.1145/1143844.1143974
  \endverb
  \field{isbn}{1595933832}
  \field{pages}{1033\bibrangedash 1040}
  \field{title}{{Fast time series classification using numerosity reduction}}
  \verb{url}
  \verb http://dl.acm.org/citation.cfm?id=1143974
  \endverb
  \field{year}{2006}
\endentry

\entry{Xu2012}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Xu}{X.}%
     {Zhixiang}{Z.}%
     {}{}%
     {}{}}%
    {{}%
     {Weinberger}{W.}%
     {Kilian~Q.}{K.~Q.}%
     {}{}%
     {}{}}%
    {{}%
     {Chapelle}{C.}%
     {Olivier}{O.}%
     {}{}%
     {}{}}%
  }
  \keyw{distance learning,mahalanobis distance,metric learning,semi-definite
  programming,support vector machines}
  \strng{namehash}{XZWKQCO1}
  \strng{fullhash}{XZWKQCO1}
  \field{labelalpha}{XWC12}
  \field{sortinit}{X}
  \field{abstract}{%
  Recent work in metric learning has significantly improved the
  state-of-the-art in k-nearest neighbor classification. Support vector
  machines (SVM), particularly with RBF kernels, are amongst the most popular
  classification algorithms that uses distance metrics to compare examples.
  This paper provides an empirical analysis of the efficacy of three of the
  most popular Mahalanobis metric learning algorithms as pre-processing for SVM
  training. We show that none of these algorithms generate metrics that lead to
  particularly satisfying improvements for SVM-RBF classification. As a remedy
  we introduce support vector metric learning (SVML), a novel algorithm that
  seamlessly combines the learning of a Mahalanobis metric with the training of
  the RBF-SVM parameters. We demonstrate the capabilities of SVML on nine
  benchmark data sets of varying sizes and difficulties. In our study, SVML
  outperforms all alternative state-of-the-art metric learning algorithms in
  terms of accuracy and establishes itself as a serious alternative to the
  standard Euclidean metric with model selection by cross validation.%
  }
  \verb{eprint}
  \verb 1208.3422
  \endverb
  \field{pages}{1\bibrangedash 17}
  \field{title}{{Distance Metric Learning for Kernel Machines}}
  \verb{url}
  \verb http://arxiv.org/abs/1208.3422
  \endverb
  \field{journaltitle}{Arxiv}
  \field{eprinttype}{arXiv}
  \field{year}{2012}
\endentry

\entry{Yin2008}{inproceedings}{}
  \name{author}{2}{}{%
    {{}%
     {Yin}{Y.}%
     {J.}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Gaber}{G.}%
     {M.}{M.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{YJGM1}
  \strng{fullhash}{YJGM1}
  \field{labelalpha}{YG08}
  \field{sortinit}{Y}
  \field{abstract}{%
  Event detection is a critical task in sensor networks, especially for
  environmental monitoring applications. Traditional solutions to event
  detection are based on analyzing one-shot data points, which might incur a
  high false alarm rate because sensor data is inherently unreliable and noisy.
  To address this issue, we propose a novel Distributed Single-pass Incremental
  Clustering (DSIC) technique to cluster the time series obtained at sensor
  nodes based on their underlying trends. In order to achieve scalability and
  energy-efficiency, our DSIC technique uses a hierarchical structure of sensor
  networks as the underlying infrastructure. The algorithm first compresses the
  time series produced at individual sensor nodes into a compact representation
  using Haar wavelet transform, and then, based on dynamic time warping
  distances, hierarchically groups the approximate time series into a global
  clustering model in an incremental manner. Experimental results on both real
  data and synthetic data demonstrate that our DSIC algorithm is accurate,
  energy-efficient and robust with respect to network topology changes.%
  }
  \field{booktitle}{ICDM}
  \verb{doi}
  \verb 10.1109/ICDM.2008.58
  \endverb
  \field{isbn}{9780769535029}
  \field{issn}{15504786}
  \field{title}{{Clustering distributed time series in sensor networks}}
  \field{year}{2008}
\endentry

\entry{Yang1999}{incollection}{}
  \name{author}{2}{}{%
    {{}%
     {Yang}{Y.}%
     {Yiming}{Y.}%
     {}{}%
     {}{}}%
    {{}%
     {Liu}{L.}%
     {Xin}{X.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{YYLX1}
  \strng{fullhash}{YYLX1}
  \field{labelalpha}{YL99}
  \field{sortinit}{Y}
  \field{abstract}{%
  This paper reports a controlled study with statistical signifi cance tests on
  five text categorization methods: the Support Vector Machines (SVM), a
  kNearest Neighbor (kNN) clas sifier, a neural network (NNet) approach, the
  Linear Least squares Fit (LLSF) mapping and a Naive Bayes (NB) classi fier.
  We focus on the robustness of these methods in dealing with a skewed category
  distribution, and their performance as function of the trainingset category
  frequency. Our re sults show that SVM, kNN and LLSF significantly outper form
  NNet and NB when the number of positive training instances per category are
  small (less than ten), and that all the methods perform comparably when the
  categories are sufficiently common (over 300 instances).%
  }
  \field{booktitle}{Proceedings of the 22nd annual international ACM SIGIR
  conference on Research and development in information retrieval SIGIR 99}
  \verb{doi}
  \verb 10.1145/312624.312647
  \endverb
  \field{isbn}{1581130961}
  \field{pages}{42\bibrangedash 49}
  \field{title}{{A re-examination of text categorization methods}}
  \field{year}{1999}
\endentry

\entry{Zhang2006a}{article}{}
  \name{author}{4}{}{%
    {{}%
     {Zhang}{Z.}%
     {Hui}{H.}%
     {}{}%
     {}{}}%
    {{}%
     {Ho}{H.}%
     {T.B. Tb Tu~Bao}{T.~T. T.~B.}%
     {}{}%
     {}{}}%
    {{}%
     {Zhang}{Z.}%
     {Yang}{Y.}%
     {}{}%
     {}{}}%
    {{}%
     {Lin}{L.}%
     {M.S. Mao-Song~Ms}{M.~M.-S.~M.}%
     {}{}%
     {}{}}%
  }
  \keyw{2005,attracted increasing interest in,clustering,data mining,feature
  extraction,financial domains,in the bioinformatics and,of
  dimension-,particularly for long time,received,september 4,series,such as
  those arising,the last decade,the widely known curse,time series,time series
  clustering has,wavelet}
  \strng{namehash}{ZH+1}
  \strng{fullhash}{ZHHTTTBZYLMMSM1}
  \field{labelalpha}{Zha+06}
  \field{sortinit}{Z}
  \field{abstract}{%
  Thewidelyknowncurse of dimension- ality howto choose the appropriate
  dimension is a challenging task especially for clustering problem in the
  absence of data labels that has not been well studied in the literature. I
  this paper we propose an unsupervised feature extraction algorithm using
  orthogonal wavelet transform for automatically choosing the dimensionality of
  features. The feature extraction algorithm selects the feature dimensionality
  by leveraging twoconflicting requirements, i.e., lower dimensionality and
  lower sum of squared errors between the features and the original time
  series. The proposed feature extraction algorithm is efficient with time
  complexityO(mn) when using Haar wavelet. Encouraging experimental results are
  obtained on several synthetic and real-world time series datasets.%
  }
  \field{number}{3}
  \field{pages}{305\bibrangedash 319}
  \field{title}{{Unsupervised Feature Extraction for Time Series Clustering
  Using Orthogonal Wavelet Transform.}}
  \verb{url}
  \verb http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Uns
  \verb upervised+Feature+Extraction+for+Time+Series+Clustering+Using+Orthogona
  \verb l+Wavelet+Transform{\#}0$\backslash$nhttp://www.freepatentsonline.com/a
  \verb rticle/Informatica/158092585.html$\backslash$nhttp://search.ebscohost.c
  \verb om
  \endverb
  \field{volume}{30}
  \field{journaltitle}{Guest Editor}
  \field{year}{2006}
\endentry

\entry{ZhangX.-L.Z.-G.Luo2014}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Zhang}{Z.}%
     {X.-L.}{X.-L.}%
     {}{}%
     {}{}}%
    {{}%
     {Luo}{L.}%
     {Z.-G.}{Z.-G.}%
     {}{}%
     {}{}}%
    {{}%
     {Li}{L.}%
     {M.}{M.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{ZXLLZGLM1}
  \strng{fullhash}{ZXLLZGLM1}
  \field{labelalpha}{ZLL14}
  \field{sortinit}{Z}
  \field{number}{6}
  \field{pages}{1072\bibrangedash 1082}
  \field{title}{{Merge-weighted dynamic time warping for speech recognition}}
  \field{volume}{29}
  \field{journaltitle}{Journal of Computer Science and Technology}
  \field{year}{2014}
\endentry

\entry{Zhang2010}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Zhang}{Z.}%
     {Yu}{Y.}%
     {}{}%
     {}{}}%
    {{}%
     {Yeung}{Y.}%
     {Dit-Yan}{D.-Y.}%
     {}{}%
     {}{}}%
  }
  \keyw{metric learning,multi-task learning,transfer learning}
  \strng{namehash}{ZYYDY1}
  \strng{fullhash}{ZYYDY1}
  \field{labelalpha}{ZY10}
  \field{sortinit}{Z}
  \field{abstract}{%
  Distance metric learning plays a very crucial role in many data mining
  algorithms because the performance of an algorithm relies heavily on choosing
  a good metric. However, the labeled data available in many applications is
  scarce and hence the metrics learned are often unsatisfactory. In this paper,
  we consider a transfer learning setting in which some related source tasks
  with labeled data are available to help the learning of the target task. We
  first propose a convex formulation for multi-task metric learning by modeling
  the task relationships in the form of a task covariance matrix. Then we
  regard transfer learning as a special case of multitask learning and adapt
  the formulation of multi-task metric learning to the transfer learning
  setting for our method, called transfer metric learning (TML). In TML, we
  learn the metric and the task covariances between the source tasks and the
  target task under a unified convex formulation. To solve the convex
  optimization problem, we use an alternating method in which each subproblem
  has an efficient solution. Experimental results on some commonly used
  transfer learning applications demonstrate the effectiveness of our method.
  © 2010 ACM.%
  }
  \verb{doi}
  \verb 10.1145/1835804.1835954
  \endverb
  \field{isbn}{9781450300551}
  \field{pages}{1199}
  \field{title}{{Transfer metric learning by learning task relationships}}
  \verb{url}
  \verb http://dl.acm.org/citation.cfm?doid=1835804.1835954
  \endverb
  \field{journaltitle}{Proceedings of the 16th ACM SIGKDD international
  conference on Knowledge discovery and data mining - KDD '10}
  \field{year}{2010}
\endentry

\entry{Dreyfus2006}{book}{}
  \name{author}{1}{}{%
    {{}%
     {{G. Dreyfus, J.-M. Martinez, M. Samuelides M. B. Gordon, F. Badran}}{G.}%
     {S.~Thiria}{S.~T.}%
     {}{}%
     {}{}}%
  }
  \keyw{Bio-ing{\'{e}}nierie,Machine {\`{a}} Vecteurs
  Supports,Pr{\'{e}}vision,Reconaissance de formes,Robotique et commande de
  processus,R{\'{e}}seaux de neurones,cartes topologiques,data mining}
  \strng{namehash}{GST1}
  \strng{fullhash}{GST1}
  \field{labelalpha}{{G. }06}
  \field{sortinit}{{G}}
  \field{abstract}{%
  En une vingtaine d’ann{\'{e}}es, l’apprentissage artificiel est devenu
  une branche majeure des math{\'{e}}matiques appliqu{\'{e}}es, {\`{a}}
  l’intersection des statistiques et de l’intelligence artificielle. Son
  objectif est de r{\'{e}}aliser des mod{\`{e}}les qui apprennent « par
  l’exemple » : il s’appuie sur des donn{\'{e}}es num{\'{e}}riques
  (r{\'{e}}sultats de mesures ou de simulations), contrairement aux
  mod{\`{e}}les « de connaissances » qui s’appuient sur des {\'{e}}quations
  issues des premiers principes de la physique, de la chimie, de la biologie,
  de l’{\'{e}}conomie, etc. L’apprentis- sage statistique est d’une
  grande utilit{\'{e}} lorsque l’on cherche {\`{a}} mod{\'{e}}liser des
  processus complexes, souvent non lin{\'{e}}aires, pour lesquels les
  connaissances th{\'{e}}oriques sont trop impr{\'{e}}cises pour permettre des
  pr{\'{e}}dictions pr{\'{e}}cises. Ses domaines d’applications sont
  multiples : fouille de donn{\'{e}}es, bio-informatique, g{\'{e}}nie des
  proc{\'{e}}d{\'{e}}s, aide au diagnostic m{\'{e}}dical,
  t{\'{e}}l{\'{e}}communications, interface cerveau-machines, et bien
  d’autres. Cet ouvrage refl{\`{e}}te en partie l’{\'{e}}volution de cette
  discipline, depuis ses balbutiements au d{\'{e}}but des ann{\'{e}}es 1980,
  jusqu’{\`{a}} sa situation actuelle ; il n’a pas du tout la
  pr{\'{e}}tention de faire un point, m{\^{e}}me partiel, sur l’ensemble des
  d{\'{e}}veloppements pass{\'{e}}s et actuels, mais plut{\^{o}}t d’insister
  sur les principes et sur les m{\'{e}}thodes {\'{e}}prouv{\'{e}}s, dont les
  bases scientifiques sont s{\^{u}}res. Dans un domaine sans cesse parcouru de
  modes multiples et {\'{e}}ph{\'{e}}m{\`{e}}res, il est utile, pour qui
  cherche {\`{a}} acqu{\'{e}}rir les connaissances et principes de base,
  d’insister sur les aspects p{\'{e}}rennes du domaine. Cet ouvrage fait
  suite {\`{a}} R{\'{e}}seaux de neurones, m{\'{e}}thodologies et applications,
  des m{\^{e}}mes auteurs, paru en 2000, r{\'{e}}{\'{e}}dit{\'{e}} en 2004,
  chez le m{\^{e}}me {\'{e}}diteur, puis publi{\'{e}} en traduction anglaise
  chez Springer. Consacr{\'{e}} essentiellement aux r{\'{e}}seaux de neurones
  et aux cartes auto-adaptatives, il a largement contribu{\'{e}} {\`{a}}
  populariser ces techniques et {\`{a}} convaincre leurs utilisateurs qu’il
  est possible d’obtenir des r{\'{e}}sultats remarquables, {\`{a}} condition
  de mettre en {\oe}uvre une m{\'{e}}thodologie de conception rigoureuse,
  scientifique- ment fond{\'{e}}e, dans un domaine o{\`{u}} l’empirisme a
  longtemps tenu lieu de m{\'{e}}thode. Tout en restant fid{\`{e}}le {\`{a}}
  l’esprit de cet ouvrage, combinant fondements math{\'{e}}matiques et
  m{\'{e}}thodologie de mise en {\oe}uvre, les auteurs ont {\'{e}}largi le
  champ de la pr{\'{e}}sentation, afin de permettre au lecteur d’aborder
  d’autres m{\'{e}}thodes d’apprentissage statistique que celles qui sont
  directement d{\'{e}}crites dans cet ouvrage. En effet, les succ{\`{e}}s de
  l’apprentissage dans un grand nombre de domaines ont pouss{\'{e}} au
  d{\'{e}}veloppement de tr{\`{e}}s nombreuses variantes, souvent
  destin{\'{e}}es {\`{a}} r{\'{e}}pondre efficacement aux exigences de telle ou
  telle classe d’applications. Toutes ces variantes ont n{\'{e}}anmoins des
  bases th{\'{e}}oriques et des aspects m{\'{e}}thodolo- giques communs,
  qu’il est important d’avoir pr{\'{e}}sents {\`{a}} l’esprit. Le terme
  d’apprentissage, comme celui de r{\'{e}}seau de neurones, {\'{e}}voque
  {\'{e}}videmment le fonctionnement du cerveau. Il ne faut pourtant pas
  s’attendre {\`{a}} trouver ici d’explications sur les m{\'{e}}canismes de
  traitement des informations dans les syst{\`{e}}mes nerveux ; ces derniers
  sont d’une grande complexit{\'{e}}, r{\'{e}}sultant de processus
  {\'{e}}lectriques et chimiques subtils, encore mal compris en d{\'{e}}pit de
  la grande quantit{\'{e}} de donn{\'{e}}es exp{\'{e}}rimentales disponibles.
  Si les m{\'{e}}thodes d’apprentissage statistique peuvent {\^{e}}tre
  d’une grande utilit{\'{e}} pour cr{\'{e}}er des mod{\`{e}}les empiriques de
  telle ou telle fonction r{\'{e}}alis{\'{e}}e par le syst{\`{e}}me nerveux,
  celles qui sont d{\'{e}}crites dans cet ouvrage n’ont aucunement la
  pr{\'{e}}tention d’imiter, m{\^{e}}me vaguement, le fonctionne- ment du
  cerveau. L’apprentissage artificiel, notamment statistique, permettra-t-il
  un jour de donner aux ordinateurs des capacit{\'{e}}s analogues {\`{a}}
  celles des {\^{e}}tres humains ? Se rapprochera-t-on de cet objectif en
  perfectionnant les techniques actuelles d’apprentissage, ou bien des
  approches radicalement nouvelles sont-elles indispensables ? Faut-il
  s’inspirer de ce que l’on sait, ou croit savoir, sur le fonctionnement du
  cerveau ? Ces questions font l’objet de d{\'{e}}bats passionn{\'{e}}s, et
  passionnants, au sein de la communaut{\'{e}} scientifique : on n’en
  trouvera pas les r{\'{e}}ponses ici.%
  }
  \field{edition}{Eyrolles}
  \field{isbn}{9782212114645}
  \field{pages}{471}
  \field{title}{{Apprentissage Apprentissage statistique}}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley D
  \verb esktop/Downloaded/G. Dreyfus, J.-M. Martinez, M. Samuelides M. B. Gordo
  \verb n, F. Badran - 2006 - Apprentissage Apprentissage statistique.pdf:pdf
  \endverb
  \field{year}{2006}
\endentry

\entry{WienerN1942}{report}{}
  \name{author}{1}{}{%
    {{}%
     {{Wiener N}}{W.}%
     {}{}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{W1}
  \strng{fullhash}{W1}
  \field{labelalpha}{{Wie}42}
  \field{sortinit}{{W}}
  \field{title}{{Extrapolation, Interpolation {\&} Smoothing of Stationary Time
  Series - With Engineering Applications}}
  \list{institution}{1}{%
    {Report of the Services 19, Research Project DIC-6037 MIT}%
  }
  \field{type}{techreport}
  \field{year}{1942}
\endentry

\lossort
\endlossort

\endinput
