% $ biblatex auxiliary file $
% $ biblatex version 2.5 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated as
% required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup

\entry{Aizerman1964}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Aizerman}{A.}%
     {M.}{M.}%
     {}{}%
     {}{}}%
    {{}%
     {Braverman}{B.}%
     {E.}{E.}%
     {}{}%
     {}{}}%
    {{}%
     {Rozonoer}{R.}%
     {L.}{L.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{AMBERL1}
  \strng{fullhash}{AMBERL1}
  \field{labelalpha}{ABR64}
  \field{sortinit}{A}
  \field{abstract}{%
  Introduction of kernels%
  }
  \field{pages}{821\bibrangedash 837}
  \field{title}{{Theoretical foundations of the potential function method in
  pattern recognition learning}}
  \field{volume}{25}
  \field{journaltitle}{Automation and Remote Control}
  \field{year}{1964}
\endentry

\entry{Abraham2010a}{inproceedings}{}
  \name{author}{2}{}{%
    {{}%
     {Abraham}{A.}%
     {Z.}{Z.}%
     {}{}%
     {}{}}%
    {{}%
     {Tan}{T.}%
     {P.N.}{P.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{AZTP1}
  \strng{fullhash}{AZTP1}
  \field{labelalpha}{AT10}
  \field{sortinit}{A}
  \field{abstract}{%
  Zero-inflated time series data are commonly encountered in many applications,
  including climate and ecological modeling, disease monitoring, manufacturing
  defect detection, and traffic monitoring. Such data often leads to poor model
  fitting using standard regression methods because they tend to underestimate
  the frequency of zeros and the magnitude of non-zero values. This paper
  presents an integrated framework that simultaneously performs classification
  and regression to accurately predict future values of a zero-inflated time
  series. A regression model is initially applied to predict the value of the
  time series. The regression output is then fed into a classification model to
  determine whether the predicted value should be adjusted to zero. Our
  regression and classification models are trained to optimize a joint
  objective function that considers both classification errors on the time
  series and regression errors on data points that have non-zero values. We
  demonstrate the effectiveness of our framework in the context of its
  application to a precipitation downscaling problem for climate impact
  assessment studies. Read More:
  http://epubs.siam.org/doi/abs/10.1137/1.9781611972801.57%
  }
  \field{booktitle}{ACM SIGKDD}
  \field{title}{{An Integrated Framework for Simultaneous Classification and
  Regression of Time-Series Data}}
  \verb{url}
  \verb https://siam.org/proceedings/datamining/2010/dm10\_057\_abrahamz.pdf
  \endverb
  \field{year}{2010}
\endentry

\entry{Berndt1994a}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Berndt}{B.}%
     {Donald}{D.}%
     {}{}%
     {}{}}%
    {{}%
     {Clifford}{C.}%
     {James}{J.}%
     {}{}%
     {}{}}%
  }
  \keyw{dynamic programming,dynamic time warping,knowledge discovery,pat,tern
  analysis,time series}
  \strng{namehash}{BDCJ1}
  \strng{fullhash}{BDCJ1}
  \field{labelalpha}{BC94}
  \field{sortinit}{B}
  \field{abstract}{%
  Knowledge discovery in databases presents many interesting challenges within
  the context of providing computer tools for exploring large data archives.
  Electronic data repositories are growing qulckiy and contain data from
  commercial, scientific, and other domains. Much of this data is inherently
  temporal, such as stock prices or NASA telemetry data. Detecting patterns in
  such data streams or time series is an important knowledge discovery task.
  This paper describes some primary experiments with a dynamic programming
  approach to the problem. The pattern detection algorithm is based on the
  dynamic time warping technique used in the speech recognition field.
  Keywords: dynamic programming, dynamic time warping, knowledge discovery,
  pattern analysis, time series.%
  }
  \field{pages}{359\bibrangedash 370}
  \field{title}{{Using dynamic time warping to find patterns in time series}}
  \verb{url}
  \verb http://www.aaai.org/Papers/Workshops/1994/WS-94-03/WS94-03-031.pdf
  \endverb
  \field{volume}{398}
  \field{journaltitle}{Workshop on Knowledge Knowledge Discovery in Databases}
  \field{year}{1994}
\endentry

\entry{Benesty2009}{article}{}
  \name{author}{4}{}{%
    {{}%
     {Benesty}{B.}%
     {J.}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Chen}{C.}%
     {J.}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Huang}{H.}%
     {Y.}{Y.}%
     {}{}%
     {}{}}%
    {{}%
     {Cohen}{C.}%
     {I.}{I.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{BJ+1}
  \strng{fullhash}{BJCJHYCI1}
  \field{labelalpha}{Ben+09}
  \field{sortinit}{B}
  \field{abstract}{%
  This chapter develops several forms of the Pearson correlation coefficient in
  the different domains. This coefficient can be used as an optimization
  criterion to derive different optimal noise reduction filters [14], but is
  even more useful for analyzing these optimal filters for their noise
  reduction performance.%
  }
  \verb{doi}
  \verb 10.1007/978-3-642-00296-0
  \endverb
  \field{isbn}{978-3-642-00295-3}
  \field{title}{{Pearson correlation coefficient}}
  \verb{url}
  \verb http://www.springerlink.com/index/10.1007/978-3-642-00296-0$\backslash$
  \verb nhttp://link.springer.com/content/pdf/10.1007/978-3-642-00296-0\_5.pdf
  \endverb
  \field{journaltitle}{Noise Reduction in Speech Processing}
  \field{year}{2009}
\endentry

\entry{Boser1992}{inproceedings}{}
  \name{author}{3}{}{%
    {{}%
     {Boser}{B.}%
     {Bernhard~E.}{B.~E.}%
     {}{}%
     {}{}}%
    {{}%
     {Guyon}{G.}%
     {Isabelle~M.}{I.~M.}%
     {}{}%
     {}{}}%
    {{}%
     {Vapnik}{V.}%
     {Vladimir~N.}{V.~N.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{BBEGIMVVN1}
  \strng{fullhash}{BBEGIMVVN1}
  \field{labelalpha}{BGV92}
  \field{sortinit}{B}
  \field{abstract}{%
  A training algorithm that maximizes the margin between the training patterns
  and the decision boundary is presented. The technique is applicable to a wide
  variety of classifiaction functions, including Perceptrons, polynomials, and
  Radial Basis Functions. The effective number of parameters is adjusted
  automatically to match the complexity of the problem. The solution is
  expressed as a linear combination of supporting patterns. These are the
  subset of training patterns that are closest to the decision boundary. Bounds
  on the generalization performance based on the leave-one-out method and the
  VC-dimension are given. Experimental results on optical character recognition
  problems demonstrate the good generalization obtained when compared with
  other learning algorithms. 1 INTRODUCTION Good generalization performance of
  pattern classifiers is achieved when the capacity of the classification
  function is matched to the size of the training set. Classifiers with a large
  numb...%
  }
  \field{booktitle}{Proceedings of the 5th Annual ACM Workshop on Computational
  Learning Theory}
  \verb{doi}
  \verb 10.1.1.21.3818
  \endverb
  \field{isbn}{089791497X}
  \field{issn}{0-89791-497-X}
  \field{pages}{144\bibrangedash 152}
  \field{title}{{A Training Algorithm for Optimal Margin Classifiers}}
  \verb{url}
  \verb http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.21.3818
  \endverb
  \field{year}{1992}
\endentry

\entry{Bishop2006}{book}{}
  \name{author}{1}{}{%
    {{}%
     {Bishop}{B.}%
     {Christopher~M}{C.~M.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{BCM1}
  \strng{fullhash}{BCM1}
  \field{labelalpha}{Bis06}
  \field{sortinit}{B}
  \field{abstract}{%
  The dramatic growth in practical applications for machine learning over the
  last ten years has been accompanied by many important developments in the
  underlying algorithms and techniques. For example, Bayesian methods have
  grown from a specialist niche to become mainstream, while graphical models
  have emerged as a general framework for describing and applying probabilistic
  techniques. The practical applicability of Bayesian methods has been greatly
  enhanced by the development of a range of approximate inference algorithms
  such as variational Bayes and expectation propagation, while new models based
  on kernels have had a significant impact on both algorithms and applications.
  This completely new textbook reflects these recent developments while
  providing a comprehensive introduction to the fields of pattern recognition
  and machine learning. It is aimed at advanced undergraduates or first-year
  PhD students, as well as researchers and practitioners. No previous knowledge
  of pattern recognition or machine learning concepts is assumed. Familiarity
  with multivariate calculus and basic linear algebra is required, and some
  experience in the use of probabilities would be helpful though not essential
  as the book includes a self-contained introduction to basic probability
  theory. The book is suitable for courses on machine learning, statistics,
  computer science, signal processing, computer vision, data mining, and
  bioinformatics. Extensive support is provided for course instructors,
  including more than 400 exercises, graded according to difficulty. Example
  solutions for a subset of the exercises are available from the book web site,
  while solutions for the remainder can be obtained by instructors from the
  publisher. The book is supported by a great deal of additional material, and
  the reader is encouraged to visit the book web site for the latest
  information. A forthcoming companion volume will deal with practical aspects
  of pattern recognition and machine learning, and will include free software
  implementations of the key algorithms along with example data sets and
  demonstration programs. Christopher Bishop is Assistant Director at Microsoft
  Research Cambridge, and also holds a Chair in Computer Science at the
  University of Edinburgh. He is a Fellow of Darwin College Cambridge, and was
  recently elected Fellow of the Royal Academy of Engineering. The author's
  previous textbook "Neural Networks for Pattern Recognition" has been widely
  adopted.%
  }
  \field{booktitle}{Pattern Recognition}
  \verb{doi}
  \verb 10.1117/1.2819119
  \endverb
  \verb{eprint}
  \verb 0-387-31073-8
  \endverb
  \field{isbn}{9780387310732}
  \field{issn}{10179909}
  \field{number}{4}
  \field{pages}{738}
  \field{title}{{Pattern Recognition and Machine Learning}}
  \verb{url}
  \verb http://www.library.wisc.edu/selectedtocs/bg0137.pdf
  \endverb
  \field{volume}{4}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/Google Drive/Th\`{e}se/Bibliographie/L
  \verb ivre/Bishop - Pattern Recognition and Machine Learning.pdf:pdf
  \endverb
  \field{eprinttype}{arXiv}
  \field{year}{2006}
\endentry

\entry{Bottou2007}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Bottou}{B.}%
     {L}{L}%
     {}{}%
     {}{}}%
    {{}%
     {Lin}{L.}%
     {CJ}{C.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{BLLC1}
  \strng{fullhash}{BLLC1}
  \field{labelalpha}{BL07}
  \field{sortinit}{B}
  \field{abstract}{%
  Considerable efforts have been devoted to the implementation of efficient
  optimization method for solving the Support Vector Machine dual problem. This
  document proposes an historical perspective and and in depth review of the
  algorithmic and computational issues associated with this problem.%
  }
  \field{isbn}{0262026252}
  \field{pages}{1\bibrangedash 27}
  \field{title}{{Support vector machine solvers}}
  \verb{url}
  \verb http://140.112.30.28/~cjlin/papers/bottou\_lin.pdf
  \endverb
  \field{journaltitle}{Large scale kernel machines}
  \field{year}{2007}
\endentry

\entry{Brigham1967}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Brigham}{B.}%
     {E.~O.}{E.~O.}%
     {}{}%
     {}{}}%
    {{}%
     {Morrow}{M.}%
     {R.~E.}{R.~E.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{BEOMRE1}
  \strng{fullhash}{BEOMRE1}
  \field{labelalpha}{BM67}
  \field{sortinit}{B}
  \field{abstract}{%
  The fast Fourier transform (FFT), a computer algorithm that computes the
  discrete Fourier transform much faster than other algorithms, is explained.
  Examples and detailed procedures are provided to assist the reader in
  learning how to use the algorithm. The savings in computer time can be huge;
  for example, an N = 210-point transform can be computed with the FFT 100
  times faster than with the use of a direct approach.%
  }
  \verb{doi}
  \verb 10.1109/MSPEC.1967.5217220
  \endverb
  \field{isbn}{0018-9235}
  \field{issn}{0018-9235}
  \field{number}{12}
  \field{pages}{63 \bibrangedash 70}
  \field{title}{{The fast Fourier transform}}
  \verb{url}
  \verb http://ieeexplore.ieee.org/ielx5/6/5217195/05217220.pdf?tp=\&arnumber=5
  \verb 217220\&isnumber=5217195$\backslash$nhttp://ieeexplore.ieee.org/stamp/s
  \verb tamp.jsp?tp=\&arnumber=5217220
  \endverb
  \field{volume}{4}
  \field{journaltitle}{Spectrum, IEEE}
  \field{year}{1967}
\endentry

\entry{Belongie2002}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Belongie}{B.}%
     {Serge}{S.}%
     {}{}%
     {}{}}%
    {{}%
     {Malik}{M.}%
     {Jitendra}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Puzicha}{P.}%
     {Jan}{J.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{BSMJPJ1}
  \strng{fullhash}{BSMJPJ1}
  \field{labelalpha}{BMP02}
  \field{sortinit}{B}
  \field{abstract}{%
  We present a novel approach to measuring similarity between shapes and
  exploit it for object recognition. In our framework, the measurement of
  similarity is preceded by 1) solving for correspondences between points on
  the two shapes, 2) using the correspondences to estimate an aligning
  transform. In order to solve the correspondence problem, we attach a
  descriptor, the shape context, to each point. The shape context at a
  reference point captures the distribution of the remaining points relative to
  it, thus offering a globally discriminative characterization. Corresponding
  points on two similar shapes will have similar shape contexts, enabling us to
  solve for correspondences as an optimal assignment problem. Given the point
  correspondences, we estimate the transformation that best aligns the two
  shapes; regularized thin-plate splines provide a flexible class of
  transformation maps for this purpose. The dissimilarity between the two
  shapes is computed as a sum of matching errors between corresponding points,
  together with a term measuring the magnitude of the aligning transform. We
  treat recognition in a nearest-neighbor classification framework as the
  problem of finding the stored prototype shape that is maximally similar to
  that in the image. Results are presented for silhouettes, trademarks,
  handwritten digits, and the COIL data set.%
  }
  \verb{doi}
  \verb 10.1.1.18.8852
  \endverb
  \field{isbn}{9781424455409}
  \field{issn}{01628828}
  \field{pages}{509\bibrangedash 522}
  \field{title}{{Shape Matching and Object Recognition Using Shape Contexts}}
  \verb{url}
  \verb http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.18.8852
  \endverb
  \field{volume}{24}
  \field{journaltitle}{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}
  \field{year}{2002}
\endentry

\entry{Cover1967b}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Cover}{C.}%
     {T.}{T.}%
     {}{}%
     {}{}}%
    {{}%
     {Hart}{H.}%
     {P.}{P.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{CTHP1}
  \strng{fullhash}{CTHP1}
  \field{labelalpha}{CH67}
  \field{sortinit}{C}
  \field{abstract}{%
  The nearest neighbor decision rule assigns to an unclassified sample point
  the classification of the nearest of a set of previously classified points.
  This rule is independent of the underlying joint distribution on the sample
  points and their classifications, and hence the probability of
  error<tex>R</tex>of such a rule must be at least as great as the Bayes
  probability of error<tex>R\^{}\{ast\}</tex>--the minimum probability of error
  over all decision rules taking underlying probability structure into account.
  However, in a large sample analysis, we will show in the<tex>M</tex>-category
  case that<tex>R\^{}\{ast\} leq R leq R\^{}\{ast\}(2
  --MR\^{}\{ast\}/(M-1))</tex>, where these bounds are the tightest possible,
  for all suitably smooth underlying distributions. Thus for any number of
  categories, the probability of error of the nearest neighbor rule is bounded
  above by twice the Bayes probability of error. In this sense, it may be said
  that half the classification information in an infinite sample set is
  contained in the nearest neighbor.%
  }
  \verb{doi}
  \verb 10.1109/TIT.1967.1053964
  \endverb
  \field{isbn}{0018-9448}
  \field{issn}{0018-9448}
  \field{number}{1}
  \field{pages}{21\bibrangedash 27}
  \field{title}{{Nearest neighbor pattern classification}}
  \field{volume}{13}
  \field{journaltitle}{IEEE Transactions on Information Theory}
  \field{year}{1967}
\endentry

\entry{Chatfield2004}{book}{}
  \name{author}{1}{}{%
    {{}%
     {Chatfield}{C.}%
     {Christopher}{C.}%
     {}{}%
     {}{}}%
  }
  \keyw{Time-series analysis.}
  \strng{namehash}{CC1}
  \strng{fullhash}{CC1}
  \field{labelalpha}{Cha04}
  \field{sortinit}{C}
  \field{abstract}{%
  "Since 1975, The Analysis of Time Series: An Introduction has introduced
  legions of statistics students and researchers to the theory and practice of
  time series analysis. The sixth edition provides an accessible, comprehensive
  introduction to the theory and practice of time series analysis. The
  treatment covers a wide range of topics, including ARIMA probability models,
  forecasting methods, spectral analysis, linear systems, state-space models,
  and the Kalman filter. It also addresses nonlinear, multivariate, and
  long-memory models. The author has carefully updated each chapter, added new
  discussions, incorporated new datasets, and made those datasets available at
  www.crcpress.com."--BOOK JACKET.%
  }
  \field{booktitle}{Texts in statistical science}
  \field{isbn}{1584883170}
  \field{pages}{xiii, 333 p.}
  \field{title}{{The analysis of time series : an introduction}}
  \field{year}{2004}
\endentry

\entry{Chopra2005}{inproceedings}{}
  \name{author}{3}{}{%
    {{}%
     {Chopra}{C.}%
     {Sumit}{S.}%
     {}{}%
     {}{}}%
    {{}%
     {Hadsell}{H.}%
     {Raia}{R.}%
     {}{}%
     {}{}}%
    {{}%
     {LeCun}{L.}%
     {Yann}{Y.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{CSHRLY1}
  \strng{fullhash}{CSHRLY1}
  \field{labelalpha}{CHL05}
  \field{sortinit}{C}
  \field{abstract}{%
  We present a method for training a similarity metric from data. The method
  can be used for recognition or verification applications where the number of
  categories is very large and not known during training, and where the number
  of training samples for a single category is very small. The idea is to learn
  a function that maps input patterns into a target space such that the L1 norm
  in the target space approximates the "semantic" distance in the input space.
  The method is applied to a face verification task. The learning process
  minimizes a discriminative loss function that drives the similarity metric to
  be small for pairs of faces from the same person, and large for pairs from
  different persons. The mapping from raw to the target space is a
  convolutional network whose architecture is designed for robustness to
  geometric distortions. The system is tested on the Purdue/AR face database
  which has a very high degree of variability in the pose, lighting,
  expression, position, and artificial occlusions such as dark glasses and
  obscuring scarves.%
  }
  \field{booktitle}{CVPR}
  \verb{doi}
  \verb 10.1109/CVPR.2005.202
  \endverb
  \field{isbn}{0769523722}
  \field{issn}{10636919}
  \field{pages}{539\bibrangedash 546}
  \field{title}{{Learning a similarity metric discriminatively, with
  application to face verification}}
  \field{volume}{1}
  \field{year}{2005}
\endentry

\entry{Chen1996}{misc}{}
  \name{author}{3}{}{%
    {{}%
     {Chen}{C.}%
     {Ming~Syan}{M.~S.}%
     {}{}%
     {}{}}%
    {{}%
     {Han}{H.}%
     {Jiawei}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Yu}{Y.}%
     {Philip~S.}{P.~S.}%
     {}{}%
     {}{}}%
  }
  \keyw{Association rules,Classification,Data clustering,Data cubes,Data
  generalization and characterization,Data mining,Knowledge
  discovery,Multiple-dimensional databases,Pattern matching algorithms}
  \strng{namehash}{CMSHJYPS1}
  \strng{fullhash}{CMSHJYPS1}
  \field{labelalpha}{CHY96}
  \field{sortinit}{C}
  \field{abstract}{%
  Mining information and knowledge from large databases has been recognized by
  many researchers as a key research topic in database systems and machine
  learning, and by many industrial companies as an important area with an
  opportunity of major revenues. Researchers in many different fields have
  shown great interest in data mining. Several emerging applications in
  information-providing services, such as data warehousing and online services
  over the Internet, also call for various data mining techniques to better
  understand user behavior, to improve the service provided and to increase
  business opportunities. In response to such a demand, this article provides a
  survey, from a database researcher's point of view, on the data mining
  techniques developed recently. A classification of the available data mining
  techniques is provided and a comparative study of such techniques is
  presented%
  }
  \field{booktitle}{IEEE Transactions on Knowledge and Data Engineering}
  \verb{doi}
  \verb 10.1109/69.553155
  \endverb
  \field{isbn}{1041-4347}
  \field{issn}{10414347}
  \field{number}{6}
  \field{pages}{866\bibrangedash 883}
  \field{title}{{Data mining: An Overview from a Database Perspective}}
  \field{volume}{8}
  \field{year}{1996}
\endentry

\entry{Cochran1977}{article}{}
  \name{author}{1}{}{%
    {{}%
     {Cochran}{C.}%
     {William~C}{W.~C.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{CWC1}
  \strng{fullhash}{CWC1}
  \field{labelalpha}{Coc77}
  \field{sortinit}{C}
  \field{abstract}{%
  Commentary by : Cochran William C. Current Contents : \#19, May 9, 1977%
  }
  \field{pages}{1}
  \field{title}{{Snedecor G W \& Cochran W G. Statistical methods applied to
  experiments in agriculture and biology. 5th ed. Ames, Iowa: Iowa State
  University Press, 1956.}}
  \verb{url}
  \verb papers3://publication/uuid/8C5C843E-F853-4CB4-82BC-1141F0C01CB4
  \endverb
  \field{volume}{19}
  \field{journaltitle}{Citation Classics}
  \field{year}{1977}
\endentry

\entry{Crammer2001}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Crammer}{C.}%
     {Koby}{K.}%
     {}{}%
     {}{}}%
    {{}%
     {Singer}{S.}%
     {Yoram}{Y.}%
     {}{}%
     {}{}}%
  }
  \keyw{kernel machines,multiclass problems,svm}
  \strng{namehash}{CKSY1}
  \strng{fullhash}{CKSY1}
  \field{labelalpha}{CS01}
  \field{sortinit}{C}
  \field{abstract}{%
  In this paper we describe the algorithmic implementation of multiclass
  kernel-based vector machines. Our starting point is a generalized notion of
  the margin to multiclass problems. Using this notion we cast multiclass
  categorization problems as a constrained optimization problem with a
  quadratic objective function. Unlike most of previous approaches which
  typically decompose a multiclass problem into multiple independent binary
  classification tasks, our notion of margin yields a direct method for
  training multiclass predictors. By using the dual of the optimization problem
  we are able to incorporate kernels with a compact set of constraints and
  decompose the dual problem into multiple optimization problems of reduced
  size. We describe an efficient fixed-point algorithm for solving the reduced
  optimization problems and prove its convergence. We then discuss technical
  details that yield significant running time improvements for large datasets.
  Finally, we describe various experiments with our approach comparing it to
  previously studied kernel-based methods. Our experiments indicate that for
  multiclass problems we attain state-of-the-art accuracy%
  }
  \verb{doi}
  \verb 10.1162/15324430260185628
  \endverb
  \field{isbn}{1532-4435}
  \field{issn}{15324435}
  \field{pages}{265\bibrangedash 292}
  \field{title}{{On the Algorithmic Implementation of Multiclass Kernel-based
  Vector Machines}}
  \verb{url}
  \verb http://machinelearning.wustl.edu/mlpapers/paper\_files/CrammerS01.pdf$\
  \verb backslash$nhttp://www.jmlr.org/papers/volume2/crammer01a/crammer01a.pdf
  \endverb
  \field{volume}{2}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley D
  \verb esktop/Downloaded/Crammer, Singer - 2001 - On the Algorithmic Implement
  \verb ation of Multiclass Kernel-based Vector Machines.pdf:pdf
  \endverb
  \field{journaltitle}{Journal of Machine Learning Research}
  \field{year}{2001}
\endentry

\entry{Cao2001}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Cao}{C.}%
     {Lijuan}{L.}%
     {}{}%
     {}{}}%
    {{}%
     {Tay}{T.}%
     {Francis E~H}{F.~E.~H.}%
     {}{}%
     {}{}}%
  }
  \keyw{back propagation
  algorithm,financial,generalisation,multi-layer,perceptron,support vector
  machines,time series forecasting}
  \strng{namehash}{CLTFEH1}
  \strng{fullhash}{CLTFEH1}
  \field{labelalpha}{CT01}
  \field{sortinit}{C}
  \field{pages}{184\bibrangedash 192}
  \field{title}{{Financial Forecasting Using Support Vector Machines}}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley D
  \verb esktop/Downloaded/Cao, Tay - 2001 - Financial Forecasting Using Support
  \verb  Vector Machines.pdf:pdf
  \endverb
  \field{journaltitle}{Neural Computing \& Applications}
  \field{year}{2001}
\endentry

\entry{Cortes1995}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Cortes}{C.}%
     {Corinna}{C.}%
     {}{}%
     {}{}}%
    {{}%
     {Vapnik}{V.}%
     {Vladimir}{V.}%
     {}{}%
     {}{}}%
  }
  \keyw{efficient learning algorithms,neural networks,pattern
  recognition,polynomial classifiers,radial basis function classifiers}
  \strng{namehash}{CCVV1}
  \strng{fullhash}{CCVV1}
  \field{labelalpha}{CV95}
  \field{sortinit}{C}
  \field{abstract}{%
  The support-vector network is a new leaming machine for two-group
  classification problems. The machine conceptually implements the following
  idea: input vectors are non-linearly mapped to a very high- dimension feature
  space. In this feature space a linear decision surface is constructed.
  Special properties of the decision surface ensures high generalization
  ability of the learning machine. The idea behind the support-vector network
  was previously implemented for the restricted case where the training data
  can be separated without errors. We here extend this result to non-separable
  training data. High generalization ability of support-vector networks
  utilizing polynomial input transformations is demon- strated. We also compare
  the performance of the support-vector network to various classical learning
  algorithms that all took part in a benchmark study of Optical Character
  Recognition.%
  }
  \verb{doi}
  \verb 10.1007/BF00994018
  \endverb
  \verb{eprint}
  \verb arXiv:1011.1669v3
  \endverb
  \field{isbn}{0885-6125}
  \field{issn}{08856125}
  \field{number}{3}
  \field{pages}{273\bibrangedash 297}
  \field{title}{{Support-vector networks}}
  \field{volume}{20}
  \field{journaltitle}{Machine Learning}
  \field{eprinttype}{arXiv}
  \field{year}{1995}
\endentry

\entry{Campbell2011}{book}{}
  \name{author}{2}{}{%
    {{}%
     {Campbell}{C.}%
     {Colin}{C.}%
     {}{}%
     {}{}}%
    {{}%
     {Ying}{Y.}%
     {Yiming}{Y.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{CCYY1}
  \strng{fullhash}{CCYY1}
  \field{labelalpha}{CY11}
  \field{sortinit}{C}
  \field{booktitle}{Synthesis Lectures on Artificial Intelligence and Machine
  Learning}
  \verb{doi}
  \verb 10.2200/S00324ED1V01Y201102AIM010
  \endverb
  \field{isbn}{9781608456161}
  \field{issn}{1939-4608}
  \field{number}{1}
  \field{pages}{1\bibrangedash 95}
  \field{title}{{Learning with Support Vector Machines}}
  \verb{url}
  \verb http://www.morganclaypool.com/doi/abs/10.2200/S00324ED1V01Y201102AIM010
  \endverb
  \field{volume}{5}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley D
  \verb esktop/Downloaded/Campbell, Ying - 2011 - Learning with Support Vector
  \verb Machines.pdf:pdf
  \endverb
  \field{month}{02}
  \field{year}{2011}
\endentry

\entry{AhlameDouzal-Chouakria2011}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Douzal-Chouakria}{D.-C.}%
     {A.}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Amblard}{A.}%
     {C.}{C.}%
     {}{}%
     {}{}}%
  }
  \list{publisher}{1}{%
    {VLDB Endowment}%
  }
  \keyw{Classification,Supervised classification,Time series proximity
  measures,trees Learning metric}
  \strng{namehash}{DCAAC1}
  \strng{fullhash}{DCAAC1}
  \field{labelalpha}{DCA11}
  \field{sortinit}{D}
  \field{abstract}{%
  This paper proposes an extension of classification trees to time series input
  variables. A new split criterion based on time series proximities is
  introduced. First, the criterion relies on an adaptive (i.e., parameterized)
  time series metric to cover both behaviors and values proximities. The
  metrics parameters may change from one internal node to another to achieve
  the best bisection of the set of time series. Second, the criterion involves
  the automatic extraction of the most discriminating subsequences. The
  proposed time series classification tree is applied to a wide range of
  datasets: public and new, real and synthetic, univariate and multivariate
  data. We show, through the experiments performed in this study, that the
  proposed tree outperforms temporal trees using standard time series distances
  and performs well compared to other competitive time series classifiers%
  }
  \field{isbn}{0000000000000}
  \field{issn}{2150-8097}
  \field{title}{{Classification trees for time series}}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley D
  \verb esktop/Downloaded/Douzal-Chouakria, Amblard - 2011 - Classification tre
  \verb es for time series.pdf:pdf
  \endverb
  \field{journaltitle}{Pattern Recognition journal}
  \field{year}{2011}
\endentry

\entry{Chouakria2007}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Douzal-Chouakria}{D.-C.}%
     {A.}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Nagabhushan}{N.}%
     {P.}{P.}%
     {}{}%
     {}{}}%
  }
  \keyw{Classification,Dynamic time warping,Fr??chet distance,Time Series}
  \strng{namehash}{DCANP1}
  \strng{fullhash}{DCANP1}
  \field{labelalpha}{DCN07}
  \field{sortinit}{D}
  \field{abstract}{%
  Abstract The most widely used measures of time series proximity are the
  Euclidean distance and dynamic time warping. The latter can be derived from
  the distance introduced by Maurice Frchet in 1906 to account for the
  proximity between curves. The major limitation of these proximity measures is
  that they are based on the closeness of the values regardless of the
  similarity w.r.t. the growth behavior of the time series. To alleviate this
  drawback we propose a new dissimilarity index, based on an automatic adaptive
  tuning function, to include both proximity measures w.r.t. values and w.r.t.
  behavior. A comparative numerical analysis between the proposed index and the
  classical distance measures is performed on the basis of two datasets: a
  synthetic dataset and a dataset from a public health study.%
  }
  \verb{doi}
  \verb 10.1007/s11634-006-0004-6
  \endverb
  \field{issn}{18625347}
  \field{title}{{Adaptive dissimilarity index for measuring time series
  proximity}}
  \field{journaltitle}{Advances in Data Analysis and Classification}
  \field{year}{2007}
\endentry

\entry{Denoeux1995}{article}{}
  \name{author}{1}{}{%
    {{}%
     {Denoeux}{D.}%
     {T.}{T.}%
     {}{}%
     {}{}}%
  }
  \keyw{Dempster's rule of combination,Dempster-Shafer theory,Density
  functional theory,Error analysis,H infinity control,Medical services,Nearest
  neighbor searches,Neural networks,Voting,ambiguity,class membership,distance
  rejection,distance-weighted k-NN procedures,evidence,imperfect
  knowledge,inference mechanisms,k-nearest neighbor classification rule,pattern
  classification,statistical analysis,unseen pattern classification,voting}
  \strng{namehash}{DT1}
  \strng{fullhash}{DT1}
  \field{labelalpha}{Den95}
  \field{sortinit}{D}
  \field{abstract}{%
  In this paper, the problem of classifying an unseen pattern on the basis of
  its nearest neighbors in a recorded data set is addressed from the point of
  view of Dempster-Shafer theory. Each neighbor of a sample to be classified is
  considered as an item of evidence that supports certain hypotheses regarding
  the class membership of that pattern. The degree of support is defined as a
  function of the distance between the two vectors. The evidence of the k
  nearest neighbors is then pooled by means of Dempster's rule of combination.
  This approach provides a global treatment of such issues as ambiguity and
  distance rejection, and imperfect knowledge regarding the class membership of
  training patterns. The effectiveness of this classification scheme as
  compared to the voting and distance-weighted k-NN procedures is demonstrated
  using several sets of simulated and real-world data%
  }
  \verb{doi}
  \verb 10.1109/21.376493
  \endverb
  \field{issn}{00189472}
  \field{number}{5}
  \field{pages}{804\bibrangedash 813}
  \field{title}{{A k-nearest neighbor classification rule based on
  Dempster-Shafer theory}}
  \verb{url}
  \verb http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=376493
  \endverb
  \field{volume}{25}
  \field{journaltitle}{IEEE Transactions on Systems, Man, and Cybernetics}
  \field{year}{1995}
\endentry

\entry{Dietterich1995}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Dietterich}{D.}%
     {Thomas~G.}{T.~G.}%
     {}{}%
     {}{}}%
    {{}%
     {Hild}{H.}%
     {Hermann}{H.}%
     {}{}%
     {}{}}%
    {{}%
     {Bakiri}{B.}%
     {Ghulum}{G.}%
     {}{}%
     {}{}}%
  }
  \keyw{ID3,backpropagation,experimental comparisons,text-to-speech}
  \strng{namehash}{DTGHHBG1}
  \strng{fullhash}{DTGHHBG1}
  \field{labelalpha}{DHB95}
  \field{sortinit}{D}
  \field{abstract}{%
  The performance of the error backpropagation (BP) and ID3 learning algorithms
  was compared on the task of mapping English text to phonemes and stresses.
  Under the distributed output code developed by Sejnowski and Rosenberg, it is
  shown that BP consistently out-performs ID3 on this task by several
  percentage points. Three hypotheses explaining this difference were explored:
  (a) ID3 is overfitting the training data, (b) BP is able to share hidden
  units across several output units and hence can learn the output units
  better, and (c) BP captures statistical information that ID3 does not. We
  conclude that only hypothesis (c) is correct. By augmenting ID3 with a simple
  statistical learning procedure, the performance of BP can be closely matched.
  More complex statistical procedures can improve the performance of both BP
  and ID3 substantially in this domain.%
  }
  \verb{doi}
  \verb 10.1007/BF00993821
  \endverb
  \field{isbn}{0885-6125}
  \field{issn}{08856125}
  \field{number}{1}
  \field{pages}{51\bibrangedash 80}
  \field{title}{{A comparison of ID3 and backpropagation for English
  text-to-speech mapping}}
  \field{volume}{18}
  \field{journaltitle}{Machine Learning}
  \field{year}{1995}
\endentry

\entry{Dietterich1997}{article}{}
  \name{author}{1}{}{%
    {{}%
     {Dietterich}{D.}%
     {T.}{T.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{DT2}
  \strng{fullhash}{DT2}
  \field{labelalpha}{Die97}
  \field{sortinit}{D}
  \field{title}{{Approximate Statistical Tests for Comparing Supervised
  Classification Learning Algorithms}}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley D
  \verb esktop/Downloaded/Dietterich - 1997 - Approximate Statistical Tests for
  \verb  Comparing Supervised Classification Learning Algorithms.pdf:pdf
  \endverb
  \field{year}{1997}
\endentry

\entry{Ding2008}{article}{}
  \name{author}{5}{}{%
    {{}%
     {Ding}{D.}%
     {Hui}{H.}%
     {}{}%
     {}{}}%
    {{}%
     {Trajcevski}{T.}%
     {Goce}{G.}%
     {}{}%
     {}{}}%
    {{}%
     {Scheuermann}{S.}%
     {Peter}{P.}%
     {}{}%
     {}{}}%
    {{}%
     {Wang}{W.}%
     {Xiaoyue}{X.}%
     {}{}%
     {}{}}%
    {{}%
     {Keogh}{K.}%
     {Eamonn}{E.}%
     {}{}%
     {}{}}%
  }
  \list{publisher}{1}{%
    {VLDB Endowment}%
  }
  \strng{namehash}{DH+1}
  \strng{fullhash}{DHTGSPWXKE1}
  \field{labelalpha}{Din+08}
  \field{sortinit}{D}
  \field{abstract}{%
  The last decade has witnessed a tremendous growths of interests in
  applications that deal with querying and mining of time series data. Numerous
  representation methods for dimensionality reduction and similarity measures
  geared towards time series have been introduced. Each individual work
  introducing a particular method has made specific claims and, aside from the
  occasional theoretical justifications, provided quantitative experimental
  observations. However, for the most part, the comparative aspects of these
  experiments were too narrowly focused on demonstrating the benefits of the
  proposed methods over some of the previously introduced ones. In order to
  provide a comprehensive validation, we conducted an extensive set of time
  series experiments re-implementing 8 different representation methods and 9
  similarity measures and their variants, and testing their effectiveness on 38
  time series data sets from a wide variety of application domains. In this
  paper, we give an overview of these different techniques and present our
  comparative experimental findings regarding their effectiveness. Our
  experiments have provided both a unified validation of some of the existing
  achievements, and in some cases, suggested that certain claims in the
  literature may be unduly optimistic. 1.%
  }
  \verb{doi}
  \verb 10.1145/1454159.1454226
  \endverb
  \verb{eprint}
  \verb 1012.2789v1
  \endverb
  \field{isbn}{0000000000000}
  \field{issn}{2150-8097}
  \field{number}{2}
  \field{pages}{1542\bibrangedash 1552}
  \field{title}{{Querying and Mining of Time Series Data : Experimental
  Comparison of Representations and Distance Measures}}
  \verb{url}
  \verb http://dl.acm.org/citation.cfm?id=1454226
  \endverb
  \field{volume}{1}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley D
  \verb esktop/Downloaded/Ding, Trajcevski, Scheuermann - 2008 - Querying and M
  \verb ining of Time Series Data Experimental Comparison of Representations an
  \verb d Distance.pdf:pdf
  \endverb
  \field{journaltitle}{Proceedings of the VLDB Endowment}
  \field{eprinttype}{arXiv}
  \field{year}{2008}
\endentry

\entry{Fan2008}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Fan}{F.}%
     {RE}{R.}%
     {}{}%
     {}{}}%
    {{}%
     {Chang}{C.}%
     {KW}{K.}%
     {}{}%
     {}{}}%
    {{}%
     {Hsieh}{H.}%
     {CJ}{C.}%
     {}{}%
     {}{}}%
  }
  \keyw{large-scale linear classification,logistic regression,machine
  learning,open,source,support vector machines}
  \strng{namehash}{FRCKHC1}
  \strng{fullhash}{FRCKHC1}
  \field{labelalpha}{FCH08}
  \field{sortinit}{F}
  \field{abstract}{%
  LIBLINEAR is an open source library for large-scale linear classification. It
  supports logistic regression and linear support vector machines. We provide
  easy-to-use command-line tools and library calls for users and developers.
  Comprehensive documents are available for both beginners and advanced users.
  Experiments demonstrate that LIBLINEAR is very efficient on large sparse data
  sets.%
  }
  \verb{doi}
  \verb 10.1038/oby.2011.351
  \endverb
  \field{isbn}{089791497X}
  \field{issn}{15324435}
  \field{title}{{LIBLINEAR: A library for large linear classification}}
  \verb{url}
  \verb http://dl.acm.org/citation.cfm?id=1442794
  \endverb
  \field{journaltitle}{The Journal of Machine Learning}
  \field{year}{2008}
\endentry

\entry{Faloutsos1994}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Faloutsos}{F.}%
     {Christos}{C.}%
     {}{}%
     {}{}}%
    {{}%
     {Ranganathan}{R.}%
     {M.}{M.}%
     {}{}%
     {}{}}%
    {{}%
     {Manolopoulos}{M.}%
     {Yannis}{Y.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{FCRMMY1}
  \strng{fullhash}{FCRMMY1}
  \field{labelalpha}{FRM94}
  \field{sortinit}{F}
  \field{abstract}{%
  We present an ecient indexing method to locate 1-dimensional subsequences
  within a collection of sequences, such that the subsequences match a given
  (query) pattern within a specified tolerance. The idea is to map each data
  sequence into a small set of multidimensional rectangles in feature space.
  Then, these rectangles can be readily indexed using traditional spatial
  access methods, like the R*-tree [9]. In more detail, we use a sliding window
  over the data sequence and extract its features; the result is a trail in
  feature space. We propose an efficient and effective algorithm to divide such
  trails into sub-trails, which are subsequently represented by their Minimum
  Bounding Rectangles (MBRs). We also examine queries of varying lengths, and
  we show how to handle each case eciently. We implemented our method and
  carried out experiments on synthetic and real data (stock price movements).
  We compared the method to sequential scanning, which is the only obvious
  competitor. The results were excellent: our method accelerated the search
  time from 3 times up to 100 times.%
  }
  \verb{doi}
  \verb 10.1145/191843.191925
  \endverb
  \field{isbn}{0897916395}
  \field{issn}{01635808}
  \field{number}{2}
  \field{pages}{419\bibrangedash 429}
  \field{title}{{Fast subsequence matching in time-series databases}}
  \verb{url}
  \verb http://portal.acm.org/citation.cfm?doid=191843.191925
  \endverb
  \field{volume}{23}
  \field{journaltitle}{ACM SIGMOD Record}
  \field{year}{1994}
\endentry

\entry{Goldberger2004}{article}{}
  \name{author}{6}{}{%
    {{}%
     {Goldberger}{G.}%
     {Jacob}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Roweis}{R.}%
     {Sam~T}{S.~T.}%
     {}{}%
     {}{}}%
    {{}%
     {Hinton}{H.}%
     {Geoffrey~E}{G.~E.}%
     {}{}%
     {}{}}%
    {{}%
     {Salakhutdinov}{S.}%
     {Ruslan}{R.}%
     {}{}%
     {}{}}%
    {{}%
     {Roweis}{R.}%
     {Sam~T}{S.~T.}%
     {}{}%
     {}{}}%
    {{}%
     {Salakhutdinov}{S.}%
     {Ruslan}{R.}%
     {}{}%
     {}{}}%
  }
  \keyw{Learning/Statistics \& Optimisation,Theory \& Algorithms}
  \strng{namehash}{GJ+1}
  \strng{fullhash}{GJRSTHGESRRSTSR1}
  \field{labelalpha}{Gol+04}
  \field{sortinit}{G}
  \field{abstract}{%
  In this paper we propose a novel method for learning
  a$\backslash$r$\backslash$nMahalanobis distance measure to be used in the KNN
  classification$\backslash$r$\backslash$nalgorithm. The algorithm directly
  maximizes a stochastic variant of$\backslash$r$\backslash$nthe leave-one-out
  KNN score on the training set. It can also$\backslash$r$\backslash$nlearn a
  low-dimensional linear embedding of labeled data that
  can$\backslash$r$\backslash$nbe used for data visualization and fast
  classification.$\backslash$r$\backslash$nUnlike other methods, our
  classification model is non-parametric,$\backslash$r$\backslash$nmaking no
  assumptions about the shape of the class distributions
  or$\backslash$r$\backslash$nthe boundaries between them. The performance of
  the method$\backslash$r$\backslash$nis demonstrated on several data sets,
  both for metric learning and$\backslash$r$\backslash$nlinear dimensionality
  reduction.%
  }
  \verb{doi}
  \verb 10.1.1.108.7841
  \endverb
  \field{pages}{513\bibrangedash 520}
  \field{title}{{Neighbourhood Components Analysis}}
  \verb{url}
  \verb http://eprints.pascal-network.org/archive/00001570/
  \endverb
  \field{journaltitle}{Advances in Neural Information Processing Systems}
  \field{year}{2004}
\endentry

\entry{Hsu2008}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Hsu}{H.}%
     {Chih-Wei}{C.-W.}%
     {}{}%
     {}{}}%
    {{}%
     {Chang}{C.}%
     {Chih-Chung}{C.-C.}%
     {}{}%
     {}{}}%
    {{}%
     {Lin}{L.}%
     {Chih-Jen}{C.-J.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{HCWCCCLCJ1}
  \strng{fullhash}{HCWCCCLCJ1}
  \field{labelalpha}{HCL08}
  \field{sortinit}{H}
  \field{abstract}{%
  The support vector machine (SVM) is a popular classi cation technique.
  However, beginners who are not familiar with SVM often get unsatisfactory
  results since they miss some easy but signi cant steps. In this guide, we
  propose a simple procedure which usually gives reasonable results. developed
  well-differentiated superficial transitional cell bladder cancer.
  CONCLUSIONS: Patients with SCI often prefer SPC than other methods offered to
  them, because of quality-of-life issues. The incidence of significant
  complications might not be as high as previously reported, and with a
  commitment to careful follow-up, SPC can be a safe option for carefully
  selected patients if adequate surveillance can be ensured.%
  }
  \verb{doi}
  \verb 10.1177/02632760022050997
  \endverb
  \verb{eprint}
  \verb 0-387-31073-8
  \endverb
  \field{isbn}{013805326X}
  \field{issn}{1464-410X}
  \field{number}{1}
  \field{pages}{1396\bibrangedash 400}
  \field{title}{{A Practical Guide to Support Vector Classification}}
  \verb{url}
  \verb http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf
  \endverb
  \field{volume}{101}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/Google Drive/Th\`{e}se/Bibliographie/S
  \verb VM Librairie/A Practical Guide to Support Vector Classification.pdf:pdf
  \endverb
  \field{journaltitle}{BJU international}
  \field{eprinttype}{arXiv}
  \field{year}{2008}
\endentry

\entry{Hwang2012}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Hwang}{H.}%
     {Seok~Hwan}{S.~H.}%
     {}{}%
     {}{}}%
    {{}%
     {Ham}{H.}%
     {Dae~Heon}{D.~H.}%
     {}{}%
     {}{}}%
    {{}%
     {Kim}{K.}%
     {Joong~Hoon}{J.~H.}%
     {}{}%
     {}{}}%
  }
  \keyw{forecasting,forecasting performance,support vector machine}
  \strng{namehash}{HSHHDHKJH1}
  \strng{fullhash}{HSHHDHKJH1}
  \field{labelalpha}{HHK12}
  \field{sortinit}{H}
  \verb{doi}
  \verb 10.1007/s12205-012-1519-3
  \endverb
  \field{issn}{1226-7988}
  \field{number}{5}
  \field{pages}{870\bibrangedash 882}
  \field{title}{{Forecasting performance of LS-SVM for nonlinear hydrological
  time series}}
  \verb{url}
  \verb http://link.springer.com/10.1007/s12205-012-1519-3
  \endverb
  \field{volume}{16}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/Google Drive/Th\`{e}se/Bibliographie/S
  \verb VM + Time Series/Hwang-KSCE Journal of Civil Engineering-2012\_Forecast
  \verb ing performance of LS-SVM for nonlinear hydrological time series.pdf:pd
  \verb f
  \endverb
  \field{journaltitle}{KSCE Journal of Civil Engineering}
  \field{year}{2012}
\endentry

\entry{Heisele2001}{inproceedings}{}
  \name{author}{3}{}{%
    {{}%
     {Heisele}{H.}%
     {B}{B}%
     {}{}%
     {}{}}%
    {{}%
     {Ho}{H.}%
     {P}{P}%
     {}{}%
     {}{}}%
    {{}%
     {Poggio}{P.}%
     {T}{T}%
     {}{}%
     {}{}}%
  }
  \keyw{Active shape model,Biology computing,Face recognition,Image
  databases,Image recognition,Mouth,Robustness,SVM classifier,Solid
  modeling,Support vector machine classification,Support vector
  machines,clustering,component-based approach,facial components,feature
  extraction,feature vector,global methods,learning automata}
  \strng{namehash}{HBHPPT1}
  \strng{fullhash}{HBHPPT1}
  \field{labelalpha}{HHP01}
  \field{sortinit}{H}
  \field{abstract}{%
  We present a component-based method and two global methods for face
  recognition and evaluate them with respect to robustness against pose
  changes. In the component system we first locate facial components, extract
  them and combine them into a single feature vector which is classified by a
  Support Vector Machine (SVM). The two global systems recognize faces by
  classifying a single feature vector consisting of the gray values of the
  whole face image. In the first global system we trained a single SVM
  classifier for each person in the database. The second system consists of
  sets of viewpoint-specific SVM classifiers and involves clustering during
  training. We performed extensive tests on a database which included faces
  rotated up to about 40Â° in depth. The component system clearly outperformed
  both global systems on all tests.%
  }
  \field{booktitle}{IEEE International Conference on Computer Vision, ICCV}
  \verb{doi}
  \verb 10.1109/ICCV.2001.937693
  \endverb
  \field{isbn}{0-7695-1143-0}
  \field{issn}{1089-7801}
  \field{number}{July}
  \field{pages}{688\bibrangedash 694}
  \field{title}{{Face recognition with support vector machines: global versus
  component-based approach}}
  \verb{url}
  \verb http://dx.doi.org/10.1109/ICCV.2001.937693
  \endverb
  \field{volume}{2}
  \field{year}{2001}
\endentry

\entry{Hu2013}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Hu}{H.}%
     {Jianming}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Wang}{W.}%
     {Jianzhou}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Zeng}{Z.}%
     {Guowei}{G.}%
     {}{}%
     {}{}}%
  }
  \list{publisher}{1}{%
    {Elsevier Ltd}%
  }
  \keyw{Ensemble Empirical Mode Decomposition (EEMD),Support Vector Machine
  (SVM),Wind farm,Wind speed forecasting}
  \strng{namehash}{HJWJZG1}
  \strng{fullhash}{HJWJZG1}
  \field{labelalpha}{HWZ13}
  \field{sortinit}{H}
  \field{abstract}{%
  In this paper, a hybrid forecasting approach, which combines the Ensemble
  Empirical Mode Decomposition (EEMD) and the Support Vector Machine (SVM), is
  proposed to improve the quality of wind speed forecasting. The essence of the
  methodology incorporates three phases. First, the original data of wind speed
  are decomposed into a number of independent Intrinsic Mode Functions (IMFs)
  and one residual series by EEMD using the principle of decomposition. In
  order to forecast these IMFs, excepting the highest frequency acquired by
  EEMD, the respective estimates are yielded using the SVM algorithm. Finally,
  these respective estimates are combined into the final wind speed forecasts
  using the principle of ensemble. The proposed hybrid method is examined by
  forecasting the mean monthly wind speed of three wind farms located in
  northwest China. The obtained results confirm an observable improvement for
  the forecasting validity of the proposed hybrid approach. This tool shows
  great promise for the forecasting of intricate time series which are
  intrinsically highly volatile and irregular. ?? 2013 Elsevier Ltd.%
  }
  \verb{doi}
  \verb 10.1016/j.renene.2013.05.012
  \endverb
  \field{isbn}{0960-1481}
  \field{issn}{09601481}
  \field{pages}{185\bibrangedash 194}
  \field{title}{{A hybrid forecasting approach applied to wind speed time
  series}}
  \verb{url}
  \verb http://dx.doi.org/10.1016/j.renene.2013.05.012
  \endverb
  \field{volume}{60}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/Google Drive/Th\`{e}se/Bibliographie/S
  \verb VM + Time Series/Hu-Renewable Energy-2013\_A hybrid forecasting approac
  \verb h applied to wind speed time series.pdf:pdf
  \endverb
  \field{journaltitle}{Renewable Energy}
  \field{year}{2013}
\endentry

\entry{Jain1999}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Jain}{J.}%
     {a.~K.}{a.~K.}%
     {}{}%
     {}{}}%
    {{}%
     {Murty}{M.}%
     {M.~N.}{M.~N.}%
     {}{}%
     {}{}}%
    {{}%
     {Flynn}{F.}%
     {P.~J.}{P.~J.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{JaKMMNFPJ1}
  \strng{fullhash}{JaKMMNFPJ1}
  \field{labelalpha}{JMF99}
  \field{sortinit}{J}
  \field{abstract}{%
  Clustering is the unsupervised classification of patterns (observations, data
  items, or feature vectors) into groups (clusters). The clustering problem has
  been addressed in many contexts and by researchers in many disciplines; this
  reflects its broad appeal and usefulness as one of the steps in exploratory
  data analysis. However, clustering is a difficult problem combinatorially,
  and differences in assumptions and contexts in different communities has made
  the transfer of useful generic concepts and methodologies slow to occur. This
  paper presents an overview of pattern clustering methods from a statistical
  pattern recognition perspective, with a goal of providing useful advice and
  references to fundamental concepts accessible to the broad community of
  clustering practitioners. We present a taxonomy of clustering techniques, and
  identify cross-cutting themes and recent advances. We also describe some
  important applications of clustering algorithms such as image segmentation,
  object recognition, and information retrieval.%
  }
  \verb{doi}
  \verb 10.1145/331499.331504
  \endverb
  \verb{eprint}
  \verb arXiv:1101.1881v2
  \endverb
  \field{isbn}{0360-0300}
  \field{issn}{03600300}
  \field{number}{3}
  \field{pages}{264\bibrangedash 323}
  \field{title}{{Data clustering: a review}}
  \verb{url}
  \verb http://portal.acm.org/citation.cfm?doid=331499.331504
  \endverb
  \field{volume}{31}
  \field{journaltitle}{ACM Computing Surveys}
  \field{eprinttype}{arXiv}
  \field{year}{1999}
\endentry

\entry{Kalman1960}{article}{}
  \name{author}{1}{}{%
    {{}%
     {Kalman}{K.}%
     {R~E}{R.~E.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{KRE1}
  \strng{fullhash}{KRE1}
  \field{labelalpha}{Kal60}
  \field{sortinit}{K}
  \field{abstract}{%
  The classical filtering and prediction problem is re-examined using the Bode-
  Shannon representation of random processes and the ``state
  transition\&apos;\&apos; method of analysis of dynamic systems. New results
  are: (1) The formulation and methods of solution of the problem apply without
  modifica- tion to stationary and nonstationary statistics and to
  growing-memory and infinite- memory filters. (2) A nonlinear difference (or
  differential) equation is derived for the covariance matrix of the optimal
  estimation error. From the solution of this equation the co- efficients of
  the difference (or differential) equation of the optimal linear filter are
  ob- tained without further calculations. (3) The filtering problem is shown
  to be the dual of the noise-free regulator problem. The new method developed
  here is applied to two well-known problems, confirming and extending earlier
  results. The discussion is largely self-contained and proceeds from first
  principles; basic concepts of the theory of random processes are reviewed in
  the Appendix.%
  }
  \verb{doi}
  \verb 10.1115/1.3662552
  \endverb
  \field{isbn}{9783540769897}
  \field{issn}{0021-9223}
  \field{number}{Series D}
  \field{pages}{35\bibrangedash 45}
  \field{title}{{A New Approach to Linear Filtering and Prediction Problems}}
  \field{volume}{82}
  \field{journaltitle}{Transactions of the ASME Journal of Basic Engineering}
  \field{year}{1960}
\endentry

\entry{Keogh2004}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Keogh}{K.}%
     {Eamonn}{E.}%
     {}{}%
     {}{}}%
    {{}%
     {Ratanamahatana}{R.}%
     {Chotirat~Ann}{C.~A.}%
     {}{}%
     {}{}}%
  }
  \keyw{dynamic time warping,indexing,lower bounding,time series}
  \strng{namehash}{KERCA1}
  \strng{fullhash}{KERCA1}
  \field{labelalpha}{KR04}
  \field{sortinit}{K}
  \field{abstract}{%
  The problem of indexing time series has attracted much interest. Most
  algorithms used to index time series utilize the Euclidean distance or some
  variation thereof. However, it has been forcefully shown that the Euclidean
  distance is a very brittle distance measure. Dy- namic time warping (DTW) is
  a much more robust distance measure for time series, allowing similar shapes
  to match even if they are out of phase in the time axis. Because of this
  flexi- bility, DTW is widely used in science, medicine, industry and finance.
  Unfortunately, however, DTW does not obey the triangular inequality and thus
  has resisted attempts at exact indexing. Instead, many researchers have
  introduced approximate indexing techniques or abandoned the idea of indexing
  and concentrated on speeding up sequential searches. In this work, we intro-
  duce a novel technique for the exact indexing of DTW. We prove that our
  method guarantees no false dismissals and we demonstrate its vast superiority
  over all competing approaches in the largest and most comprehensive set of
  time series indexing experiments ever undertaken.%
  }
  \verb{doi}
  \verb 10.1007/s10115-004-0154-9
  \endverb
  \field{issn}{0219-1377}
  \field{number}{3}
  \field{pages}{358\bibrangedash 386}
  \field{title}{{Exact indexing of dynamic time warping}}
  \verb{url}
  \verb http://www.springerlink.com/index/10.1007/s10115-004-0154-9
  \endverb
  \field{volume}{7}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley D
  \verb esktop/Downloaded/Keogh, Ratanamahatana - 2004 - Exact indexing of dyna
  \verb mic time warping.pdf:pdf
  \endverb
  \field{journaltitle}{Knowledge and Information Systems}
  \field{month}{05}
  \field{year}{2004}
\endentry

\entry{Kijsirikul2002}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Kijsirikul}{K.}%
     {B}{B}%
     {}{}%
     {}{}}%
    {{}%
     {Ussivakul}{U.}%
     {N}{N}%
     {}{}%
     {}{}}%
  }
  \keyw{decision directed acyclic graph,decision levels,directed
  graphs,learning (artificial intelligence),learning automata,linear support
  vector machines,multiclass support vector machines,pattern
  classification,probability adaptive directed acyclic graph}
  \strng{namehash}{KBUN1}
  \strng{fullhash}{KBUN1}
  \field{labelalpha}{KU02}
  \field{sortinit}{K}
  \field{abstract}{%
  Presents a method of extending support vector machines (SVMs) for dealing
  with multiclass problems. Motivated by the decision directed acyclic graph
  (DDAG), we propose the adaptive DAG (ADAG): a modified structure of the DDAG
  that has a lower number of decision levels and reduces the dependency on the
  sequence of nodes. Thus, the ADAG improves the accuracy of the DDAG while
  maintaining low computational requirement%
  }
  \verb{doi}
  \verb 10.1109/IJCNN.2002.1005608
  \endverb
  \field{pages}{980\bibrangedash 985}
  \field{title}{{Multiclass Support Vector Machines using Adaptive Directed
  Acyclic Graph}}
  \field{volume}{1}
  \field{journaltitle}{Neural Networks, 2002. IJCNN '02. Proceedings of the
  2002 International Joint Conference on}
  \field{year}{2002}
\endentry

\entry{Lhermitte2011a}{article}{}
  \name{author}{4}{}{%
    {{}%
     {Lhermitte}{L.}%
     {S.}{S.}%
     {}{}%
     {}{}}%
    {{}%
     {Verbesselt}{V.}%
     {J.}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Verstraeten}{V.}%
     {W.W.}{W.}%
     {}{}%
     {}{}}%
    {{}%
     {Coppin}{C.}%
     {P.}{P.}%
     {}{}%
     {}{}}%
  }
  \keyw{Change detection,Classification,Ecosystem dynamics,Time series
  analysis}
  \strng{namehash}{LS+1}
  \strng{fullhash}{LSVJVWCP1}
  \field{labelalpha}{Lhe+11}
  \field{sortinit}{L}
  \field{abstract}{%
  Time series of remote sensing imagery or derived vegetation indices and
  biophysical products have been shown particularly useful to characterize land
  ecosystem dynamics. Various methods have been developed based on temporal
  trajectory analysis to characterize, classify and detect changes in ecosystem
  dynamics. Although time series similarity measures play an important role in
  these methods, a quantitative comparison of the similarity measures is
  lacking. The objective of this study was to provide an overview and
  quantitative comparison of the similarity measures in function of varying
  time series and ecosystem characteristics, such as amplitude, timing and
  noise effects. For this purpose, the performance was evaluated for the
  commonly used similarity measures (D), ranging from Manhattan (DMan),
  Euclidean (DE) and Mahalanobis (DMah) distance measures, to correlation
  (DCC), Principal Component Analysis (PCA; DPCA) and Fourier based
  (DFFT,D$\xi$,DFk) similarities. The quantitative comparison consists of a
  series of Monte-Carlo simulations based on subsets of global MODIS Normalized
  Difference Vegetation index (NDVI) and Enhanced Vegetation Index (EVI) and
  Leaf Area Index (LAI) data. Results of the simulations reveal four main
  groups of time series similarity measures with different sensitivities: (i)
  DMan, DE, DPCA, DFk quantify the difference in time series values, (ii) DMah
  accounts for temporal correlation and non-stationarity of variance, (iii) DCC
  measures the temporal correlation, and (iv) the Fourier based DFFT and D$\xi$
  show their specific sensitivity based on the selected Fourier components. The
  difference measures show relatively the highest sensitivity to amplitude
  effects, whereas the correlation based measures are highly sensitive to
  variations in timing and noise. The Fourier based measures, finally, depend
  highly on the signal to noise ratio and the balance between amplitude and
  phase dominance. The heterogeneity in sensitivity of each D stresses the
  importance of (i) understanding the time series characteristics before
  applying any classification of change detection approach and (ii) defining
  the variability one wants to identify/account for. This requires an
  understanding of the ecosystem dynamics and time series characteristics
  related to the baseline, amplitude, timing, noise and variability of the
  ecosystem time series. This is also illustrated in the quantitative
  comparison, where the different sensitivities of D for the NDVI, EVI, and LAI
  data relate specifically to the temporal characteristics of each data set.
  Additionally, the effect of noise and intra- and interclass variability is
  demonstrated in a case study based on land cover classification.%
  }
  \verb{doi}
  \verb 10.1016/j.rse.2011.06.020
  \endverb
  \field{isbn}{0034-4257}
  \field{issn}{00344257}
  \field{number}{12}
  \field{pages}{3129\bibrangedash 3152}
  \field{title}{{A comparison of time series similarity measures for
  classification and change detection of ecosystem dynamics}}
  \verb{url}
  \verb http://www.sciencedirect.com/science/article/pii/S0034425711002446
  \endverb
  \field{volume}{115}
  \field{journaltitle}{Remote Sensing of Environment}
  \field{year}{2011}
\endentry

\entry{Liang2012}{article}{}
  \name{author}{4}{}{%
    {{}%
     {Liang}{L.}%
     {Chunquan}{C.}%
     {}{}%
     {}{}}%
    {{}%
     {Zhang}{Z.}%
     {Yang}{Y.}%
     {}{}%
     {}{}}%
    {{}%
     {Shi}{S.}%
     {Peng}{P.}%
     {}{}%
     {}{}}%
    {{}%
     {Hu}{H.}%
     {Zhengguo}{Z.}%
     {}{}%
     {}{}}%
  }
  \list{publisher}{1}{%
    {Elsevier Inc.}%
  }
  \keyw{Positive unlabeled learning,Uncertain attribute,Uncertain data
  stream,Very fast decision tree}
  \strng{namehash}{LC+1}
  \strng{fullhash}{LCZYSPHZ1}
  \field{labelalpha}{Lia+12}
  \field{sortinit}{L}
  \field{abstract}{%
  Most data stream classification algorithms need to supply input with a large
  amount of precisely labeled data. However, in many data stream applications,
  streaming data contains inherent uncertainty, and labeled samples are
  difficult to be collected, while abundant data are unlabeled. In this paper,
  we focus on classifying uncertain data streams with only positive and
  unlabeled samples available. Based on concept-adapting very fast decision
  tree (CVFDT) algorithm, we propose an algorithm namely puuCVFDT (CVFDT for
  positive and unlabeled uncertain data). Experimental results on both
  synthetic and real-life datasets demonstrate the strong ability and
  efficiency of puuCVFDT to handle concept drift with uncertainty under
  positive and unlabeled learning scenario. Even when 90\% of the samples in
  the stream are unlabeled, the classification performance of the proposed
  algorithm is still compared to that of CVFDT, which is learned from fully
  labeled data without uncertainty. ?? 2012 Elsevier Inc. All rights reserved.%
  }
  \verb{doi}
  \verb 10.1016/j.ins.2012.05.023
  \endverb
  \field{issn}{00200255}
  \field{pages}{50\bibrangedash 67}
  \field{title}{{Learning very fast decision tree from uncertain data streams
  with positive and unlabeled samples}}
  \verb{url}
  \verb http://dx.doi.org/10.1016/j.ins.2012.05.023
  \endverb
  \field{volume}{213}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/Google Drive/Th\`{e}se/Bibliographie/D
  \verb ecision Tree/Liang-Elsevier-2012\_Learning very fast decision tree from
  \verb  uncertain data streams with positive and unlabeled samples.pdf:pdf
  \endverb
  \field{journaltitle}{Information Sciences}
  \field{year}{2012}
\endentry

\entry{McNames2002}{article}{}
  \name{author}{1}{}{%
    {{}%
     {McNames}{M.}%
     {James}{J.}%
     {}{}%
     {}{}}%
  }
  \keyw{Chaos,Embedding dimension,Local models,Metric optimization,Time series
  prediction}
  \strng{namehash}{MJ1}
  \strng{fullhash}{MJ1}
  \field{labelalpha}{McN02}
  \field{sortinit}{M}
  \field{abstract}{%
  Local models have emerged as one of the most accurate methods of time series
  prediction, but their performance is sensitive to the choice of
  user-specified parameters such as the size of the neighborhood, the embedding
  dimension, and the distance metric. This paper describes a new method of
  optimizing these parameters to minimize the multi-step cross-validation
  error. Empirical results indicate that multi-step optimization is susceptible
  to shallow local minima unless the optimization is limited to 10 or fewer
  steps ahead. The models optimized using the new method consistently performed
  better than those optimized with adaptive analog forecasts. ?? 2002 Elsevier
  Science B.V. All rights reserved.%
  }
  \verb{doi}
  \verb 10.1016/S0925-2312(01)00647-6
  \endverb
  \field{issn}{09252312}
  \field{number}{May 2001}
  \field{pages}{279\bibrangedash 297}
  \field{title}{{Local averaging optimization for chaotic time series
  prediction}}
  \field{volume}{48}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/Desktop/Neurocomputing.pdf:pdf
  \endverb
  \field{journaltitle}{Neurocomputing}
  \field{year}{2002}
\endentry

\entry{Morse2007}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Morse}{M.}%
     {Michael~D}{M.~D.}%
     {}{}%
     {}{}}%
    {{}%
     {Patel}{P.}%
     {Jignesh~M}{J.~M.}%
     {}{}%
     {}{}}%
  }
  \keyw{clustering,time series,trajectory similarity}
  \strng{namehash}{MMDPJM1}
  \strng{fullhash}{MMDPJM1}
  \field{labelalpha}{MP07}
  \field{sortinit}{M}
  \field{abstract}{%
  A variety of techniques currently exist for measuring the similarity between
  time series datasets. Of these techniques, the methods whosematching criteria
  is bounded by a specified Ç« threshold value, such as the LCSS and the EDR
  techniques, have been shown to be robust in the presence of noise, time
  shifts, and data scaling. Our work proposes a new algorithm, called the Fast
  Time Series Evaluation (FTSE) method, which can be used to evaluate such
  threshold value techniques, including LCSS and EDR. Using FTSE,we show that
  these techniques can be evaluated faster than using either traditional
  dynamic programming or even warp-restricting methods such as the Sakoe-Chiba
  band and the Itakura Parallelogram. We also show that FTSE can be used in a
  framework that can evaluate a richer range of epsilon threshold-based scoring
  techniques, of which EDR and LCSS are just two examples. This framework,
  called Swale, extends the epsilon threshold-based scoring techniques to
  include arbitrary match rewards and gap penalties. Through extensive
  empirical evaluation, we show that Swale can obtain greater accuracy than
  existing methods.%
  }
  \verb{doi}
  \verb 10.1145/1247480.1247544
  \endverb
  \field{isbn}{9781595936868}
  \field{pages}{569}
  \field{title}{{An efficient and accurate method for evaluating time series
  similarity}}
  \verb{url}
  \verb http://dl.acm.org/citation.cfm?id=1247544$\backslash$nhttp://portal.acm
  \verb .org/citation.cfm?doid=1247480.1247544
  \endverb
  \field{journaltitle}{ACM SIGMOD international conference on Management of
  data}
  \field{year}{2007}
\endentry

\entry{Montero2014}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Montero}{M.}%
     {Pablo}{P.}%
     {}{}%
     {}{}}%
    {{}%
     {Vilar}{V.}%
     {Jos\'{e}}{J.}%
     {}{}%
     {}{}}%
  }
  \keyw{clustering,dissimilarity measure,time series data,validation indices}
  \strng{namehash}{MPVJ1}
  \strng{fullhash}{MPVJ1}
  \field{labelalpha}{MV14}
  \field{sortinit}{M}
  \field{abstract}{%
  Time series clustering is an active research area with applications in a wide
  range of fields. One key component in cluster analysis is determining a
  proper dissimilarity mea- sure between two data objects, and many criteria
  have been proposed in the literature to assess dissimilarity between two time
  series. The R package TSclust is aimed to im- plement a large set of
  well-established peer-reviewed time series dissimilarity measures, including
  measures based on raw data, extracted features, underlying parametric models,
  complexity levels, and forecast behaviors. Computation of these measures
  allows the user to perform clustering by using conventional clustering
  algorithms. TSclust also includes a clustering procedure based on p values
  from checking the equality of generating models, and some utilities to
  evaluate cluster solutions. The implemented dissimilarity functions are
  accessible individually for an easier extension and possible use out of the
  clustering context. The main features of TSclust are described and examples
  of its use are presented.%
  }
  \field{number}{1}
  \field{title}{{TSclust : An R Package for Time Series Clustering}}
  \verb{url}
  \verb http://www.jstatsoft.org/v62/i01/paper
  \endverb
  \field{volume}{62}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/Desktop/TS clust.pdf:pdf
  \endverb
  \field{journaltitle}{Journal of Statistical Software November}
  \field{year}{2014}
\endentry

\entry{Najmeddine2012}{inproceedings}{}
  \name{author}{4}{}{%
    {{}%
     {Najmeddine}{N.}%
     {H.}{H.}%
     {}{}%
     {}{}}%
    {{}%
     {Jay}{J.}%
     {A.}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Marechal}{M.}%
     {P.}{P.}%
     {}{}%
     {}{}}%
    {{}%
     {Mari\'{e}}{M.}%
     {S.}{S.}%
     {}{}%
     {}{}}%
  }
  \keyw{Data mining,INCAS.,Time series,diagnosis and decision
  support,sensors,similarity measures}
  \strng{namehash}{NH+1}
  \strng{fullhash}{NHJAMPMS1}
  \field{labelalpha}{Naj+12}
  \field{sortinit}{N}
  \field{booktitle}{RFIA}
  \field{isbn}{9782953951523}
  \field{title}{{Mesures de similarit\'{e} pour lâaide \`{a} lâanalyse des
  donn\'{e}es \'{e}nerg\'{e}tiques de b\^{a}timents}}
  \verb{url}
  \verb https://hal-cea.archives-ouvertes.fr/file/index/docid/661016/filename/a
  \verb rticle53\_modif.pdf
  \endverb
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley D
  \verb esktop/Downloaded/Najmeddine et al. - 2012 - Mesures de similarit\'{e}
  \verb pour lâaide \`{a} lâanalyse des donn\'{e}es \'{e}nerg\'{e}tiques de
  \verb  b\^{a}timents.pdf:pdf
  \endverb
  \field{year}{2012}
\endentry

\entry{Nguyen2012}{inproceedings}{}
  \name{author}{5}{}{%
    {{}%
     {Nguyen}{N.}%
     {L.}{L.}%
     {}{}%
     {}{}}%
    {{}%
     {Wu}{W.}%
     {P.}{P.}%
     {}{}%
     {}{}}%
    {{}%
     {Chan}{C.}%
     {W.}{W.}%
     {}{}%
     {}{}}%
    {{}%
     {Peng}{P.}%
     {W.}{W.}%
     {}{}%
     {}{}}%
    {{}%
     {Zhang}{Z.}%
     {Y.}{Y.}%
     {}{}%
     {}{}}%
  }
  \keyw{sentiment analysis,sentiment prediction,social network analysis}
  \strng{namehash}{NL+1}
  \strng{fullhash}{NLWPCWPWZY1}
  \field{labelalpha}{Ngu+12}
  \field{sortinit}{N}
  \field{abstract}{%
  More and more people express their opinions on social media such as Facebook
  and Twitter. Predictive analysis on social media time-series allows the
  stake-holders to leverage this immediate, accessible and vast reachable
  communication channel to react and proact against the public opinion. In
  particular, understanding and predicting the sentiment change of the public
  opinions will allow business and government agencies to react against
  negative sentiment and design strategies such as dispelling rumors and post
  balanced messages to revert the public opinion. In this paper, we present a
  strategy of building statistical models from the social media dynamics to
  predict collective sentiment dynamics. We model the collective sentiment
  change without delving into micro analysis of individual tweets or users and
  their corresponding low level network structures. Experiments on large-scale
  Twitter data show that the model can achieve above 85\% accuracy on
  directional sentiment prediction.%
  }
  \field{booktitle}{WISDOM}
  \verb{doi}
  \verb 10.1145/2346676.2346682
  \endverb
  \field{isbn}{9781450315432}
  \field{title}{{Predicting collective sentiment dynamics from time-series
  social media}}
  \field{year}{2012}
\endentry

\entry{Duda1973}{book}{}
  \name{author}{2}{}{%
    {{}%
     {{O Duda}}{O.}%
     {Richard}{R.}%
     {}{}%
     {}{}}%
    {{}%
     {{E Hart}}{E.}%
     {Peter}{P.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{OREP1}
  \strng{fullhash}{OREP1}
  \field{labelalpha}{OE73}
  \field{sortinit}{O}
  \field{abstract}{%
  Classic book on pattern recognition. Interesting points: 1) p. 66, and p.
  114: Mentions the problems with dimensionality curse. 2) p. 243-246: Mentions
  Multidimensional scaling (MDS), Karhunen-Loeve and dimensionality reduction.
  Also, has the spiral data-set as a sample. 3) p. 333: mentions
  SVD/eigenvalues for linear fitting.%
  }
  \field{booktitle}{Leonardo}
  \verb{doi}
  \verb 10.2307/1573081
  \endverb
  \field{isbn}{0471223611}
  \field{issn}{0024094X}
  \field{pages}{482}
  \field{title}{{Pattern Classification and Scene Analysis}}
  \verb{url}
  \verb http://www.jstor.org/stable/1573081?origin=crossref
  \endverb
  \field{volume}{7}
  \field{year}{1973}
\endentry

\entry{PANAGIOTAKIS2008}{article}{}
  \name{author}{5}{}{%
    {{}%
     {PANAGIOTAKIS}{P.}%
     {COSTAS}{C.}%
     {}{}%
     {}{}}%
    {{}%
     {RAMASSO}{R.}%
     {EMMANUEL}{E.}%
     {}{}%
     {}{}}%
    {{}%
     {TZIRITAS}{T.}%
     {GEORGIOS}{G.}%
     {}{}%
     {}{}}%
    {{}%
     {ROMBAUT}{R.}%
     {MICH\`{E}LE}{M.}%
     {}{}%
     {}{}}%
    {{}%
     {PELLERIN}{P.}%
     {DENIS}{D.}%
     {}{}%
     {}{}}%
  }
  \keyw{People detection,people counting,team activity recognition,transferable
  belief model,video analysis}
  \strng{namehash}{PC+1}
  \strng{fullhash}{PCRETGRMPD1}
  \field{labelalpha}{PAN+08}
  \field{sortinit}{P}
  \field{abstract}{%
  We present a shape-based method for automatic people detection and counting
  without any assumption concerning camera motion. In order to evaluate the
  robustness of the proposed method, we apply it for classifying athletics
  videos into two classes: videos of individual and videos of team sports. The
  videos used are real and characterized by dynamic and unconstrained
  environment. Moreover, in the case of team sport, we propose a shape
  deformations based method for running/hurdling discrimination (activity
  recognition). Robust, adaptive and independent from color, illumination
  changes and the camera motion, the proposed features are combined in the
  Transferable Belief Model (TBM) framework providing a two-level (frames and
  shot) video categorization. Experimental results of 97\% of accuracy for
  individual/team sport categorization using a dataset of 252 real videos of
  athletic meetings, acquired by moving cameras under varying view angles,
  indicate the stability and the good performance of the proposed scheme.%
  }
  \verb{doi}
  \verb 10.1142/S0218001408006752
  \endverb
  \field{issn}{0218-0014}
  \field{number}{06}
  \field{pages}{1187\bibrangedash 1213}
  \field{title}{{SHAPE-BASED INDIVIDUAL/GROUP DETECTION FOR SPORT VIDEOS
  CATEGORIZATION}}
  \verb{url}
  \verb http://www.worldscientific.com/doi/abs/10.1142/S0218001408006752?journa
  \verb lCode=ijprai
  \endverb
  \field{volume}{22}
  \field{journaltitle}{International Journal of Pattern Recognition and
  Artificial Intelligence}
  \field{year}{2008}
\endentry

\entry{Prekopcsak2012}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Prekopcs\'{a}k}{P.}%
     {Zolt\'{a}n}{Z.}%
     {}{}%
     {}{}}%
    {{}%
     {Lemire}{L.}%
     {Daniel}{D.}%
     {}{}%
     {}{}}%
  }
  \keyw{Distance measure learning,Mahalanobis distance measure,Nearest
  Neighbor,Time-series classification}
  \strng{namehash}{PZLD1}
  \strng{fullhash}{PZLD1}
  \field{labelalpha}{PL12}
  \field{sortinit}{P}
  \field{abstract}{%
  To classify time series by nearest neighbors, we need to specify or learn one
  or several distance measures. We consider variations of the Mahalanobis
  distance measures which rely on the inverse covariance matrix of the data.
  Unfortunately --- for time series data --- the covariance matrix has often
  low rank. To alleviate this problem we can either use a pseudoinverse,
  covariance shrinking or limit the matrix to its diagonal. We review these
  alternatives and benchmark them against competitive methods such as the
  related Large Margin Nearest Neighbor Classification (LMNN) and the Dynamic
  Time Warping (DTW) distance. As we expected, we find that the DTW is
  superior, but the Mahalanobis distance measures are one to two orders of
  magnitude faster. To get best results with Mahalanobis distance measures, we
  recommend learning one distance measure per class using either covariance
  shrinking or the diagonal approach.%
  }
  \verb{doi}
  \verb 10.1007/s11634-012-0110-6
  \endverb
  \verb{eprint}
  \verb 1010.1526
  \endverb
  \field{isbn}{1163401201106}
  \field{issn}{18625347}
  \field{number}{3}
  \field{pages}{185\bibrangedash 200}
  \field{title}{{Time series classification by class-specific Mahalanobis
  distance measures}}
  \field{volume}{6}
  \field{journaltitle}{Advances in Data Analysis and Classification}
  \field{eprinttype}{arXiv}
  \field{year}{2012}
\endentry

\entry{Ramasso2008}{article}{}
  \name{author}{4}{}{%
    {{}%
     {Ramasso}{R.}%
     {E.}{E.}%
     {}{}%
     {}{}}%
    {{}%
     {Panagiotakis}{P.}%
     {C.}{C.}%
     {}{}%
     {}{}}%
    {{}%
     {Pellerin}{P.}%
     {D.}{D.}%
     {}{}%
     {}{}}%
    {{}%
     {Rombaut}{R.}%
     {M.}{M.}%
     {}{}%
     {}{}}%
  }
  \keyw{Human action recognition,Moving camera,Temporal Belief
  Filter,Transferable Belief Model}
  \strng{namehash}{RE+1}
  \strng{fullhash}{REPCPDRM1}
  \field{labelalpha}{Ram+08}
  \field{sortinit}{R}
  \field{abstract}{%
  This paper focuses on human behavior recognition where the main
  problem$\backslash$nis to bridge the semantic gap between the analogue
  observations of the$\backslash$nreal world and the symbolic world of human
  interpretation. For that, a$\backslash$nfusion architecture based on the
  Transferable Belief Model framework is$\backslash$nproposed and applied to
  action recognition of an athlete in video$\backslash$nsequences of athletics
  meeting with moving camera. Relevant features are$\backslash$nextracted from
  videos, based on both the camera motion analysis and the$\backslash$ntracking
  of particular points on the athlete's silhouette. Some models$\backslash$nof
  interpretation are used to link the numerical features to the
  symbols$\backslash$nto be recognized, which are running, jumping and falling
  actions. A$\backslash$nTemporal Belief Filter is then used to improve the
  robustness of action$\backslash$nrecognition. The proposed approach
  demonstrates good performance when$\backslash$ntested on real videos of
  athletics sports videos (high jumps, pole$\backslash$nvaults, triple jumps
  and long jumps) acquired by a moving camera and$\backslash$ndifferent view
  angles. The proposed system is also compared to
  Bayesian$\backslash$nNetworks.%
  }
  \verb{doi}
  \verb 10.1007/s10044-007-0073-y
  \endverb
  \field{issn}{14337541}
  \field{number}{1}
  \field{pages}{1\bibrangedash 19}
  \field{title}{{Human action recognition in videos based on the transferable
  belief model : AAAApplication to athletics jumps}}
  \field{volume}{11}
  \field{journaltitle}{Pattern Analysis and Applications}
  \field{year}{2008}
\endentry

\entry{Rabiner1993}{book}{}
  \name{author}{2}{}{%
    {{}%
     {Rabiner}{R.}%
     {L.}{L.}%
     {}{}%
     {}{}}%
    {{}%
     {Juang}{J.}%
     {B.}{B.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{RLJB1}
  \strng{fullhash}{RLJB1}
  \field{labelalpha}{RJ93}
  \field{sortinit}{R}
  \field{abstract}{%
  Provides a theoretically sound, technically accurate, and complete
  description of the basic knowledge and ideas that constitute a modern system
  for speech recognition by machine. Covers production, perception, and
  acoustic-phonetic characterization of the speech signal; signal processing
  and analysis methods for speech recognition; pattern comparison techniques;
  speech recognition system design and implementation; theory and
  implementation of hidden Markov models; speech recognition based on connected
  word models; large vocabulary continuous speech recognition; and task-
  oriented application of automatic speech recognition. For practicing
  engineers, scientists, linguists, and programmers interested in speech
  recognition.%
  }
  \field{booktitle}{Prentice Hall}
  \verb{doi}
  \verb 10.1002/ev.1647
  \endverb
  \field{isbn}{0130151572}
  \field{title}{{Fundamentals of Speech Recognition}}
  \verb{url}
  \verb http://cmp.felk.cvut.cz/cmp/support/phd112.html
  \endverb
  \field{volume}{103}
  \field{year}{1993}
\endentry

\entry{Salvador}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Salvador}{S.}%
     {Stan}{S.}%
     {}{}%
     {}{}}%
    {{}%
     {Chan}{C.}%
     {Philip}{P.}%
     {}{}%
     {}{}}%
  }
  \keyw{dynamic time warping,time series}
  \strng{namehash}{SSCP1}
  \strng{fullhash}{SSCP1}
  \field{labelalpha}{SC}
  \field{sortinit}{S}
  \field{abstract}{%
  The dynamic time warping (DTW) algorithm is able to find the optimal
  alignment between two time series. It is often used to determine time series
  similarity, classification, and to find corresponding regions between two
  time series. DTW has a quadratic time and space complexity that limits its
  use to only small time series data sets. In this paper we introduce FastDTW,
  an approximation of DTW that has a linear time and space complexity. FastDTW
  uses a multilevel approach that recursively projects a solution from a coarse
  resolution and refines the projected solution. We prove the linear time and
  space complexity of FastDTW both theoretically and empirically. We also
  analyze the accuracy of FastDTW compared to two other existing approximate
  DTW algorithms: Sakoe-Chuba Bands and Data Abstraction. Our results show a
  large improvement in accuracy over the existing methods.%
  }
  \field{title}{{FastDTW : Toward Accurate Dynamic Time Warping in Linear Time
  and Space}}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley D
  \verb esktop/Downloaded/Salvador, Chan - Unknown - FastDTW Toward Accurate Dy
  \verb namic Time Warping in Linear Time and Space.pdf:pdf
  \endverb
\endentry

\entry{Sakoe1978a}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Sakoe}{S.}%
     {H.}{H.}%
     {}{}%
     {}{}}%
    {{}%
     {Chiba}{C.}%
     {S.}{S.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{SHCS1}
  \strng{fullhash}{SHCS1}
  \field{labelalpha}{SC78}
  \field{sortinit}{S}
  \field{abstract}{%
  This paper reports on an optimum dynamic programming (DP) based
  time-normalization algorithm for spoken word recognition. First, a general
  principle of time-normalization is given using timewarping function. Then,
  two time-normalized distance definitions, d e d symmetric and asymmetric
  forms, are derived from the principle. These two forms are compared with each
  other through theoretical discussions and experimental studies. The symmetric
  form algorithm superiority is established. A new technique, called slope
  constraint, is successfully introduced, iwn hich the warping function slope
  isr estricted so as to improve discrimination between words in different
  categories. The effective slope constraint characteristic is qualitatively
  analyzed, and the optimum slope constraint condition is determined through
  experiments. The optimized algorithm is then extensively subjected to
  experimentat comparison with various DP-algorithms, previously applied to
  spoken word recognition by different research groups. The experiment shows
  that the present algorithm gives no more than about twothirds errors, even
  compared to the best conventional algorithm.%
  }
  \verb{doi}
  \verb 10.1109/TASSP.1978.1163055
  \endverb
  \field{isbn}{1558601244}
  \field{issn}{00963518}
  \field{title}{{Dynamic Programming Algorithm Optimization for Spoken Word
  Recognition}}
  \field{journaltitle}{IEEE transactions on acoustics, speech, and signal
  processing}
  \field{year}{1978}
\endentry

\entry{Shental2002}{article}{}
  \name{author}{4}{}{%
    {{}%
     {Shental}{S.}%
     {Noam}{N.}%
     {}{}%
     {}{}}%
    {{}%
     {Hertz}{H.}%
     {Tomer}{T.}%
     {}{}%
     {}{}}%
    {{}%
     {Weinshall}{W.}%
     {Daphna}{D.}%
     {}{}%
     {}{}}%
    {{}%
     {Pavel}{P.}%
     {Misha}{M.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{SN+1}
  \strng{fullhash}{SNHTWDPM1}
  \field{labelalpha}{She+02}
  \field{sortinit}{S}
  \field{abstract}{%
  We present a method for training a similarity metric from data. The method
  can be used for recognition or verification applications where the number of
  categories is very large and not known during training, and where the number
  of training samples for a single category is very small. The idea is to learn
  a function that maps input patterns into a target space such that the L1 norm
  in the target space approximates the "semantic" distance in the input space.
  The method is applied to a face verification task. The learning process
  minimizes a discriminative loss function that drives the similarity metric to
  be small for pairs of faces from the same person, and large for pairs from
  different persons. The mapping from raw to the target space is a
  convolutional network whose architecture is designed for robustness to
  geometric distortions. The system is tested on the Purdue/AR face database
  which has a very high degree of variability in the pose, lighting,
  expression, position, and artificial occlusions such as dark glasses and
  obscuring scarves.%
  }
  \verb{doi}
  \verb 10.1007/3-540-47979-1\_52
  \endverb
  \field{isbn}{978-3-540-43748-2}
  \field{pages}{776\bibrangedash 790}
  \field{title}{{Adjustment Learning and Relevant Component Analysis}}
  \verb{url}
  \verb http://dx.doi.org/10.1007/3-540-47979-1\_52
  \endverb
  \field{volume}{2353}
  \field{journaltitle}{European Conference on Computer Vision (ECCV)}
  \field{year}{2002}
\endentry

\entry{Sahidullah2012}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Sahidullah}{S.}%
     {Md}{M.}%
     {}{}%
     {}{}}%
    {{}%
     {Saha}{S.}%
     {Goutam}{G.}%
     {}{}%
     {}{}}%
  }
  \keyw{Block transform,Correlation matrix,DCT,Decorrelation technique,Linear
  transformation,MFCC,Missing feature theory,Narrow-band noise,Speaker
  recognition}
  \strng{namehash}{SMSG1}
  \strng{fullhash}{SMSG1}
  \field{labelalpha}{SS12}
  \field{sortinit}{S}
  \field{abstract}{%
  Standard Mel frequency cepstrum coefficient (MFCC) computation technique
  utilizes discrete cosine transform (DCT) for decorrelating log energies of
  filter bank output. The use of DCT is reasonable here as the covariance
  matrix of Mel filter bank log energy (MFLE) can be compared with that of
  highly correlated Markov-I process. This full-band based MFCC computation
  technique where each of the filter bank output has contribution to all
  coefficients, has two main disadvantages. First, the covariance matrix of the
  log energies does not exactly follow Markov-I property. Second, full-band
  based MFCC feature gets severely degraded when speech signal is corrupted
  with narrow-band channel noise, though few filter bank outputs may remain
  unaffected. In this work, we have studied a class of linear transformation
  techniques based on block wise transformation of MFLE which effectively
  decorrelate the filter bank log energies and also capture speech information
  in an efficient manner. A thorough study has been carried out on the block
  based transformation approach by investigating a new partitioning technique
  that highlights associated advantages. This article also reports a novel
  feature extraction scheme which captures complementary information to wide
  band information; that otherwise remains undetected by standard MFCC and
  proposed block transform (BT) techniques. The proposed features are evaluated
  on NIST SRE databases using Gaussian mixture model-universal background model
  (GMM-UBM) based speaker recognition system. We have obtained significant
  performance improvement over baseline features for both matched and
  mismatched condition, also for standard and narrow-band noises. The proposed
  method achieves significant performance improvement in presence of
  narrow-band noise when clubbed with missing feature theory based score
  computation scheme. ?? 2011 Elsevier B.V. All rights reserved.%
  }
  \verb{doi}
  \verb 10.1016/j.specom.2011.11.004
  \endverb
  \field{isbn}{0167-6393}
  \field{issn}{01676393}
  \field{number}{4}
  \field{pages}{543\bibrangedash 565}
  \field{title}{{Design, analysis and experimental evaluation of block based
  transformation in MFCC computation for speaker recognition}}
  \verb{url}
  \verb http://dx.doi.org/10.1016/j.specom.2011.11.004
  \endverb
  \field{volume}{54}
  \field{journaltitle}{Speech Communication}
  \field{year}{2012}
\endentry

\entry{Schlkopf2013}{book}{}
  \name{author}{2}{}{%
    {{}%
     {Schlkopf}{S.}%
     {Bernhard}{B.}%
     {}{}%
     {}{}}%
    {{}%
     {Smola}{S.}%
     {Alexander~J.}{A.~J.}%
     {}{}%
     {}{}}%
  }
  \keyw{icle}
  \strng{namehash}{SBSAJ1}
  \strng{fullhash}{SBSAJ1}
  \field{labelalpha}{SS13}
  \field{sortinit}{S}
  \field{abstract}{%
  Predicting the binding mode of flexible polypeptides to proteins is an
  important task that falls outside the domain of applicability of most small
  molecule and proteinâprotein docking tools. Here, we test the small
  molecule flexible ligand docking program Glide on a set of 19
  non-$\alpha$-helical peptides and systematically improve pose prediction
  accuracy by enhancing Glide sampling for flexible polypeptides. In addition,
  scoring of the poses was improved by post-processing with physics-based
  implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10
  scoring poses as a metric, the success rate (RMSD â¤ 2.0 \AA for the
  interface backbone atoms) increased from 21\% with default Glide SP settings
  to 58\% with the enhanced peptide sampling and scoring protocol in the case
  of redocking to the native protein structure. This approaches the accuracy of
  the recently developed Rosetta FlexPepDock method (63\% success for these 19
  peptides) while being over 100 times faster. Cross-docking was performed for
  a subset of cases where an unbound receptor structure was available, and in
  that case, 40\% of peptides were docked successfully. We analyze the results
  and find that the optimized polypeptide protocol is most accurate for
  extended peptides of limited size and number of formal charges, defining a
  domain of applicability for this approach.%
  }
  \field{booktitle}{Journal of Chemical Information and Modeling}
  \verb{doi}
  \verb 10.1017/CBO9781107415324.004
  \endverb
  \verb{eprint}
  \verb arXiv:1011.1669v3
  \endverb
  \field{isbn}{9788578110796}
  \field{issn}{1098-6596}
  \field{pages}{1689\bibrangedash 1699}
  \field{title}{{Learning with Kernels}}
  \field{volume}{53}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/Google Drive/Th\`{e}se/Bibliographie/L
  \verb ivre/Schokopf, Smola-Learning with Kernels.pdf:pdf
  \endverb
  \field{eprinttype}{arXiv}
  \field{year}{2013}
\endentry

\entry{Sadri2003}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Sadri}{S.}%
     {Javad}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Suen}{S.}%
     {Ching~Y}{C.~Y.}%
     {}{}%
     {}{}}%
    {{}%
     {Bui}{B.}%
     {Tien~D.}{T.~D.}%
     {}{}%
     {}{}}%
  }
  \keyw{feature extraction,machine learning,mlp neural network,multiple support
  vector classifiers,ocr,optical character recognition,support,svm,vector
  machine}
  \strng{namehash}{SJSCYBTD1}
  \strng{fullhash}{SJSCYBTD1}
  \field{labelalpha}{SSB03}
  \field{sortinit}{S}
  \field{abstract}{%
  : A new method for recognition of isolated handwritten Arabic/Persian digits
  is presented. This method is based on Support Vector Machines (SVMs), and a
  new approach of feature extraction. Each digit is considered from four
  different views, and from each view 16 features are extracted and combined to
  obtain 64 features. Using these features, multiple SVM classifiers are
  trained to separate different classes of digits. CENPARMI Indian
  (Arabic/Persian) handwritten digit database is used for training and testing
  of SVM classifiers. Based on this database, differences between Arabic and
  Persian digits in digit recognition are shown. This database provides 7390
  samples for training and 3035 samples for testing from the real life samples.
  Experiments show that the proposed features can provide a very good
  recognition result using Support Vector Machines at a recognition rate
  94.14\%, compared with 91.25 \% obtained by MLP neural network classifier
  using the same features and test set.%
  }
  \field{pages}{300\bibrangedash 307}
  \field{title}{{Application of Support Vector Machines for recognition of
  handwritten Arabic/Persian digits}}
  \field{volume}{1}
  \field{journaltitle}{Second Conference on Machine Vision and Image Processing
  \& Applications (MVIP 2003)}
  \field{year}{2003}
\endentry

\entry{Torrence1998}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Torrence}{T.}%
     {Christopher}{C.}%
     {}{}%
     {}{}}%
    {{}%
     {Compo}{C.}%
     {Gilbert~P.}{G.~P.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{TCCGP1}
  \strng{fullhash}{TCCGP1}
  \field{labelalpha}{TC98}
  \field{sortinit}{T}
  \field{abstract}{%
  A practical step-by-step guide to wavelet analysis is given, with examples
  taken from time series of the El Ni\~{n}oâSouthern Oscillation (ENSO). The
  guide includes a comparison to the windowed Fourier transform, the choice of
  an appropriate wavelet basis function, edge effects due to finite-length time
  series, and the relationship between wavelet scale and Fourier frequency. New
  statistical significance tests for wavelet power spectra are developed by
  deriving theoretical wavelet spectra for white and red noise processes and
  using these to establish significance levels and confidence intervals. It is
  shown that smoothing in time or scale can be used to increase the confidence
  of the wavelet spectrum. Empirical formulas are given for the effect of
  smoothing on significance levels and confidence intervals. Extensions to
  wavelet analysis such as filtering, the power Hovm\"{o}ller, cross-wavelet
  spectra, and coherence are described. The statistical significance tests are
  used to give a quantitative measure of changes in ENSO variance on
  interdecadal timescales. Using new datasets that extend back to 1871, the
  Ni\~{n}o3 sea surface temperature and the Southern Oscillation index show
  significantly higher power during 1880â1920 and 1960â90, and lower power
  during 1920â60, as well as a possible 15-yr modulation of variance. The
  power Hovm\"{o}ller of sea level pressure shows significant variations in
  2â8-yr wavelet power in both longitude and time.%
  }
  \verb{doi}
  \verb 10.1175/1520-0477(1998)079<0061:APGTWA>2.0.CO;2
  \endverb
  \field{isbn}{0871706881}
  \field{issn}{00030007}
  \field{number}{1}
  \field{pages}{61\bibrangedash 78}
  \field{title}{{A Practical Guide to Wavelet Analysis}}
  \field{volume}{79}
  \field{journaltitle}{Bulletin of the American Meteorological Society}
  \field{year}{1998}
\endentry

\entry{Tan2005b}{book}{}
  \name{author}{3}{}{%
    {{}%
     {Tan}{T.}%
     {Pang-Ning}{P.-N.}%
     {}{}%
     {}{}}%
    {{}%
     {Steinbach}{S.}%
     {Michael}{M.}%
     {}{}%
     {}{}}%
    {{}%
     {Kumar}{K.}%
     {Vipin}{V.}%
     {}{}%
     {}{}}%
  }
  \keyw{- credit risk assessments,credit scoring techniques,single classifiers}
  \strng{namehash}{TPNSMKV1}
  \strng{fullhash}{TPNSMKV1}
  \field{labelalpha}{TSK05}
  \field{sortinit}{T}
  \field{abstract}{%
  -This paper is review of current usage of data mining, machine learning and
  other algorithms for credit risk assessment. We are witnessing importance of
  credit risk assessment, especially after the global economic crisis on 2008.S
  o, it is very important to have a proper way to deal with the credit risk and
  provide powerful and accurate model for credit risk assessment. Many credit
  scoring techniques such as statistical techniques (logistic regression,
  discriminant analysis) or advanced techniques such as neural networks,
  decision trees, genetic algorithm, or support vector machines are used for
  credit risk assessment. Some of them are described in this article with
  theirs advantages/disadvantages. Even with many models and methods, it is
  still hard to say which model is the best or which classifier or which data
  mining technique is the best. Each model depends on particular data set or
  attributes set, so it is very important to develop flexible model which is
  adaptable to every dataset or attribute set.%
  }
  \field{booktitle}{Addison Wesley}
  \field{isbn}{9789604743179}
  \field{pages}{500}
  \field{title}{{Introduction to Data Mining}}
  \field{year}{2005}
\endentry

\entry{Vlachos2006}{article}{}
  \name{author}{4}{}{%
    {{}%
     {Vlachos}{V.}%
     {Michail}{M.}%
     {}{}%
     {}{}}%
    {{}%
     {Hadjieleftheriou}{H.}%
     {Marios}{M.}%
     {}{}%
     {}{}}%
    {{}%
     {Gunopulos}{G.}%
     {Dimitrios}{D.}%
     {}{}%
     {}{}}%
    {{}%
     {Keogh}{K.}%
     {Eamonn}{E.}%
     {}{}%
     {}{}}%
  }
  \keyw{Dynamic time warping,Ensemble index,Longest common subsequence,Motion
  capture,Trajectories}
  \strng{namehash}{VM+1}
  \strng{fullhash}{VMHMGDKE1}
  \field{labelalpha}{Vla+06}
  \field{sortinit}{V}
  \field{abstract}{%
  While most time series data mining research has concentrated on providing
  solutions for a single distance function, in this work we motivate the need
  for an index structure that can support multiple distance measures. Our
  specific area of interest is the efficient retrieval and analysis of similar
  trajectories. Trajectory datasets are very common in environmental
  applications, mobility experiments, and video surveillance and are especially
  important for the discovery of certain biological patterns. Our primary
  similarity measure is based on the longest common subsequence (LCSS) model
  that offers enhanced robustness, particularly for noisy data, which are
  encountered very often in real-world applications. However, our index is able
  to accommodate other distance measures as well, including the ubiquitous
  Euclidean distance and the increasingly popular dynamic time warping (DTW).
  While other researchers have advocated one or other of these similarity
  measures, a major contribution of our work is the ability to support all
  these measures without the need to restructure the index. Our framework
  guarantees no false dismissals and can also be tailored to provide much
  faster response time at the expense of slightly reduced precision/recall. The
  experimental results demonstrate that our index can help speed up the
  computation of expensive similarity measures such as the LCSS and the DTW.%
  }
  \verb{doi}
  \verb 10.1007/s00778-004-0144-2
  \endverb
  \field{isbn}{1-58113-737-0}
  \field{issn}{10668888}
  \field{number}{1}
  \field{pages}{1\bibrangedash 20}
  \field{title}{{Indexing multidimensional time-series}}
  \field{volume}{15}
  \field{journaltitle}{VLDB Journal}
  \field{year}{2006}
\endentry

\entry{Wang2002}{article}{}
  \name{author}{1}{}{%
    {{}%
     {Wang}{W.}%
     {Jung-Ying}{J.-Y.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{WJY1}
  \strng{fullhash}{WJY1}
  \field{labelalpha}{Wan02}
  \field{sortinit}{W}
  \field{abstract}{%
  Recently a new learning method called support vector machines (SVM) has shown
  comparable or better results than neural networks on some applications. In
  this thesis we exploit the possibility of using SVM for three important
  issues of bioinformatics: the prediction of protein secondary structure,
  multi-class protein fold recognition, and the prediction of human signal
  peptide cleavage sites. By using similar data, we demonstrate that SVM can
  easily achieve comparable accuracy as using neural networks. Therefore, in
  the future it is a promising direction to apply SVM on more bioinformatics
  applications.%
  }
  \field{pages}{1\bibrangedash 56}
  \field{title}{{Support Vector Machines ( SVM ) in bioinformatics
  Bioinformatics applications}}
  \verb{url}
  \verb http://www.csie.ntu.edu.tw/~p88012/Bio\_SVM.pdf
  \endverb
  \field{journaltitle}{Bioinformatics}
  \field{year}{2002}
\endentry

\entry{Weinberger2009}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Weinberger}{W.}%
     {K.}{K.}%
     {}{}%
     {}{}}%
    {{}%
     {Saul}{S.}%
     {L.}{L.}%
     {}{}%
     {}{}}%
  }
  \keyw{convex optimization,ing,mahalanobis distance,metric learn-,multi-class
  classification,semi-definite programming,support vector machines}
  \strng{namehash}{WKSL1}
  \strng{fullhash}{WKSL1}
  \field{labelalpha}{WS09}
  \field{sortinit}{W}
  \field{abstract}{%
  The accuracy of k-nearest neighbor (kNN) classification depends significantly
  on the metric used to compute distances between different examples. In this
  paper, we show how to learn a Maha- lanobis distance metric for kNN
  classification from labeled examples. The Mahalanobis metric can equivalently
  be viewed as a global linear transformation of the input space that precedes
  kNN classification using Euclidean distances. In our approach, the metric is
  trained with the goal that the k-nearest neighbors always belong to the same
  class while examples from different classes are separated by a largemargin.
  As in support vectormachines (SVMs), themargin criterion leads to a convex
  optimization based on the hinge loss. Unlike learning in SVMs, however, our
  approach re- quires no modification or extension for problems in multiway (as
  opposed to binary) classification. In our framework, the Mahalanobis distance
  metric is obtained as the solution to a semidefinite program. On several data
  sets of varying size and difficulty, we find that metrics trained in this way
  lead to significant improvements in kNN classification. Sometimes these
  results can be further improved by clustering the training examples and
  learning an individual metric within each cluster. We show how to learn and
  combine these local metrics in a globally integrated manner. Keywords: convex
  optimization, semi-definite programming,Mahalanobis distance,metric learn-
  ing, multi-class classification, support vector machines 1.%
  }
  \field{pages}{207\bibrangedash 244}
  \field{title}{{Distance Metric Learning for Large Margin Nearest Neighbor
  Classification}}
  \verb{url}
  \verb http://www.jmlr.org/papers/volume10/weinberger09a/weinberger09a.pdf
  \endverb
  \field{volume}{10}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley D
  \verb esktop/Downloaded/Weinberger, Saul - 2009 - Distance Metric Learning fo
  \verb r Large Margin Nearest Neighbor Classification.pdf:pdf
  \endverb
  \field{journaltitle}{Journal of Machine Learning Research}
  \field{year}{2009}
\endentry

\entry{Xi2006a}{inproceedings}{}
  \name{author}{5}{}{%
    {{}%
     {Xi}{X.}%
     {Xiaopeng}{X.}%
     {}{}%
     {}{}}%
    {{}%
     {Keogh}{K.}%
     {Eamonn}{E.}%
     {}{}%
     {}{}}%
    {{}%
     {Shelton}{S.}%
     {Christian}{C.}%
     {}{}%
     {}{}}%
    {{}%
     {Wei}{W.}%
     {Li}{L.}%
     {}{}%
     {}{}}%
    {{}%
     {Ratanamahatana}{R.}%
     {Chotirat~Ann}{C.~A.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{XX+1}
  \strng{fullhash}{XXKESCWLRCA1}
  \field{labelalpha}{Xi+06}
  \field{sortinit}{X}
  \field{abstract}{%
  Many algorithms have been proposed for the problem of time series
  classification. However, it is clear that one-nearest-neighbor with Dynamic
  Time Warping (DTW) distance is exceptionally difficult to beat. This approach
  has one weakness, however; it is computationally too demanding for many
  realtime applications. One way to mitigate this problem is to speed up the
  DTW calculations. Nonetheless, there is a limit to how much this can help. In
  this work, we propose an additional technique, numerosity reduction, to speed
  up one-nearest-neighbor DTW. While the idea of numerosity reduction for
  nearest-neighbor classifiers has a long history, we show here that we can
  leverage off an original observation about the relationship between dataset
  size and DTW constraints to produce an extremely compact dataset with little
  or no loss in accuracy. We test our ideas with a comprehensive set of
  experiments, and show that it can efficiently produce extremely fast accurate
  classifiers.%
  }
  \field{booktitle}{Proceedings of the 23rd international conference on Machine
  learning (ICML)}
  \verb{doi}
  \verb 10.1145/1143844.1143974
  \endverb
  \field{isbn}{1595933832}
  \field{pages}{1033\bibrangedash 1040}
  \field{title}{{Fast time series classification using numerosity reduction}}
  \verb{url}
  \verb http://dl.acm.org/citation.cfm?id=1143974
  \endverb
  \field{year}{2006}
\endentry

\entry{Yin2008}{inproceedings}{}
  \name{author}{2}{}{%
    {{}%
     {Yin}{Y.}%
     {J.}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Gaber}{G.}%
     {M.}{M.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{YJGM1}
  \strng{fullhash}{YJGM1}
  \field{labelalpha}{YG08}
  \field{sortinit}{Y}
  \field{abstract}{%
  Event detection is a critical task in sensor networks, especially for
  environmental monitoring applications. Traditional solutions to event
  detection are based on analyzing one-shot data points, which might incur a
  high false alarm rate because sensor data is inherently unreliable and noisy.
  To address this issue, we propose a novel Distributed Single-pass Incremental
  Clustering (DSIC) technique to cluster the time series obtained at sensor
  nodes based on their underlying trends. In order to achieve scalability and
  energy-efficiency, our DSIC technique uses a hierarchical structure of sensor
  networks as the underlying infrastructure. The algorithm first compresses the
  time series produced at individual sensor nodes into a compact representation
  using Haar wavelet transform, and then, based on dynamic time warping
  distances, hierarchically groups the approximate time series into a global
  clustering model in an incremental manner. Experimental results on both real
  data and synthetic data demonstrate that our DSIC algorithm is accurate,
  energy-efficient and robust with respect to network topology changes.%
  }
  \field{booktitle}{ICDM}
  \verb{doi}
  \verb 10.1109/ICDM.2008.58
  \endverb
  \field{isbn}{9780769535029}
  \field{issn}{15504786}
  \field{title}{{Clustering distributed time series in sensor networks}}
  \field{year}{2008}
\endentry

\entry{Yang1999}{incollection}{}
  \name{author}{2}{}{%
    {{}%
     {Yang}{Y.}%
     {Yiming}{Y.}%
     {}{}%
     {}{}}%
    {{}%
     {Liu}{L.}%
     {Xin}{X.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{YYLX1}
  \strng{fullhash}{YYLX1}
  \field{labelalpha}{YL99}
  \field{sortinit}{Y}
  \field{abstract}{%
  This paper reports a controlled study with statistical signifi cance tests on
  five text categorization methods: the Support Vector Machines (SVM), a
  kNearest Neighbor (kNN) clas sifier, a neural network (NNet) approach, the
  Linear Least squares Fit (LLSF) mapping and a Naive Bayes (NB) classi fier.
  We focus on the robustness of these methods in dealing with a skewed category
  distribution, and their performance as function of the trainingset category
  frequency. Our re sults show that SVM, kNN and LLSF significantly outper form
  NNet and NB when the number of positive training instances per category are
  small (less than ten), and that all the methods perform comparably when the
  categories are sufficiently common (over 300 instances).%
  }
  \field{booktitle}{Proceedings of the 22nd annual international ACM SIGIR
  conference on Research and development in information retrieval SIGIR 99}
  \verb{doi}
  \verb 10.1145/312624.312647
  \endverb
  \field{isbn}{1581130961}
  \field{pages}{42\bibrangedash 49}
  \field{title}{{A re-examination of text categorization methods}}
  \field{year}{1999}
\endentry

\entry{Dreyfus2006}{book}{}
  \name{author}{1}{}{%
    {{}%
     {{G. Dreyfus, J.-M. Martinez, M. Samuelides M. B. Gordon, F. Badran}}{G.}%
     {S.~Thiria}{S.~T.}%
     {}{}%
     {}{}}%
  }
  \keyw{Bio-ing\'{e}nierie,Machine \`{a} Vecteurs
  Supports,Pr\'{e}vision,Reconaissance de formes,Robotique et commande de
  processus,R\'{e}seaux de neurones,cartes topologiques,data mining}
  \strng{namehash}{GST1}
  \strng{fullhash}{GST1}
  \field{labelalpha}{{G. }06}
  \field{sortinit}{{G}}
  \field{abstract}{%
  En une vingtaine dâann\'{e}es, lâapprentissage artificiel est devenu une
  branche majeure des math\'{e}matiques appliqu\'{e}es, \`{a} lâintersection
  des statistiques et de lâintelligence artificielle. Son objectif est de
  r\'{e}aliser des mod\`{e}les qui apprennent Â« par lâexemple Â» : il
  sâappuie sur des donn\'{e}es num\'{e}riques (r\'{e}sultats de mesures ou de
  simulations), contrairement aux mod\`{e}les Â« de connaissances Â» qui
  sâappuient sur des \'{e}quations issues des premiers principes de la
  physique, de la chimie, de la biologie, de lâ\'{e}conomie, etc.
  Lâapprentis- sage statistique est dâune grande utilit\'{e} lorsque lâon
  cherche \`{a} mod\'{e}liser des processus complexes, souvent non
  lin\'{e}aires, pour lesquels les connaissances th\'{e}oriques sont trop
  impr\'{e}cises pour permettre des pr\'{e}dictions pr\'{e}cises. Ses domaines
  dâapplications sont multiples : fouille de donn\'{e}es, bio-informatique,
  g\'{e}nie des proc\'{e}d\'{e}s, aide au diagnostic m\'{e}dical,
  t\'{e}l\'{e}communications, interface cerveau-machines, et bien dâautres.
  Cet ouvrage refl\`{e}te en partie lâ\'{e}volution de cette discipline,
  depuis ses balbutiements au d\'{e}but des ann\'{e}es 1980, jusquâ\`{a} sa
  situation actuelle ; il nâa pas du tout la pr\'{e}tention de faire un
  point, m\^{e}me partiel, sur lâensemble des d\'{e}veloppements pass\'{e}s
  et actuels, mais plut\^{o}t dâinsister sur les principes et sur les
  m\'{e}thodes \'{e}prouv\'{e}s, dont les bases scientifiques sont s\^{u}res.
  Dans un domaine sans cesse parcouru de modes multiples et
  \'{e}ph\'{e}m\`{e}res, il est utile, pour qui cherche \`{a} acqu\'{e}rir les
  connaissances et principes de base, dâinsister sur les aspects p\'{e}rennes
  du domaine. Cet ouvrage fait suite \`{a} R\'{e}seaux de neurones,
  m\'{e}thodologies et applications, des m\^{e}mes auteurs, paru en 2000,
  r\'{e}\'{e}dit\'{e} en 2004, chez le m\^{e}me \'{e}diteur, puis publi\'{e} en
  traduction anglaise chez Springer. Consacr\'{e} essentiellement aux
  r\'{e}seaux de neurones et aux cartes auto-adaptatives, il a largement
  contribu\'{e} \`{a} populariser ces techniques et \`{a} convaincre leurs
  utilisateurs quâil est possible dâobtenir des r\'{e}sultats remarquables,
  \`{a} condition de mettre en \oe uvre une m\'{e}thodologie de conception
  rigoureuse, scientifique- ment fond\'{e}e, dans un domaine o\`{u}
  lâempirisme a longtemps tenu lieu de m\'{e}thode. Tout en restant
  fid\`{e}le \`{a} lâesprit de cet ouvrage, combinant fondements
  math\'{e}matiques et m\'{e}thodologie de mise en \oe uvre, les auteurs ont
  \'{e}largi le champ de la pr\'{e}sentation, afin de permettre au lecteur
  dâaborder dâautres m\'{e}thodes dâapprentissage statistique que celles
  qui sont directement d\'{e}crites dans cet ouvrage. En effet, les succ\`{e}s
  de lâapprentissage dans un grand nombre de domaines ont pouss\'{e} au
  d\'{e}veloppement de tr\`{e}s nombreuses variantes, souvent destin\'{e}es
  \`{a} r\'{e}pondre efficacement aux exigences de telle ou telle classe
  dâapplications. Toutes ces variantes ont n\'{e}anmoins des bases
  th\'{e}oriques et des aspects m\'{e}thodolo- giques communs, quâil est
  important dâavoir pr\'{e}sents \`{a} lâesprit. Le terme
  dâapprentissage, comme celui de r\'{e}seau de neurones, \'{e}voque
  \'{e}videmment le fonctionnement du cerveau. Il ne faut pourtant pas
  sâattendre \`{a} trouver ici dâexplications sur les m\'{e}canismes de
  traitement des informations dans les syst\`{e}mes nerveux ; ces derniers sont
  dâune grande complexit\'{e}, r\'{e}sultant de processus \'{e}lectriques et
  chimiques subtils, encore mal compris en d\'{e}pit de la grande quantit\'{e}
  de donn\'{e}es exp\'{e}rimentales disponibles. Si les m\'{e}thodes
  dâapprentissage statistique peuvent \^{e}tre dâune grande utilit\'{e}
  pour cr\'{e}er des mod\`{e}les empiriques de telle ou telle fonction
  r\'{e}alis\'{e}e par le syst\`{e}me nerveux, celles qui sont d\'{e}crites
  dans cet ouvrage nâont aucunement la pr\'{e}tention dâimiter, m\^{e}me
  vaguement, le fonctionne- ment du cerveau. Lâapprentissage artificiel,
  notamment statistique, permettra-t-il un jour de donner aux ordinateurs des
  capacit\'{e}s analogues \`{a} celles des \^{e}tres humains ? Se
  rapprochera-t-on de cet objectif en perfectionnant les techniques actuelles
  dâapprentissage, ou bien des approches radicalement nouvelles sont-elles
  indispensables ? Faut-il sâinspirer de ce que lâon sait, ou croit savoir,
  sur le fonctionnement du cerveau ? Ces questions font lâobjet de d\'{e}bats
  passionn\'{e}s, et passionnants, au sein de la communaut\'{e} scientifique :
  on nâen trouvera pas les r\'{e}ponses ici.%
  }
  \field{edition}{Eyrolles}
  \field{isbn}{9782212114645}
  \field{pages}{471}
  \field{title}{{Apprentissage Apprentissage statistique}}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley D
  \verb esktop/Downloaded/G. Dreyfus, J.-M. Martinez, M. Samuelides M. B. Gordo
  \verb n, F. Badran - 2006 - Apprentissage Apprentissage statistique.pdf:pdf
  \endverb
  \field{year}{2006}
\endentry

\entry{WienerN1942}{report}{}
  \name{author}{1}{}{%
    {{}%
     {{Wiener N}}{W.}%
     {}{}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{W1}
  \strng{fullhash}{W1}
  \field{labelalpha}{{Wie}42}
  \field{sortinit}{{W}}
  \field{title}{{Extrapolation, Interpolation \& Smoothing of Stationary Time
  Series - With Engineering Applications}}
  \list{institution}{1}{%
    {Report of the Services 19, Research Project DIC-6037 MIT}%
  }
  \field{type}{techreport}
  \field{year}{1942}
\endentry

\lossort
\endlossort

\endinput
