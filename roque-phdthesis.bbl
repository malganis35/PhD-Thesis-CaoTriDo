% $ biblatex auxiliary file $
% $ biblatex version 2.5 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated as
% required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup

\entry{Aizerman1964}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Aizerman}{A.}%
     {M.}{M.}%
     {}{}%
     {}{}}%
    {{}%
     {Braverman}{B.}%
     {E.}{E.}%
     {}{}%
     {}{}}%
    {{}%
     {Rozonoer}{R.}%
     {L.}{L.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{AMBERL1}
  \strng{fullhash}{AMBERL1}
  \field{labelalpha}{ABR64}
  \field{sortinit}{A}
  \field{abstract}{%
  Introduction of kernels%
  }
  \field{pages}{821\bibrangedash 837}
  \field{title}{{Theoretical foundations of the potential function method in
  pattern recognition learning}}
  \field{volume}{25}
  \field{journaltitle}{Automation and Remote Control}
  \field{year}{1964}
\endentry

\entry{Boser1992}{inproceedings}{}
  \name{author}{3}{}{%
    {{}%
     {Boser}{B.}%
     {Bernhard~E.}{B.~E.}%
     {}{}%
     {}{}}%
    {{}%
     {Guyon}{G.}%
     {Isabelle~M.}{I.~M.}%
     {}{}%
     {}{}}%
    {{}%
     {Vapnik}{V.}%
     {Vladimir~N.}{V.~N.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{BBEGIMVVN1}
  \strng{fullhash}{BBEGIMVVN1}
  \field{labelalpha}{BGV92}
  \field{sortinit}{B}
  \field{abstract}{%
  A training algorithm that maximizes the margin between the training patterns
  and the decision boundary is presented. The technique is applicable to a wide
  variety of classifiaction functions, including Perceptrons, polynomials, and
  Radial Basis Functions. The effective number of parameters is adjusted
  automatically to match the complexity of the problem. The solution is
  expressed as a linear combination of supporting patterns. These are the
  subset of training patterns that are closest to the decision boundary. Bounds
  on the generalization performance based on the leave-one-out method and the
  VC-dimension are given. Experimental results on optical character recognition
  problems demonstrate the good generalization obtained when compared with
  other learning algorithms. 1 INTRODUCTION Good generalization performance of
  pattern classifiers is achieved when the capacity of the classification
  function is matched to the size of the training set. Classifiers with a large
  numb...%
  }
  \field{booktitle}{Proceedings of the 5th Annual ACM Workshop on Computational
  Learning Theory}
  \verb{doi}
  \verb 10.1.1.21.3818
  \endverb
  \field{isbn}{089791497X}
  \field{issn}{0-89791-497-X}
  \field{pages}{144\bibrangedash 152}
  \field{title}{{A Training Algorithm for Optimal Margin Classifiers}}
  \verb{url}
  \verb http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.21.3818
  \endverb
  \field{year}{1992}
\endentry

\entry{Chatfield2004}{book}{}
  \name{author}{1}{}{%
    {{}%
     {Chatfield}{C.}%
     {Christopher}{C.}%
     {}{}%
     {}{}}%
  }
  \keyw{Time-series analysis.}
  \strng{namehash}{CC1}
  \strng{fullhash}{CC1}
  \field{labelalpha}{Cha04}
  \field{sortinit}{C}
  \field{abstract}{%
  "Since 1975, The Analysis of Time Series: An Introduction has introduced
  legions of statistics students and researchers to the theory and practice of
  time series analysis. The sixth edition provides an accessible, comprehensive
  introduction to the theory and practice of time series analysis. The
  treatment covers a wide range of topics, including ARIMA probability models,
  forecasting methods, spectral analysis, linear systems, state-space models,
  and the Kalman filter. It also addresses nonlinear, multivariate, and
  long-memory models. The author has carefully updated each chapter, added new
  discussions, incorporated new datasets, and made those datasets available at
  www.crcpress.com."--BOOK JACKET.%
  }
  \field{booktitle}{Texts in statistical science}
  \field{isbn}{1584883170}
  \field{pages}{xiii, 333 p.}
  \field{title}{{The analysis of time series : an introduction}}
  \field{year}{2004}
\endentry

\entry{Cortes1995}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Cortes}{C.}%
     {Corinna}{C.}%
     {}{}%
     {}{}}%
    {{}%
     {Vapnik}{V.}%
     {Vladimir}{V.}%
     {}{}%
     {}{}}%
  }
  \keyw{efficient learning algorithms,neural networks,pattern
  recognition,polynomial classifiers,radial basis function classifiers}
  \strng{namehash}{CCVV1}
  \strng{fullhash}{CCVV1}
  \field{labelalpha}{CV95}
  \field{sortinit}{C}
  \field{abstract}{%
  The support-vector network is a new leaming machine for two-group
  classification problems. The machine conceptually implements the following
  idea: input vectors are non-linearly mapped to a very high- dimension feature
  space. In this feature space a linear decision surface is constructed.
  Special properties of the decision surface ensures high generalization
  ability of the learning machine. The idea behind the support-vector network
  was previously implemented for the restricted case where the training data
  can be separated without errors. We here extend this result to non-separable
  training data. High generalization ability of support-vector networks
  utilizing polynomial input transformations is demon- strated. We also compare
  the performance of the support-vector network to various classical learning
  algorithms that all took part in a benchmark study of Optical Character
  Recognition.%
  }
  \verb{doi}
  \verb 10.1007/BF00994018
  \endverb
  \verb{eprint}
  \verb arXiv:1011.1669v3
  \endverb
  \field{isbn}{0885-6125}
  \field{issn}{08856125}
  \field{number}{3}
  \field{pages}{273\bibrangedash 297}
  \field{title}{{Support-vector networks}}
  \field{volume}{20}
  \field{journaltitle}{Machine Learning}
  \field{eprinttype}{arXiv}
  \field{year}{1995}
\endentry

\entry{Campbell2011}{book}{}
  \name{author}{2}{}{%
    {{}%
     {Campbell}{C.}%
     {Colin}{C.}%
     {}{}%
     {}{}}%
    {{}%
     {Ying}{Y.}%
     {Yiming}{Y.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{CCYY1}
  \strng{fullhash}{CCYY1}
  \field{labelalpha}{CY11}
  \field{sortinit}{C}
  \field{booktitle}{Synthesis Lectures on Artificial Intelligence and Machine
  Learning}
  \verb{doi}
  \verb 10.2200/S00324ED1V01Y201102AIM010
  \endverb
  \field{isbn}{9781608456161}
  \field{issn}{1939-4608}
  \field{number}{1}
  \field{pages}{1\bibrangedash 95}
  \field{title}{{Learning with Support Vector Machines}}
  \verb{url}
  \verb http://www.morganclaypool.com/doi/abs/10.2200/S00324ED1V01Y201102AIM010
  \endverb
  \field{volume}{5}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley D
  \verb esktop/Downloaded/Campbell, Ying - 2011 - Learning with Support Vector
  \verb Machines.pdf:pdf
  \endverb
  \field{month}{02}
  \field{year}{2011}
\endentry

\entry{Ding2008}{article}{}
  \name{author}{5}{}{%
    {{}%
     {Ding}{D.}%
     {Hui}{H.}%
     {}{}%
     {}{}}%
    {{}%
     {Trajcevski}{T.}%
     {Goce}{G.}%
     {}{}%
     {}{}}%
    {{}%
     {Scheuermann}{S.}%
     {Peter}{P.}%
     {}{}%
     {}{}}%
    {{}%
     {Wang}{W.}%
     {Xiaoyue}{X.}%
     {}{}%
     {}{}}%
    {{}%
     {Keogh}{K.}%
     {Eamonn}{E.}%
     {}{}%
     {}{}}%
  }
  \list{publisher}{1}{%
    {VLDB Endowment}%
  }
  \strng{namehash}{DH+1}
  \strng{fullhash}{DHTGSPWXKE1}
  \field{labelalpha}{Din+08}
  \field{sortinit}{D}
  \field{abstract}{%
  The last decade has witnessed a tremendous growths of interests in
  applications that deal with querying and mining of time series data. Numerous
  representation methods for dimensionality reduction and similarity measures
  geared towards time series have been introduced. Each individual work
  introducing a particular method has made specific claims and, aside from the
  occasional theoretical justifications, provided quantitative experimental
  observations. However, for the most part, the comparative aspects of these
  experiments were too narrowly focused on demonstrating the benefits of the
  proposed methods over some of the previously introduced ones. In order to
  provide a comprehensive validation, we conducted an extensive set of time
  series experiments re-implementing 8 different representation methods and 9
  similarity measures and their variants, and testing their effectiveness on 38
  time series data sets from a wide variety of application domains. In this
  paper, we give an overview of these different techniques and present our
  comparative experimental findings regarding their effectiveness. Our
  experiments have provided both a unified validation of some of the existing
  achievements, and in some cases, suggested that certain claims in the
  literature may be unduly optimistic. 1.%
  }
  \verb{doi}
  \verb 10.1145/1454159.1454226
  \endverb
  \verb{eprint}
  \verb 1012.2789v1
  \endverb
  \field{isbn}{0000000000000}
  \field{issn}{2150-8097}
  \field{number}{2}
  \field{pages}{1542\bibrangedash 1552}
  \field{title}{{Querying and Mining of Time Series Data : Experimental
  Comparison of Representations and Distance Measures}}
  \verb{url}
  \verb http://dl.acm.org/citation.cfm?id=1454226
  \endverb
  \field{volume}{1}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley D
  \verb esktop/Downloaded/Ding, Trajcevski, Scheuermann - 2008 - Querying and M
  \verb ining of Time Series Data Experimental Comparison of Representations an
  \verb d Distance.pdf:pdf
  \endverb
  \field{journaltitle}{Proceedings of the VLDB Endowment}
  \field{eprinttype}{arXiv}
  \field{year}{2008}
\endentry

\entry{Najmeddine2012}{inproceedings}{}
  \name{author}{4}{}{%
    {{}%
     {Najmeddine}{N.}%
     {H.}{H.}%
     {}{}%
     {}{}}%
    {{}%
     {Jay}{J.}%
     {A.}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Marechal}{M.}%
     {P.}{P.}%
     {}{}%
     {}{}}%
    {{}%
     {Mari\'{e}}{M.}%
     {S.}{S.}%
     {}{}%
     {}{}}%
  }
  \keyw{Data mining,INCAS.,Time series,diagnosis and decision
  support,sensors,similarity measures}
  \strng{namehash}{NH+1}
  \strng{fullhash}{NHJAMPMS1}
  \field{labelalpha}{Naj+12}
  \field{sortinit}{N}
  \field{booktitle}{RFIA}
  \field{isbn}{9782953951523}
  \field{title}{{Mesures de similarit\'{e} pour l’aide \`{a} l’analyse des
  donn\'{e}es \'{e}nerg\'{e}tiques de b\^{a}timents}}
  \verb{url}
  \verb https://hal-cea.archives-ouvertes.fr/file/index/docid/661016/filename/a
  \verb rticle53\_modif.pdf
  \endverb
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley D
  \verb esktop/Downloaded/Najmeddine et al. - 2012 - Mesures de similarit\'{e}
  \verb pour l’aide \`{a} l’analyse des donn\'{e}es \'{e}nerg\'{e}tiques de
  \verb  b\^{a}timents.pdf:pdf
  \endverb
  \field{year}{2012}
\endentry

\entry{Nguyen2012}{inproceedings}{}
  \name{author}{5}{}{%
    {{}%
     {Nguyen}{N.}%
     {L.}{L.}%
     {}{}%
     {}{}}%
    {{}%
     {Wu}{W.}%
     {P.}{P.}%
     {}{}%
     {}{}}%
    {{}%
     {Chan}{C.}%
     {W.}{W.}%
     {}{}%
     {}{}}%
    {{}%
     {Peng}{P.}%
     {W.}{W.}%
     {}{}%
     {}{}}%
    {{}%
     {Zhang}{Z.}%
     {Y.}{Y.}%
     {}{}%
     {}{}}%
  }
  \keyw{sentiment analysis,sentiment prediction,social network analysis}
  \strng{namehash}{NL+1}
  \strng{fullhash}{NLWPCWPWZY1}
  \field{labelalpha}{Ngu+12}
  \field{sortinit}{N}
  \field{abstract}{%
  More and more people express their opinions on social media such as Facebook
  and Twitter. Predictive analysis on social media time-series allows the
  stake-holders to leverage this immediate, accessible and vast reachable
  communication channel to react and proact against the public opinion. In
  particular, understanding and predicting the sentiment change of the public
  opinions will allow business and government agencies to react against
  negative sentiment and design strategies such as dispelling rumors and post
  balanced messages to revert the public opinion. In this paper, we present a
  strategy of building statistical models from the social media dynamics to
  predict collective sentiment dynamics. We model the collective sentiment
  change without delving into micro analysis of individual tweets or users and
  their corresponding low level network structures. Experiments on large-scale
  Twitter data show that the model can achieve above 85\% accuracy on
  directional sentiment prediction.%
  }
  \field{booktitle}{WISDOM}
  \verb{doi}
  \verb 10.1145/2346676.2346682
  \endverb
  \field{isbn}{9781450315432}
  \field{title}{{Predicting collective sentiment dynamics from time-series
  social media}}
  \field{year}{2012}
\endentry

\entry{Duda1973}{book}{}
  \name{author}{2}{}{%
    {{}%
     {{O Duda}}{O.}%
     {Richard}{R.}%
     {}{}%
     {}{}}%
    {{}%
     {{E Hart}}{E.}%
     {Peter}{P.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{OREP1}
  \strng{fullhash}{OREP1}
  \field{labelalpha}{OE73}
  \field{sortinit}{O}
  \field{abstract}{%
  Classic book on pattern recognition. Interesting points: 1) p. 66, and p.
  114: Mentions the problems with dimensionality curse. 2) p. 243-246: Mentions
  Multidimensional scaling (MDS), Karhunen-Loeve and dimensionality reduction.
  Also, has the spiral data-set as a sample. 3) p. 333: mentions
  SVD/eigenvalues for linear fitting.%
  }
  \field{booktitle}{Leonardo}
  \verb{doi}
  \verb 10.2307/1573081
  \endverb
  \field{isbn}{0471223611}
  \field{issn}{0024094X}
  \field{pages}{482}
  \field{title}{{Pattern Classification and Scene Analysis}}
  \verb{url}
  \verb http://www.jstor.org/stable/1573081?origin=crossref
  \endverb
  \field{volume}{7}
  \field{year}{1973}
\endentry

\entry{Schlkopf2013}{book}{}
  \name{author}{2}{}{%
    {{}%
     {Schlkopf}{S.}%
     {Bernhard}{B.}%
     {}{}%
     {}{}}%
    {{}%
     {Smola}{S.}%
     {Alexander~J.}{A.~J.}%
     {}{}%
     {}{}}%
  }
  \keyw{icle}
  \strng{namehash}{SBSAJ1}
  \strng{fullhash}{SBSAJ1}
  \field{labelalpha}{SS13}
  \field{sortinit}{S}
  \field{abstract}{%
  Predicting the binding mode of flexible polypeptides to proteins is an
  important task that falls outside the domain of applicability of most small
  molecule and protein−protein docking tools. Here, we test the small
  molecule flexible ligand docking program Glide on a set of 19
  non-$\alpha$-helical peptides and systematically improve pose prediction
  accuracy by enhancing Glide sampling for flexible polypeptides. In addition,
  scoring of the poses was improved by post-processing with physics-based
  implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10
  scoring poses as a metric, the success rate (RMSD ≤ 2.0 \AA for the
  interface backbone atoms) increased from 21\% with default Glide SP settings
  to 58\% with the enhanced peptide sampling and scoring protocol in the case
  of redocking to the native protein structure. This approaches the accuracy of
  the recently developed Rosetta FlexPepDock method (63\% success for these 19
  peptides) while being over 100 times faster. Cross-docking was performed for
  a subset of cases where an unbound receptor structure was available, and in
  that case, 40\% of peptides were docked successfully. We analyze the results
  and find that the optimized polypeptide protocol is most accurate for
  extended peptides of limited size and number of formal charges, defining a
  domain of applicability for this approach.%
  }
  \field{booktitle}{Journal of Chemical Information and Modeling}
  \verb{doi}
  \verb 10.1017/CBO9781107415324.004
  \endverb
  \verb{eprint}
  \verb arXiv:1011.1669v3
  \endverb
  \field{isbn}{9788578110796}
  \field{issn}{1098-6596}
  \field{pages}{1689\bibrangedash 1699}
  \field{title}{{Learning with Kernels}}
  \field{volume}{53}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/Google Drive/Th\`{e}se/Bibliographie/L
  \verb ivre/Schokopf, Smola-Learning with Kernels.pdf:pdf
  \endverb
  \field{eprinttype}{arXiv}
  \field{year}{2013}
\endentry

\entry{Tan2005b}{book}{}
  \name{author}{3}{}{%
    {{}%
     {Tan}{T.}%
     {Pang-Ning}{P.-N.}%
     {}{}%
     {}{}}%
    {{}%
     {Steinbach}{S.}%
     {Michael}{M.}%
     {}{}%
     {}{}}%
    {{}%
     {Kumar}{K.}%
     {Vipin}{V.}%
     {}{}%
     {}{}}%
  }
  \keyw{- credit risk assessments,credit scoring techniques,single classifiers}
  \strng{namehash}{TPNSMKV1}
  \strng{fullhash}{TPNSMKV1}
  \field{labelalpha}{TSK05}
  \field{sortinit}{T}
  \field{abstract}{%
  -This paper is review of current usage of data mining, machine learning and
  other algorithms for credit risk assessment. We are witnessing importance of
  credit risk assessment, especially after the global economic crisis on 2008.S
  o, it is very important to have a proper way to deal with the credit risk and
  provide powerful and accurate model for credit risk assessment. Many credit
  scoring techniques such as statistical techniques (logistic regression,
  discriminant analysis) or advanced techniques such as neural networks,
  decision trees, genetic algorithm, or support vector machines are used for
  credit risk assessment. Some of them are described in this article with
  theirs advantages/disadvantages. Even with many models and methods, it is
  still hard to say which model is the best or which classifier or which data
  mining technique is the best. Each model depends on particular data set or
  attributes set, so it is very important to develop flexible model which is
  adaptable to every dataset or attribute set.%
  }
  \field{booktitle}{Addison Wesley}
  \field{isbn}{9789604743179}
  \field{pages}{500}
  \field{title}{{Introduction to Data Mining}}
  \field{year}{2005}
\endentry

\entry{Xi2006a}{inproceedings}{}
  \name{author}{5}{}{%
    {{}%
     {Xi}{X.}%
     {Xiaopeng}{X.}%
     {}{}%
     {}{}}%
    {{}%
     {Keogh}{K.}%
     {Eamonn}{E.}%
     {}{}%
     {}{}}%
    {{}%
     {Shelton}{S.}%
     {Christian}{C.}%
     {}{}%
     {}{}}%
    {{}%
     {Wei}{W.}%
     {Li}{L.}%
     {}{}%
     {}{}}%
    {{}%
     {Ratanamahatana}{R.}%
     {Chotirat~Ann}{C.~A.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{XX+1}
  \strng{fullhash}{XXKESCWLRCA1}
  \field{labelalpha}{Xi+06}
  \field{sortinit}{X}
  \field{abstract}{%
  Many algorithms have been proposed for the problem of time series
  classification. However, it is clear that one-nearest-neighbor with Dynamic
  Time Warping (DTW) distance is exceptionally difficult to beat. This approach
  has one weakness, however; it is computationally too demanding for many
  realtime applications. One way to mitigate this problem is to speed up the
  DTW calculations. Nonetheless, there is a limit to how much this can help. In
  this work, we propose an additional technique, numerosity reduction, to speed
  up one-nearest-neighbor DTW. While the idea of numerosity reduction for
  nearest-neighbor classifiers has a long history, we show here that we can
  leverage off an original observation about the relationship between dataset
  size and DTW constraints to produce an extremely compact dataset with little
  or no loss in accuracy. We test our ideas with a comprehensive set of
  experiments, and show that it can efficiently produce extremely fast accurate
  classifiers.%
  }
  \field{booktitle}{Proceedings of the 23rd international conference on Machine
  learning (ICML)}
  \verb{doi}
  \verb 10.1145/1143844.1143974
  \endverb
  \field{isbn}{1595933832}
  \field{pages}{1033\bibrangedash 1040}
  \field{title}{{Fast time series classification using numerosity reduction}}
  \verb{url}
  \verb http://dl.acm.org/citation.cfm?id=1143974
  \endverb
  \field{year}{2006}
\endentry

\entry{Yin2008}{inproceedings}{}
  \name{author}{2}{}{%
    {{}%
     {Yin}{Y.}%
     {J.}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Gaber}{G.}%
     {M.}{M.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{YJGM1}
  \strng{fullhash}{YJGM1}
  \field{labelalpha}{YG08}
  \field{sortinit}{Y}
  \field{abstract}{%
  Event detection is a critical task in sensor networks, especially for
  environmental monitoring applications. Traditional solutions to event
  detection are based on analyzing one-shot data points, which might incur a
  high false alarm rate because sensor data is inherently unreliable and noisy.
  To address this issue, we propose a novel Distributed Single-pass Incremental
  Clustering (DSIC) technique to cluster the time series obtained at sensor
  nodes based on their underlying trends. In order to achieve scalability and
  energy-efficiency, our DSIC technique uses a hierarchical structure of sensor
  networks as the underlying infrastructure. The algorithm first compresses the
  time series produced at individual sensor nodes into a compact representation
  using Haar wavelet transform, and then, based on dynamic time warping
  distances, hierarchically groups the approximate time series into a global
  clustering model in an incremental manner. Experimental results on both real
  data and synthetic data demonstrate that our DSIC algorithm is accurate,
  energy-efficient and robust with respect to network topology changes.%
  }
  \field{booktitle}{ICDM}
  \verb{doi}
  \verb 10.1109/ICDM.2008.58
  \endverb
  \field{isbn}{9780769535029}
  \field{issn}{15504786}
  \field{title}{{Clustering distributed time series in sensor networks}}
  \field{year}{2008}
\endentry

\entry{Dreyfus2006}{book}{}
  \name{author}{1}{}{%
    {{}%
     {{G. Dreyfus, J.-M. Martinez, M. Samuelides M. B. Gordon, F. Badran}}{G.}%
     {S.~Thiria}{S.~T.}%
     {}{}%
     {}{}}%
  }
  \keyw{Bio-ing\'{e}nierie,Machine \`{a} Vecteurs
  Supports,Pr\'{e}vision,Reconaissance de formes,Robotique et commande de
  processus,R\'{e}seaux de neurones,cartes topologiques,data mining}
  \strng{namehash}{GST1}
  \strng{fullhash}{GST1}
  \field{labelalpha}{{G. }06}
  \field{sortinit}{{G}}
  \field{abstract}{%
  En une vingtaine d’ann\'{e}es, l’apprentissage artificiel est devenu une
  branche majeure des math\'{e}matiques appliqu\'{e}es, \`{a} l’intersection
  des statistiques et de l’intelligence artificielle. Son objectif est de
  r\'{e}aliser des mod\`{e}les qui apprennent « par l’exemple » : il
  s’appuie sur des donn\'{e}es num\'{e}riques (r\'{e}sultats de mesures ou de
  simulations), contrairement aux mod\`{e}les « de connaissances » qui
  s’appuient sur des \'{e}quations issues des premiers principes de la
  physique, de la chimie, de la biologie, de l’\'{e}conomie, etc.
  L’apprentis- sage statistique est d’une grande utilit\'{e} lorsque l’on
  cherche \`{a} mod\'{e}liser des processus complexes, souvent non
  lin\'{e}aires, pour lesquels les connaissances th\'{e}oriques sont trop
  impr\'{e}cises pour permettre des pr\'{e}dictions pr\'{e}cises. Ses domaines
  d’applications sont multiples : fouille de donn\'{e}es, bio-informatique,
  g\'{e}nie des proc\'{e}d\'{e}s, aide au diagnostic m\'{e}dical,
  t\'{e}l\'{e}communications, interface cerveau-machines, et bien d’autres.
  Cet ouvrage refl\`{e}te en partie l’\'{e}volution de cette discipline,
  depuis ses balbutiements au d\'{e}but des ann\'{e}es 1980, jusqu’\`{a} sa
  situation actuelle ; il n’a pas du tout la pr\'{e}tention de faire un
  point, m\^{e}me partiel, sur l’ensemble des d\'{e}veloppements pass\'{e}s
  et actuels, mais plut\^{o}t d’insister sur les principes et sur les
  m\'{e}thodes \'{e}prouv\'{e}s, dont les bases scientifiques sont s\^{u}res.
  Dans un domaine sans cesse parcouru de modes multiples et
  \'{e}ph\'{e}m\`{e}res, il est utile, pour qui cherche \`{a} acqu\'{e}rir les
  connaissances et principes de base, d’insister sur les aspects p\'{e}rennes
  du domaine. Cet ouvrage fait suite \`{a} R\'{e}seaux de neurones,
  m\'{e}thodologies et applications, des m\^{e}mes auteurs, paru en 2000,
  r\'{e}\'{e}dit\'{e} en 2004, chez le m\^{e}me \'{e}diteur, puis publi\'{e} en
  traduction anglaise chez Springer. Consacr\'{e} essentiellement aux
  r\'{e}seaux de neurones et aux cartes auto-adaptatives, il a largement
  contribu\'{e} \`{a} populariser ces techniques et \`{a} convaincre leurs
  utilisateurs qu’il est possible d’obtenir des r\'{e}sultats remarquables,
  \`{a} condition de mettre en \oe uvre une m\'{e}thodologie de conception
  rigoureuse, scientifique- ment fond\'{e}e, dans un domaine o\`{u}
  l’empirisme a longtemps tenu lieu de m\'{e}thode. Tout en restant
  fid\`{e}le \`{a} l’esprit de cet ouvrage, combinant fondements
  math\'{e}matiques et m\'{e}thodologie de mise en \oe uvre, les auteurs ont
  \'{e}largi le champ de la pr\'{e}sentation, afin de permettre au lecteur
  d’aborder d’autres m\'{e}thodes d’apprentissage statistique que celles
  qui sont directement d\'{e}crites dans cet ouvrage. En effet, les succ\`{e}s
  de l’apprentissage dans un grand nombre de domaines ont pouss\'{e} au
  d\'{e}veloppement de tr\`{e}s nombreuses variantes, souvent destin\'{e}es
  \`{a} r\'{e}pondre efficacement aux exigences de telle ou telle classe
  d’applications. Toutes ces variantes ont n\'{e}anmoins des bases
  th\'{e}oriques et des aspects m\'{e}thodolo- giques communs, qu’il est
  important d’avoir pr\'{e}sents \`{a} l’esprit. Le terme
  d’apprentissage, comme celui de r\'{e}seau de neurones, \'{e}voque
  \'{e}videmment le fonctionnement du cerveau. Il ne faut pourtant pas
  s’attendre \`{a} trouver ici d’explications sur les m\'{e}canismes de
  traitement des informations dans les syst\`{e}mes nerveux ; ces derniers sont
  d’une grande complexit\'{e}, r\'{e}sultant de processus \'{e}lectriques et
  chimiques subtils, encore mal compris en d\'{e}pit de la grande quantit\'{e}
  de donn\'{e}es exp\'{e}rimentales disponibles. Si les m\'{e}thodes
  d’apprentissage statistique peuvent \^{e}tre d’une grande utilit\'{e}
  pour cr\'{e}er des mod\`{e}les empiriques de telle ou telle fonction
  r\'{e}alis\'{e}e par le syst\`{e}me nerveux, celles qui sont d\'{e}crites
  dans cet ouvrage n’ont aucunement la pr\'{e}tention d’imiter, m\^{e}me
  vaguement, le fonctionne- ment du cerveau. L’apprentissage artificiel,
  notamment statistique, permettra-t-il un jour de donner aux ordinateurs des
  capacit\'{e}s analogues \`{a} celles des \^{e}tres humains ? Se
  rapprochera-t-on de cet objectif en perfectionnant les techniques actuelles
  d’apprentissage, ou bien des approches radicalement nouvelles sont-elles
  indispensables ? Faut-il s’inspirer de ce que l’on sait, ou croit savoir,
  sur le fonctionnement du cerveau ? Ces questions font l’objet de d\'{e}bats
  passionn\'{e}s, et passionnants, au sein de la communaut\'{e} scientifique :
  on n’en trouvera pas les r\'{e}ponses ici.%
  }
  \field{edition}{Eyrolles}
  \field{isbn}{9782212114645}
  \field{pages}{471}
  \field{title}{{Apprentissage Apprentissage statistique}}
  \verb{file}
  \verb :C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley D
  \verb esktop/Downloaded/G. Dreyfus, J.-M. Martinez, M. Samuelides M. B. Gordo
  \verb n, F. Badran - 2006 - Apprentissage Apprentissage statistique.pdf:pdf
  \endverb
  \field{year}{2006}
\endentry

\lossort
\endlossort

\endinput
