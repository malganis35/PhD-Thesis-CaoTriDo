Automatically generated by Mendeley Desktop 1.15.2
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Xu2012,
abstract = {Recent work in metric learning has significantly improved the state-of-the-art in k-nearest neighbor classification. Support vector machines (SVM), particularly with RBF kernels, are amongst the most popular classification algorithms that uses distance metrics to compare examples. This paper provides an empirical analysis of the efficacy of three of the most popular Mahalanobis metric learning algorithms as pre-processing for SVM training. We show that none of these algorithms generate metrics that lead to particularly satisfying improvements for SVM-RBF classification. As a remedy we introduce support vector metric learning (SVML), a novel algorithm that seamlessly combines the learning of a Mahalanobis metric with the training of the RBF-SVM parameters. We demonstrate the capabilities of SVML on nine benchmark data sets of varying sizes and difficulties. In our study, SVML outperforms all alternative state-of-the-art metric learning algorithms in terms of accuracy and establishes itself as a serious alternative to the standard Euclidean metric with model selection by cross validation.},
archivePrefix = {arXiv},
arxivId = {1208.3422},
author = {Xu, Zhixiang and Weinberger, Kilian Q. and Chapelle, Olivier},
eprint = {1208.3422},
journal = {Arxiv},
keywords = {distance learning,mahalanobis distance,metric learning,semi-definite programming,support vector machines},
pages = {1--17},
title = {{Distance Metric Learning for Kernel Machines}},
url = {http://arxiv.org/abs/1208.3422},
year = {2012}
}
@article{Berndt1994a,
abstract = {Knowledge discovery in databases presents many interesting challenges within the context of providing computer tools for exploring large data archives. Electronic data repositories are growing qulckiy and contain data from commercial, scientific, and other domains. Much of this data is inherently temporal, such as stock prices or NASA telemetry data. Detecting patterns in such data streams or time series is an important knowledge discovery task. This paper describes some primary experiments with a dynamic programming approach to the problem. The pattern detection algorithm is based on the dynamic time warping technique used in the speech recognition field. Keywords: dynamic programming, dynamic time warping, knowledge discovery, pattern analysis, time series.},
author = {Berndt, Donald and Clifford, James},
journal = {Workshop on Knowledge Knowledge Discovery in Databases},
keywords = {dynamic programming,dynamic time warping,knowledge discovery,pat,tern analysis,time series},
pages = {359--370},
title = {{Using dynamic time warping to find patterns in time series}},
url = {http://www.aaai.org/Papers/Workshops/1994/WS-94-03/WS94-03-031.pdf},
volume = {398},
year = {1994}
}
@article{Cover1967a,
abstract = {The nearest neighbor decision rule assigns to an unclassified sample point the classification of the nearest of a set of previously classified points. This rule is independent of the underlying joint distribution on the sample points and their classifications, and hence the probability of error<tex>R</tex>of such a rule must be at least as great as the Bayes probability of error<tex>R{\^{}}{\{}ast{\}}</tex>--the minimum probability of error over all decision rules taking underlying probability structure into account. However, in a large sample analysis, we will show in the<tex>M</tex>-category case that<tex>R{\^{}}{\{}ast{\}} leq R leq R{\^{}}{\{}ast{\}}(2 --MR{\^{}}{\{}ast{\}}/(M-1))</tex>, where these bounds are the tightest possible, for all suitably smooth underlying distributions. Thus for any number of categories, the probability of error of the nearest neighbor rule is bounded above by twice the Bayes probability of error. In this sense, it may be said that half the classification information in an infinite sample set is contained in the nearest neighbor.},
author = {Cover, T. and Hart, P.},
doi = {10.1109/TIT.1967.1053964},
isbn = {0018-9448},
issn = {0018-9448},
journal = {IEEE Transactions on Information Theory},
number = {1},
pages = {21--27},
pmid = {21919855},
title = {{Nearest neighbor pattern classification}},
volume = {13},
year = {1967}
}
@article{Simard1992,
abstract = {Memory-based classification algorithms such as radial basis functions or K-nearest neighbors typically rely on simple distances (Euclidean, dot product...), which are not particularly meaningful on pattern vectors. More complex, better suited distance measures are often expensive and rather ad-hoc (elastic matching, deformable templates). We propose a new distance measure which (a) can be made locally invariant to any set of transformations of the input and (b) can be computed efficiently. We tested the method on large handwritten character databases provided by the Post Office and the NIST. Using invariances with respect to translation, rotation, scaling, shearing and line thickness, the method consistently outperformed all other systems on the same databases.},
author = {Simard, Patrice and LeCun, Yann and Denker, John S.},
file = {:C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Simard, LeCun, Denker - 1992 - Efficient pattern recognition using a new transformation distance.pdf:pdf},
isbn = {1558602747},
journal = {Advances in Neural Information Processing Systems (NIPS)},
pages = {50--58},
title = {{Efficient pattern recognition using a new transformation distance}},
url = {http://papers.nips.cc/paper/656-efficient-pattern-recognition-using-a-new-transformation-distance},
year = {1992}
}
@article{Cochran1977,
abstract = {Commentary by : Cochran William C. Current Contents : {\#}19, May 9, 1977},
author = {Cochran, William C},
journal = {Citation Classics},
pages = {1},
title = {{Snedecor G W {\&} Cochran W G. Statistical methods applied to experiments in agriculture and biology. 5th ed. Ames, Iowa: Iowa State University Press, 1956.}},
url = {papers3://publication/uuid/8C5C843E-F853-4CB4-82BC-1141F0C01CB4},
volume = {19},
year = {1977}
}
@inproceedings{Yin2008,
abstract = {Event detection is a critical task in sensor networks, especially for environmental monitoring applications. Traditional solutions to event detection are based on analyzing one-shot data points, which might incur a high false alarm rate because sensor data is inherently unreliable and noisy. To address this issue, we propose a novel Distributed Single-pass Incremental Clustering (DSIC) technique to cluster the time series obtained at sensor nodes based on their underlying trends. In order to achieve scalability and energy-efficiency, our DSIC technique uses a hierarchical structure of sensor networks as the underlying infrastructure. The algorithm first compresses the time series produced at individual sensor nodes into a compact representation using Haar wavelet transform, and then, based on dynamic time warping distances, hierarchically groups the approximate time series into a global clustering model in an incremental manner. Experimental results on both real data and synthetic data demonstrate that our DSIC algorithm is accurate, energy-efficient and robust with respect to network topology changes.},
author = {Yin, J. and Gaber, M.},
booktitle = {ICDM},
doi = {10.1109/ICDM.2008.58},
isbn = {9780769535029},
issn = {15504786},
title = {{Clustering distributed time series in sensor networks}},
year = {2008}
}
@article{Do2012,
archivePrefix = {arXiv},
arxivId = {arXiv:1201.4714v1},
author = {Do, Huyen and Kalousis, Alexandros and Wang, Jun and Woznica, Adam},
eprint = {arXiv:1201.4714v1},
file = {:C$\backslash$:/Users/SESA245227/Desktop/ML and SVM.pdf:pdf},
journal = {Proceedings of the 15th International Con- ference on Artificial Intelligence and Statistics (AISTAS '12)},
pages = {308--317},
title = {{A metric learning perspective of SVM: on the relation of LMNN and SVM}},
url = {http://cui.unige.ch/{~}wangjun/papers/svm{\_}lmnn{\_}aistats12.pdf},
year = {2012}
}
@article{Reyes2011,
abstract = {We present a gesture recognition approach for depth video data based on a novel Feature Weighting approach within the Dynamic Time Warping framework. Depth features from human joints are compared through video sequences using Dynamic Time Warping, and weights are assigned to features based on inter-intra class gesture variability. Feature Weighting in Dynamic Time Warping is then applied for recognizing begin-end of gestures in data sequences. The obtained results recognizing several gestures in depth data show high performance compared with classical Dynamic Time Warping approach.},
author = {Reyes, Miguel},
doi = {10.1109/ICCVW.2011.6130384},
isbn = {978-1-4673-0063-6},
journal = {Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on},
pages = {1182--1188},
title = {{Feature weighting in dynamic time warping for gesture recognition in depth data}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6130384},
year = {2011}
}
@article{Hu2013,
abstract = {In this paper, a hybrid forecasting approach, which combines the Ensemble Empirical Mode Decomposition (EEMD) and the Support Vector Machine (SVM), is proposed to improve the quality of wind speed forecasting. The essence of the methodology incorporates three phases. First, the original data of wind speed are decomposed into a number of independent Intrinsic Mode Functions (IMFs) and one residual series by EEMD using the principle of decomposition. In order to forecast these IMFs, excepting the highest frequency acquired by EEMD, the respective estimates are yielded using the SVM algorithm. Finally, these respective estimates are combined into the final wind speed forecasts using the principle of ensemble. The proposed hybrid method is examined by forecasting the mean monthly wind speed of three wind farms located in northwest China. The obtained results confirm an observable improvement for the forecasting validity of the proposed hybrid approach. This tool shows great promise for the forecasting of intricate time series which are intrinsically highly volatile and irregular. ?? 2013 Elsevier Ltd.},
author = {Hu, Jianming and Wang, Jianzhou and Zeng, Guowei},
doi = {10.1016/j.renene.2013.05.012},
file = {:C$\backslash$:/Users/SESA245227/Google Drive/Th{\`{e}}se/Bibliographie/SVM + Time Series/Hu-Renewable Energy-2013{\_}A hybrid forecasting approach applied to wind speed time series.pdf:pdf},
isbn = {0960-1481},
issn = {09601481},
journal = {Renewable Energy},
keywords = {Ensemble Empirical Mode Decomposition (EEMD),Support Vector Machine (SVM),Wind farm,Wind speed forecasting},
pages = {185--194},
publisher = {Elsevier Ltd},
title = {{A hybrid forecasting approach applied to wind speed time series}},
url = {http://dx.doi.org/10.1016/j.renene.2013.05.012},
volume = {60},
year = {2013}
}
@article{Bottou2007,
abstract = {Considerable efforts have been devoted to the implementation of efficient optimization method for solving the Support Vector Machine dual problem. This document proposes an historical perspective and and in depth review of the algorithmic and computational issues associated with this problem.},
author = {Bottou, L and Lin, CJ},
isbn = {0262026252},
journal = {Large scale kernel machines},
pages = {1--27},
title = {{Support vector machine solvers}},
url = {http://140.112.30.28/{~}cjlin/papers/bottou{\_}lin.pdf},
year = {2007}
}
@article{Jain1999,
abstract = {Clustering is the unsupervised classification of patterns (observations, data items, or feature vectors) into groups (clusters). The clustering problem has been addressed in many contexts and by researchers in many disciplines; this reflects its broad appeal and usefulness as one of the steps in exploratory data analysis. However, clustering is a difficult problem combinatorially, and differences in assumptions and contexts in different communities has made the transfer of useful generic concepts and methodologies slow to occur. This paper presents an overview of pattern clustering methods from a statistical pattern recognition perspective, with a goal of providing useful advice and references to fundamental concepts accessible to the broad community of clustering practitioners. We present a taxonomy of clustering techniques, and identify cross-cutting themes and recent advances. We also describe some important applications of clustering algorithms such as image segmentation, object recognition, and information retrieval.},
archivePrefix = {arXiv},
arxivId = {arXiv:1101.1881v2},
author = {Jain, a. K. and Murty, M. N. and Flynn, P. J.},
doi = {10.1145/331499.331504},
eprint = {arXiv:1101.1881v2},
isbn = {0360-0300},
issn = {03600300},
journal = {ACM Computing Surveys},
number = {3},
pages = {264--323},
pmid = {17707831},
title = {{Data clustering: a review}},
url = {http://portal.acm.org/citation.cfm?doid=331499.331504},
volume = {31},
year = {1999}
}
@inproceedings{Do2015,
abstract = {Time series are complex data objects, they may present noise, varying delays or involve several temporal granularities. To classify time series, promising solutions refer to the combination of multiple basic metrics to compare time series according to several characteristics. This work proposes a new framework to learn a combi- nation of multiple metrics for a robust kNN classifier. By introducing the concept of pairwise space, the com- bination function is learned in this new space through a "large margin" optimization process. We apply it to compare time series on both their values and behaviors. The efficiency of the learned metric is compared to the major alternative metrics on large public datasets.},
address = {Nice, France},
author = {DO, C. and Douzal-Chouakria, A. and Mari{\'{e}}, S. and Rombaut, M.},
booktitle = {Signal Processing Conference (EUSIPCO), 2015 Proceedings of the 23rd European},
file = {:C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Do et al. - 2015 - Multiple Metric Learning for large margin kNN Classification of time series.pdf:pdf},
keywords = {Classification,Multiple metric learning,Time series,kNN},
mendeley-tags = {Classification,Multiple metric learning,Time series,kNN},
pages = {2391 -- 2395},
title = {{Multiple Metric Learning for large margin kNN Classification of time series}},
year = {2015}
}
@article{Dietterich1995,
abstract = {The performance of the error backpropagation (BP) and ID3 learning algorithms was compared on the task of mapping English text to phonemes and stresses. Under the distributed output code developed by Sejnowski and Rosenberg, it is shown that BP consistently out-performs ID3 on this task by several percentage points. Three hypotheses explaining this difference were explored: (a) ID3 is overfitting the training data, (b) BP is able to share hidden units across several output units and hence can learn the output units better, and (c) BP captures statistical information that ID3 does not. We conclude that only hypothesis (c) is correct. By augmenting ID3 with a simple statistical learning procedure, the performance of BP can be closely matched. More complex statistical procedures can improve the performance of both BP and ID3 substantially in this domain.},
author = {Dietterich, Thomas G. and Hild, Hermann and Bakiri, Ghulum},
doi = {10.1007/BF00993821},
isbn = {0885-6125},
issn = {08856125},
journal = {Machine Learning},
keywords = {ID3,backpropagation,experimental comparisons,text-to-speech},
number = {1},
pages = {51--80},
title = {{A comparison of ID3 and backpropagation for English text-to-speech mapping}},
volume = {18},
year = {1995}
}
@article{Zhang2006a,
abstract = {Thewidelyknowncurse of dimension- ality howto choose the appropriate dimension is a challenging task especially for clustering problem in the absence of data labels that has not been well studied in the literature. I this paper we propose an unsupervised feature extraction algorithm using orthogonal wavelet transform for automatically choosing the dimensionality of features. The feature extraction algorithm selects the feature dimensionality by leveraging twoconflicting requirements, i.e., lower dimensionality and lower sum of squared errors between the features and the original time series. The proposed feature extraction algorithm is efficient with time complexityO(mn) when using Haar wavelet. Encouraging experimental results are obtained on several synthetic and real-world time series datasets.},
author = {Zhang, Hui and Ho, T.B. Tb Tu Bao and Zhang, Yang and Lin, M.S. Mao-Song Ms},
journal = {Guest Editor},
keywords = {2005,attracted increasing interest in,clustering,data mining,feature extraction,financial domains,in the bioinformatics and,of dimension-,particularly for long time,received,september 4,series,such as those arising,the last decade,the widely known curse,time series,time series clustering has,wavelet},
number = {3},
pages = {305--319},
title = {{Unsupervised Feature Extraction for Time Series Clustering Using Orthogonal Wavelet Transform.}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Unsupervised+Feature+Extraction+for+Time+Series+Clustering+Using+Orthogonal+Wavelet+Transform{\#}0$\backslash$nhttp://www.freepatentsonline.com/article/Informatica/158092585.html$\backslash$nhttp://search.ebscohost.com},
volume = {30},
year = {2006}
}
@article{Vlachos2006,
abstract = {While most time series data mining research has concentrated on providing solutions for a single distance function, in this work we motivate the need for an index structure that can support multiple distance measures. Our specific area of interest is the efficient retrieval and analysis of similar trajectories. Trajectory datasets are very common in environmental applications, mobility experiments, and video surveillance and are especially important for the discovery of certain biological patterns. Our primary similarity measure is based on the longest common subsequence (LCSS) model that offers enhanced robustness, particularly for noisy data, which are encountered very often in real-world applications. However, our index is able to accommodate other distance measures as well, including the ubiquitous Euclidean distance and the increasingly popular dynamic time warping (DTW). While other researchers have advocated one or other of these similarity measures, a major contribution of our work is the ability to support all these measures without the need to restructure the index. Our framework guarantees no false dismissals and can also be tailored to provide much faster response time at the expense of slightly reduced precision/recall. The experimental results demonstrate that our index can help speed up the computation of expensive similarity measures such as the LCSS and the DTW.},
author = {Vlachos, Michail and Hadjieleftheriou, Marios and Gunopulos, Dimitrios and Keogh, Eamonn},
doi = {10.1007/s00778-004-0144-2},
isbn = {1-58113-737-0},
issn = {10668888},
journal = {VLDB Journal},
keywords = {Dynamic time warping,Ensemble index,Longest common subsequence,Motion capture,Trajectories},
number = {1},
pages = {1--20},
title = {{Indexing multidimensional time-series}},
volume = {15},
year = {2006}
}
@article{Wang2002,
abstract = {Recently a new learning method called support vector machines (SVM) has shown comparable or better results than neural networks on some applications. In this thesis we exploit the possibility of using SVM for three important issues of bioinformatics: the prediction of protein secondary structure, multi-class protein fold recognition, and the prediction of human signal peptide cleavage sites. By using similar data, we demonstrate that SVM can easily achieve comparable accuracy as using neural networks. Therefore, in the future it is a promising direction to apply SVM on more bioinformatics applications.},
author = {Wang, Jung-Ying},
journal = {Bioinformatics},
pages = {1--56},
title = {{Support Vector Machines ( SVM ) in bioinformatics Bioinformatics applications}},
url = {http://www.csie.ntu.edu.tw/{~}p88012/Bio{\_}SVM.pdf},
year = {2002}
}
@misc{Keller1985,
abstract = {Classification of objects is an important area of research and application in a variety of fields. In the presence of full knowledge of the underlying probabilities, Bayes decision theory gives optimal error rates. In those cases where this information is not present, many algorithms make use of distance or similarity among samples as a means of classification. The K-nearest neighbor decision rule has often been used in these pattern recognition problems. One of the difficulties that arises when utilizing this technique is that each of the labeled samples is given equal importance in deciding the class memberships of the pattern to be classified, regardless of their `typicalness'. The theory of fuzzy sets is introduced into the K-nearest neighbor technique to develop a fuzzy version of the algorithm. Three methods of assigning fuzzy memberships to the labeled samples are proposed, and experimental results and comparisons to the crisp version are presented.},
author = {Keller, James M. and Gray, Michael R. and Givens, James a.},
booktitle = {IEEE Transactions on Systems, Man, and Cybernetics},
doi = {10.1109/TSMC.1985.6313426},
isbn = {0018-9472},
issn = {0018-9472},
number = {4},
pages = {580--585},
title = {{A fuzzy K-nearest neighbor algorithm}},
volume = {SMC-15},
year = {1985}
}
@article{Ding2008,
abstract = {The last decade has witnessed a tremendous growths of interests in applications that deal with querying and mining of time series data. Numerous representation methods for dimensionality reduction and similarity measures geared towards time series have been introduced. Each individual work introducing a particular method has made specific claims and, aside from the occasional theoretical justifications, provided quantitative experimental observations. However, for the most part, the comparative aspects of these experiments were too narrowly focused on demonstrating the benefits of the proposed methods over some of the previously introduced ones. In order to provide a comprehensive validation, we conducted an extensive set of time series experiments re-implementing 8 different representation methods and 9 similarity measures and their variants, and testing their effectiveness on 38 time series data sets from a wide variety of application domains. In this paper, we give an overview of these different techniques and present our comparative experimental findings regarding their effectiveness. Our experiments have provided both a unified validation of some of the existing achievements, and in some cases, suggested that certain claims in the literature may be unduly optimistic. 1.},
archivePrefix = {arXiv},
arxivId = {1012.2789v1},
author = {Ding, Hui and Trajcevski, Goce and Scheuermann, Peter and Wang, Xiaoyue and Keogh, Eamonn},
doi = {10.1145/1454159.1454226},
eprint = {1012.2789v1},
file = {:C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ding, Trajcevski, Scheuermann - 2008 - Querying and Mining of Time Series Data Experimental Comparison of Representations and Distance.pdf:pdf},
isbn = {0000000000000},
issn = {2150-8097},
journal = {Proceedings of the VLDB Endowment},
number = {2},
pages = {1542--1552},
publisher = {VLDB Endowment},
title = {{Querying and Mining of Time Series Data : Experimental Comparison of Representations and Distance Measures}},
url = {http://dl.acm.org/citation.cfm?id=1454226},
volume = {1},
year = {2008}
}
@article{Tan2006,
abstract = {OBJECTIVES: Reductions in exposure to environmental tobacco smoke have been shown to attenuate the risk of cardiovascular disease. We examined whether the 2003 implementation of a comprehensive smoking ban in New York State was associated with reduced hospital admissions for acute myocardial infarction and stroke, beyond the effect of moderate, local and statewide smoking restrictions, and independent of secular trends. METHODS: We analyzed trends in county-level, age-adjusted, monthly hospital admission rates for acute myocardial infarction and stroke from 1995 to 2004 to identify any association between admission rates and implementation of the smoking ban. We used regression models to adjust for the effects of pre-existing smoking restrictions, seasonal trends in admissions, differences across counties, and secular trends. RESULTS: In 2004, there were 3813 fewer hospital admissions for acute myocardial infarction than would have been expected in the absence of the comprehensive smoking ban. Direct health care cost savings of {\$}56 million were realized in 2004. There was no reduction in the number of admissions for stroke. CONCLUSIONS: Hospital admission rates for acute myocardial infarction were reduced by 8{\%} as a result of a comprehensive smoking ban in New York State after we controlled for other relevant factors. Comprehensive smoking bans constitute a simple, effective intervention to substantially improve the public's health},
author = {Tan, Pang-Ning and Steinbach, Michael and Kumar, Vipin},
doi = {10.1016/0022-4405(81)90007-8},
file = {:C$\backslash$:/MyData/Code/Stage/2011 CaoTri/Bibliographie/Machine Learning et Automatique/Chapitre 4 - Classification - Basic Concepts Decision Tree and Model Evaluation.pdf:pdf},
isbn = {0321321367},
issn = {00224405},
journal = {Introduction to Data Mining},
number = {17},
pages = {145--205},
pmid = {20386987},
title = {{Classification : Basic Concepts , Decision Trees , and}},
url = {http://www-users.cs.umn.edu/{~}kumar/dmbook/index.php},
volume = {67},
year = {2006}
}
@inproceedings{Chopra2005,
abstract = {We present a method for training a similarity metric from data. The method can be used for recognition or verification applications where the number of categories is very large and not known during training, and where the number of training samples for a single category is very small. The idea is to learn a function that maps input patterns into a target space such that the L1 norm in the target space approximates the "semantic" distance in the input space. The method is applied to a face verification task. The learning process minimizes a discriminative loss function that drives the similarity metric to be small for pairs of faces from the same person, and large for pairs from different persons. The mapping from raw to the target space is a convolutional network whose architecture is designed for robustness to geometric distortions. The system is tested on the Purdue/AR face database which has a very high degree of variability in the pose, lighting, expression, position, and artificial occlusions such as dark glasses and obscuring scarves.},
author = {Chopra, Sumit and Hadsell, Raia and LeCun, Yann},
booktitle = {CVPR},
doi = {10.1109/CVPR.2005.202},
isbn = {0769523722},
issn = {10636919},
pages = {539--546},
title = {{Learning a similarity metric discriminatively, with application to face verification}},
volume = {1},
year = {2005}
}
@article{Rabiner1989,
abstract = {This tutorial provides an overview of the basic theory of hidden$\backslash$nMarkov models (HMMs) as originated by L.E. Baum and T. Petrie (1966) and$\backslash$ngives practical details on methods of implementation of the theory along$\backslash$nwith a description of selected applications of the theory to distinct$\backslash$nproblems in speech recognition. Results from a number of original$\backslash$nsources are combined to provide a single source of acquiring the$\backslash$nbackground required to pursue further this area of research. The author$\backslash$nfirst reviews the theory of discrete Markov chains and shows how the$\backslash$nconcept of hidden states, where the observation is a probabilistic$\backslash$nfunction of the state, can be used effectively. The theory is$\backslash$nillustrated with two simple examples, namely coin-tossing, and the$\backslash$nclassic balls-in-urns system. Three fundamental problems of HMMs are$\backslash$nnoted and several practical techniques for solving these problems are$\backslash$ngiven. The various types of HMMs that have been studied, including$\backslash$nergodic as well as left-right models, are described},
author = {Rabiner, L.R.},
doi = {10.1109/5.18626},
isbn = {1558601244},
issn = {00189219},
journal = {Proceedings of the IEEE},
number = {2},
pages = {257--286},
pmid = {21920608},
title = {{A tutorial on hidden Markov models and selected applications in speech recognition}},
volume = {77},
year = {1989}
}
@inproceedings{Boser1992,
abstract = {A training algorithm that maximizes the margin  between the training patterns and the decision  boundary is presented. The technique  is applicable to a wide variety of classifiaction  functions, including Perceptrons, polynomials,  and Radial Basis Functions. The effective  number of parameters is adjusted automatically  to match the complexity of the problem.  The solution is expressed as a linear combination  of supporting patterns. These are the  subset of training patterns that are closest to  the decision boundary. Bounds on the generalization  performance based on the leave-one-out  method and the VC-dimension are given. Experimental  results on optical character recognition  problems demonstrate the good generalization  obtained when compared with other  learning algorithms.  1 INTRODUCTION  Good generalization performance of pattern classifiers is achieved when the capacity of the classification function is matched to the size of the training set. Classifiers with a large numb...},
author = {Boser, Bernhard E. and Guyon, Isabelle M. and Vapnik, Vladimir N.},
booktitle = {Proceedings of the 5th Annual ACM Workshop on Computational Learning Theory},
doi = {10.1.1.21.3818},
isbn = {089791497X},
issn = {0-89791-497-X},
pages = {144--152},
title = {{A Training Algorithm for Optimal Margin Classifiers}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.21.3818},
year = {1992}
}
@inproceedings{Do,
abstract = {In order to classify time series, many machine learning algorithms such as the kNN classier require a metric. We propose in this work a framework to learn a combination of multiple metrics for a robust kNN classier. This combined metric includes both temporal (value and behavior) and frequential components. By introducing the concept of pairwise space, the combination function is learned in this new space through a "large margin" optimization process. The effciency of the learned metric is compared to the major alternative metrics on large public datasets.},
address = {Porto, Portugal},
author = {DO, Cao Tri and Douzal-Chouakria, Ahlame and Mari{\'{e}}, Sylvain and Rombaut, Mich{\`{e}}le},
booktitle = {Workshop on Advanced Analytics and Learning from Temporal Data},
file = {:C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Do et al. - 2015 - Temporal and frequential multiple metric learning for time series kNN classication.pdf:pdf},
keywords = {Classication,Multiple metric learning,Spectral,Time series,classificationcation,kNN,knn,metrics.,multiple metric learning,spectral,time series},
mendeley-tags = {Classication,Multiple metric learning,Spectral,Time series,kNN,metrics.},
pages = {39--45},
title = {{Temporal and frequential multiple metric learning for time series kNN classication}},
volume = {1425},
year = {2015}
}
@article{Ramasso2008,
abstract = {This paper focuses on human behavior recognition where the main problem$\backslash$nis to bridge the semantic gap between the analogue observations of the$\backslash$nreal world and the symbolic world of human interpretation. For that, a$\backslash$nfusion architecture based on the Transferable Belief Model framework is$\backslash$nproposed and applied to action recognition of an athlete in video$\backslash$nsequences of athletics meeting with moving camera. Relevant features are$\backslash$nextracted from videos, based on both the camera motion analysis and the$\backslash$ntracking of particular points on the athlete's silhouette. Some models$\backslash$nof interpretation are used to link the numerical features to the symbols$\backslash$nto be recognized, which are running, jumping and falling actions. A$\backslash$nTemporal Belief Filter is then used to improve the robustness of action$\backslash$nrecognition. The proposed approach demonstrates good performance when$\backslash$ntested on real videos of athletics sports videos (high jumps, pole$\backslash$nvaults, triple jumps and long jumps) acquired by a moving camera and$\backslash$ndifferent view angles. The proposed system is also compared to Bayesian$\backslash$nNetworks.},
author = {Ramasso, E. and Panagiotakis, C. and Pellerin, D. and Rombaut, M.},
doi = {10.1007/s10044-007-0073-y},
issn = {14337541},
journal = {Pattern Analysis and Applications},
keywords = {Human action recognition,Moving camera,Temporal Belief Filter,Transferable Belief Model},
number = {1},
pages = {1--19},
title = {{Human action recognition in videos based on the transferable belief model : AAAApplication to athletics jumps}},
volume = {11},
year = {2008}
}
@article{Montero2014,
abstract = {Time series clustering is an active research area with applications in a wide range of fields. One key component in cluster analysis is determining a proper dissimilarity mea- sure between two data objects, and many criteria have been proposed in the literature to assess dissimilarity between two time series. The R package TSclust is aimed to im- plement a large set of well-established peer-reviewed time series dissimilarity measures, including measures based on raw data, extracted features, underlying parametric models, complexity levels, and forecast behaviors. Computation of these measures allows the user to perform clustering by using conventional clustering algorithms. TSclust also includes a clustering procedure based on p values from checking the equality of generating models, and some utilities to evaluate cluster solutions. The implemented dissimilarity functions are accessible individually for an easier extension and possible use out of the clustering context. The main features of TSclust are described and examples of its use are presented.},
author = {Montero, Pablo and Vilar, Jos{\'{e}}},
file = {:C$\backslash$:/Users/SESA245227/Desktop/TS clust.pdf:pdf},
journal = {Journal of Statistical Software November},
keywords = {clustering,dissimilarity measure,time series data,validation indices},
number = {1},
title = {{TSclust : An R Package for Time Series Clustering}},
url = {http://www.jstatsoft.org/v62/i01/paper},
volume = {62},
year = {2014}
}
@article{Lhermitte2011a,
abstract = {Time series of remote sensing imagery or derived vegetation indices and biophysical products have been shown particularly useful to characterize land ecosystem dynamics. Various methods have been developed based on temporal trajectory analysis to characterize, classify and detect changes in ecosystem dynamics. Although time series similarity measures play an important role in these methods, a quantitative comparison of the similarity measures is lacking. The objective of this study was to provide an overview and quantitative comparison of the similarity measures in function of varying time series and ecosystem characteristics, such as amplitude, timing and noise effects. For this purpose, the performance was evaluated for the commonly used similarity measures (D), ranging from Manhattan (DMan), Euclidean (DE) and Mahalanobis (DMah) distance measures, to correlation (DCC), Principal Component Analysis (PCA; DPCA) and Fourier based (DFFT,D$\xi$,DFk) similarities. The quantitative comparison consists of a series of Monte-Carlo simulations based on subsets of global MODIS Normalized Difference Vegetation index (NDVI) and Enhanced Vegetation Index (EVI) and Leaf Area Index (LAI) data. Results of the simulations reveal four main groups of time series similarity measures with different sensitivities: (i) DMan, DE, DPCA, DFk quantify the difference in time series values, (ii) DMah accounts for temporal correlation and non-stationarity of variance, (iii) DCC measures the temporal correlation, and (iv) the Fourier based DFFT and D$\xi$ show their specific sensitivity based on the selected Fourier components. The difference measures show relatively the highest sensitivity to amplitude effects, whereas the correlation based measures are highly sensitive to variations in timing and noise. The Fourier based measures, finally, depend highly on the signal to noise ratio and the balance between amplitude and phase dominance. The heterogeneity in sensitivity of each D stresses the importance of (i) understanding the time series characteristics before applying any classification of change detection approach and (ii) defining the variability one wants to identify/account for. This requires an understanding of the ecosystem dynamics and time series characteristics related to the baseline, amplitude, timing, noise and variability of the ecosystem time series. This is also illustrated in the quantitative comparison, where the different sensitivities of D for the NDVI, EVI, and LAI data relate specifically to the temporal characteristics of each data set. Additionally, the effect of noise and intra- and interclass variability is demonstrated in a case study based on land cover classification.},
author = {Lhermitte, S. and Verbesselt, J. and Verstraeten, W.W. and Coppin, P.},
doi = {10.1016/j.rse.2011.06.020},
isbn = {0034-4257},
issn = {00344257},
journal = {Remote Sensing of Environment},
keywords = {Change detection,Classification,Ecosystem dynamics,Time series analysis},
number = {12},
pages = {3129--3152},
pmid = {9197400},
title = {{A comparison of time series similarity measures for classification and change detection of ecosystem dynamics}},
url = {http://www.sciencedirect.com/science/article/pii/S0034425711002446},
volume = {115},
year = {2011}
}
@article{Bellet2013,
abstract = {The need for appropriate ways to measure the distance or similarity between data is ubiquitous in machine learning, pattern recognition and data mining, but handcrafting such good metrics for specific problems is generally difficult. This has led to the emergence of metric learning, which aims at automatically learning a metric from data and has attracted a lot of interest in machine learning and related fields for the past ten years. This survey paper proposes a systematic review of the metric learning literature, highlighting the pros and cons of each approach. We pay particular attention to Mahalanobis distance metric learning, a well-studied and successful framework, but additionally present a wide range of methods that have recently emerged as powerful alternatives, including nonlinear metric learning, similarity learning and local metric learning. Recent trends and extensions, such as semi-supervised metric learning, metric learning for histogram data and the derivation of generalization guarantees, are also covered. Finally, this survey addresses metric learning for structured data, in particular edit distance learning, and attempts to give an overview of the remaining challenges in metric learning for the years to come.},
archivePrefix = {arXiv},
arxivId = {1306.6709},
author = {Bellet, Aur{\'{e}}lien and Habrard, Amaury and Sebban, Marc},
eprint = {1306.6709},
file = {:C$\backslash$:/Users/SESA245227/Desktop/ML Learning survey.pdf:pdf},
keywords = {(),edit distance,mahalanobis distance,metric learning,similarity learning},
title = {{A Survey on Metric Learning for Feature Vectors and Structured Data}},
url = {http://arxiv.org/abs/1306.6709},
year = {2013}
}
@book{Tan2005b,
abstract = {-This paper is review of current usage of data mining, machine learning and other algorithms for credit risk assessment. We are witnessing importance of credit risk assessment, especially after the global economic crisis on 2008.S o, it is very important to have a proper way to deal with the credit risk and provide powerful and accurate model for credit risk assessment. Many credit scoring techniques such as statistical techniques (logistic regression, discriminant analysis) or advanced techniques such as neural networks, decision trees, genetic algorithm, or support vector machines are used for credit risk assessment. Some of them are described in this article with theirs advantages/disadvantages. Even with many models and methods, it is still hard to say which model is the best or which classifier or which data mining technique is the best. Each model depends on particular data set or attributes set, so it is very important to develop flexible model which is adaptable to every dataset or attribute set.},
author = {Tan, Pang-Ning and Steinbach, Michael and Kumar, Vipin},
booktitle = {Addison Wesley},
isbn = {9789604743179},
keywords = {- credit risk assessments,credit scoring techniques,single classifiers},
pages = {500},
title = {{Introduction to Data Mining}},
year = {2005}
}
@article{Pcekalska2002,
abstract = {Usually, objects to be classified are represented by features. In this paper, we discuss an alternative object representation based on dissimilarity values. If such distances separate the classes well, the nearest neighbor method offers a good solution. However, dissimilarities used in practice are usually far from ideal and the performance of the nearest neighbor rule suffers from its sensitivity to noisy examples. We show that other, more global classification techniques are preferable to the nearest neighbor rule, in such cases.For classification purposes, two different ways of using generalized dissimilarity kernels are considered. In the first one, distances are isometrically embedded in a pseudo-Euclidean space and the classification task is performed there. In the second approach, classifiers are built directly on distance kernels. Both approaches are described theoretically and then compared using experiments with different dissimilarity measures and datasets including degraded data simulating the problem of missing values. http://www.crossref.org/jmlr{\_}DOI.html},
author = {P$\backslash$c{\{}e{\}}kalska, El$\backslash$.{\{}z{\}}bieta and Paclik, Pavel and Duin, Robert P W},
doi = {10.1162/15324430260185592},
file = {:C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pc{\{}e{\}}kalska, Paclik, Duin - 2002 - A Generalized Kernel Approach to Dissimilarity-based Classification.pdf:pdf},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {dissimilarity,embedding,fisher linear discriminant,nearest mean classifier,pseudo euclidean space,support vector classifier},
number = {2},
pages = {175--211},
title = {{A Generalized Kernel Approach to Dissimilarity-based Classification}},
volume = {2},
year = {2002}
}
@inproceedings{Berndt1994,
abstract = {Knowledge discovery in databases presents many interesting challenges within the context of providing computer tools for exploring large data archives. Electronic data repositories are growing qulckiy and contain data from commercial, scientific, and other domains. Much of this data is inherently temporal, such as stock prices or NASA telemetry data. Detecting patterns in such data streams or time series is an important knowledge discovery task. This paper describes some primary experiments with a dynamic programming approach to the problem. The pattern detection algorithm is based on the dynamic time warping technique used in the speech recognition field. Keywords: dynamic programming, dynamic time warping, knowledge discovery, pattern analysis, time series.},
author = {Berndt, D. and Clifford, J.},
booktitle = {KDD},
keywords = {dynamic programming,dynamic time warping,knowledge discovery,pat,tern analysis,time series},
title = {{Using dynamic time warping to find patterns in time series}},
url = {http://www.aaai.org/Papers/Workshops/1994/WS-94-03/WS94-03-031.pdf},
volume = {398},
year = {1994}
}
@article{Caiado2006c,
abstract = {The statistical discrimination and clustering literature has studied the problem of identifying similarities in time series data. Some studies use non-parametric approaches for splitting a set of time series into clusters by looking at their Euclidean distances in the space of points. A new measure of distance between time series based on the normalized periodogram is proposed. Simulation results comparing this measure with others parametric and non-parametric metrics are provided. In particular, the classification of time series as stationary or as non-stationary is discussed. The use of both hierarchical and non-hierarchical clustering algorithms is considered. An illustrative example with economic time series data is also presented. Â© 2005 Elsevier B.V. All rights reserved.},
author = {Caiado, Jorge and Crato, Nuno and Pe{\~{n}}a, Daniel},
doi = {10.1016/j.csda.2005.04.012},
isbn = {01679473},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
keywords = {Autocorrelation function,Classification,Clustering,Euclidean distance,Periodogram,Stationary and non-stationary time series},
number = {10},
pages = {2668--2684},
title = {{A periodogram-based metric for time series classification}},
volume = {50},
year = {2006}
}
@article{Benesty2009,
abstract = {This chapter develops several forms of the Pearson correlation coefficient in the different domains. This coefficient can be used as an optimization criterion to derive different optimal noise reduction filters [14], but is even more useful for analyzing these optimal filters for their noise reduction performance.},
author = {Benesty, J. and Chen, J. and Huang, Y. and Cohen, I.},
doi = {10.1007/978-3-642-00296-0},
isbn = {978-3-642-00295-3},
journal = {Noise Reduction in Speech Processing},
title = {{Pearson correlation coefficient}},
url = {http://www.springerlink.com/index/10.1007/978-3-642-00296-0$\backslash$nhttp://link.springer.com/content/pdf/10.1007/978-3-642-00296-0{\_}5.pdf},
year = {2009}
}
@article{McNames2002,
abstract = {Local models have emerged as one of the most accurate methods of time series prediction, but their performance is sensitive to the choice of user-specified parameters such as the size of the neighborhood, the embedding dimension, and the distance metric. This paper describes a new method of optimizing these parameters to minimize the multi-step cross-validation error. Empirical results indicate that multi-step optimization is susceptible to shallow local minima unless the optimization is limited to 10 or fewer steps ahead. The models optimized using the new method consistently performed better than those optimized with adaptive analog forecasts. ?? 2002 Elsevier Science B.V. All rights reserved.},
author = {McNames, James},
doi = {10.1016/S0925-2312(01)00647-6},
file = {:C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/McNames - 2002 - Local averaging optimization for chaotic time series prediction.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Chaos,Embedding dimension,Local models,Metric optimization,Time series prediction},
number = {May 2001},
pages = {279--297},
title = {{Local averaging optimization for chaotic time series prediction}},
volume = {48},
year = {2002}
}
@article{Sahidullah2012a,
author = {Sahidullah, M. and Saha, G.},
journal = {Speech Communication},
number = {4},
pages = {543--565},
title = {{Design, analysis and experimental evaluation of block based transformation in mfcc computation for speaker recognition}},
volume = {54},
year = {2012}
}
@article{Sadri2003,
abstract = {: A new method for recognition of isolated handwritten Arabic/Persian digits is presented. This method is based on Support Vector Machines (SVMs), and a new approach of feature extraction. Each digit is considered from four different views, and from each view 16 features are extracted and combined to obtain 64 features. Using these features, multiple SVM classifiers are trained to separate different classes of digits. CENPARMI Indian (Arabic/Persian) handwritten digit database is used for training and testing of SVM classifiers. Based on this database, differences between Arabic and Persian digits in digit recognition are shown. This database provides 7390 samples for training and 3035 samples for testing from the real life samples. Experiments show that the proposed features can provide a very good recognition result using Support Vector Machines at a recognition rate 94.14{\%}, compared with 91.25 {\%} obtained by MLP neural network classifier using the same features and test set.},
author = {Sadri, Javad and Suen, Ching Y and Bui, Tien D.},
journal = {Second Conference on Machine Vision and Image Processing {\&} Applications (MVIP 2003)},
keywords = {feature extraction,machine learning,mlp neural network,multiple support vector classifiers,ocr,optical character recognition,support,svm,vector machine},
pages = {300--307},
title = {{Application of Support Vector Machines for recognition of handwritten Arabic/Persian digits}},
volume = {1},
year = {2003}
}
@techreport{WienerN1942,
author = {{Wiener N}},
institution = {Report of the Services 19, Research Project DIC-6037 MIT},
title = {{Extrapolation, Interpolation {\&} Smoothing of Stationary Time Series - With Engineering Applications}},
year = {1942}
}
@article{Weinberger2009a,
abstract = {The accuracy of k-nearest neighbor (kNN) classification depends significantly on the metric used to compute distances between different examples. In this paper, we show how to learn a Mahalanobis distance metric for kNN classification from labeled examples. The Mahalanobis metric can equivalently be viewed as a global linear transformation of the input space that precedes kNN classification using Euclidean distances. In our approach, the metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. As in support vector machines (SVMs), the margin criterion leads to a convex optimization based on the hinge loss. Unlike learning in SVMs, however, our approach requires no modification or extension for problems in multiway (as opposed to binary) classification. In our framework, the Mahalanobis distance metric is obtained as the solution to a semidefinite program. On several data sets of varying size and difficulty, we find that metrics trained in this way lead to significant improvements in kNN classification. Sometimes these results can be further improved by clustering the training examples and learning an individual metric within each cluster. We show how to learn and combine these local metrics in a globally integrated manner.},
author = {Weinberger, Kilian Q and Saul, Lawrence K},
doi = {10.1126/science.277.5323.215},
file = {:C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Weinberger, Saul - 2009 - Distance Metric Learning for Large Margin Nearest Neighbor Classification(2).pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {Metric learning,convex optimization,ing,mahalanobis distance,metric learn-,multi-class classification,semi-definite programming,support vector machines},
pages = {207--244},
pmid = {17490632},
title = {{Distance Metric Learning for Large Margin Nearest Neighbor Classification}},
volume = {10},
year = {2009}
}
@article{Brigham1967,
abstract = {The fast Fourier transform (FFT), a computer algorithm that computes the discrete Fourier transform much faster than other algorithms, is explained. Examples and detailed procedures are provided to assist the reader in learning how to use the algorithm. The savings in computer time can be huge; for example, an N = 210-point transform can be computed with the FFT 100 times faster than with the use of a direct approach.},
author = {Brigham, E. O. and Morrow, R. E.},
doi = {10.1109/MSPEC.1967.5217220},
isbn = {0018-9235},
issn = {0018-9235},
journal = {Spectrum, IEEE},
number = {12},
pages = {63 --70},
title = {{The fast Fourier transform}},
url = {http://ieeexplore.ieee.org/ielx5/6/5217195/05217220.pdf?tp={\&}arnumber=5217220{\&}isnumber=5217195$\backslash$nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?tp={\&}arnumber=5217220},
volume = {4},
year = {1967}
}
@book{Dreyfus2006,
abstract = {En une vingtaine dâann{\'{e}}es, lâapprentissage artificiel est devenu une branche majeure des math{\'{e}}matiques appliqu{\'{e}}es, {\`{a}} lâintersection des statistiques et de lâintelligence artificielle. Son objectif est de r{\'{e}}aliser des mod{\`{e}}les qui apprennent Â« par lâexemple Â» : il sâappuie sur des donn{\'{e}}es num{\'{e}}riques (r{\'{e}}sultats de mesures ou de simulations), contrairement aux mod{\`{e}}les Â« de connaissances Â» qui sâappuient sur des {\'{e}}quations issues des premiers principes de la physique, de la chimie, de la biologie, de lâ{\'{e}}conomie, etc. Lâapprentis- sage statistique est dâune grande utilit{\'{e}} lorsque lâon cherche {\`{a}} mod{\'{e}}liser des processus complexes, souvent non lin{\'{e}}aires, pour lesquels les connaissances th{\'{e}}oriques sont trop impr{\'{e}}cises pour permettre des pr{\'{e}}dictions pr{\'{e}}cises. Ses domaines dâapplications sont multiples : fouille de donn{\'{e}}es, bio-informatique, g{\'{e}}nie des proc{\'{e}}d{\'{e}}s, aide au diagnostic m{\'{e}}dical, t{\'{e}}l{\'{e}}communications, interface cerveau-machines, et bien dâautres. Cet ouvrage refl{\`{e}}te en partie lâ{\'{e}}volution de cette discipline, depuis ses balbutiements au d{\'{e}}but des ann{\'{e}}es 1980, jusquâ{\`{a}} sa situation actuelle ; il nâa pas du tout la pr{\'{e}}tention de faire un point, m{\^{e}}me partiel, sur lâensemble des d{\'{e}}veloppements pass{\'{e}}s et actuels, mais plut{\^{o}}t dâinsister sur les principes et sur les m{\'{e}}thodes {\'{e}}prouv{\'{e}}s, dont les bases scientifiques sont s{\^{u}}res. Dans un domaine sans cesse parcouru de modes multiples et {\'{e}}ph{\'{e}}m{\`{e}}res, il est utile, pour qui cherche {\`{a}} acqu{\'{e}}rir les connaissances et principes de base, dâinsister sur les aspects p{\'{e}}rennes du domaine. Cet ouvrage fait suite {\`{a}} R{\'{e}}seaux de neurones, m{\'{e}}thodologies et applications, des m{\^{e}}mes auteurs, paru en 2000, r{\'{e}}{\'{e}}dit{\'{e}} en 2004, chez le m{\^{e}}me {\'{e}}diteur, puis publi{\'{e}} en traduction anglaise chez Springer. Consacr{\'{e}} essentiellement aux r{\'{e}}seaux de neurones et aux cartes auto-adaptatives, il a largement contribu{\'{e}} {\`{a}} populariser ces techniques et {\`{a}} convaincre leurs utilisateurs quâil est possible dâobtenir des r{\'{e}}sultats remarquables, {\`{a}} condition de mettre en {\oe}uvre une m{\'{e}}thodologie de conception rigoureuse, scientifique- ment fond{\'{e}}e, dans un domaine o{\`{u}} lâempirisme a longtemps tenu lieu de m{\'{e}}thode. Tout en restant fid{\`{e}}le {\`{a}} lâesprit de cet ouvrage, combinant fondements math{\'{e}}matiques et m{\'{e}}thodologie de mise en {\oe}uvre, les auteurs ont {\'{e}}largi le champ de la pr{\'{e}}sentation, afin de permettre au lecteur dâaborder dâautres m{\'{e}}thodes dâapprentissage statistique que celles qui sont directement d{\'{e}}crites dans cet ouvrage. En effet, les succ{\`{e}}s de lâapprentissage dans un grand nombre de domaines ont pouss{\'{e}} au d{\'{e}}veloppement de tr{\`{e}}s nombreuses variantes, souvent destin{\'{e}}es {\`{a}} r{\'{e}}pondre efficacement aux exigences de telle ou telle classe dâapplications. Toutes ces variantes ont n{\'{e}}anmoins des bases th{\'{e}}oriques et des aspects m{\'{e}}thodolo- giques communs, quâil est important dâavoir pr{\'{e}}sents {\`{a}} lâesprit. Le terme dâapprentissage, comme celui de r{\'{e}}seau de neurones, {\'{e}}voque {\'{e}}videmment le fonctionnement du cerveau. Il ne faut pourtant pas sâattendre {\`{a}} trouver ici dâexplications sur les m{\'{e}}canismes de traitement des informations dans les syst{\`{e}}mes nerveux ; ces derniers sont dâune grande complexit{\'{e}}, r{\'{e}}sultant de processus {\'{e}}lectriques et chimiques subtils, encore mal compris en d{\'{e}}pit de la grande quantit{\'{e}} de donn{\'{e}}es exp{\'{e}}rimentales disponibles. Si les m{\'{e}}thodes dâapprentissage statistique peuvent {\^{e}}tre dâune grande utilit{\'{e}} pour cr{\'{e}}er des mod{\`{e}}les empiriques de telle ou telle fonction r{\'{e}}alis{\'{e}}e par le syst{\`{e}}me nerveux, celles qui sont d{\'{e}}crites dans cet ouvrage nâont aucunement la pr{\'{e}}tention dâimiter, m{\^{e}}me vaguement, le fonctionne- ment du cerveau. Lâapprentissage artificiel, notamment statistique, permettra-t-il un jour de donner aux ordinateurs des capacit{\'{e}}s analogues {\`{a}} celles des {\^{e}}tres humains ? Se rapprochera-t-on de cet objectif en perfectionnant les techniques actuelles dâapprentissage, ou bien des approches radicalement nouvelles sont-elles indispensables ? Faut-il sâinspirer de ce que lâon sait, ou croit savoir, sur le fonctionnement du cerveau ? Ces questions font lâobjet de d{\'{e}}bats passionn{\'{e}}s, et passionnants, au sein de la communaut{\'{e}} scientifique : on nâen trouvera pas les r{\'{e}}ponses ici.},
author = {{G. Dreyfus, J.-M. Martinez, M. Samuelides M. B. Gordon, F. Badran}, S. Thiria},
edition = {Eyrolles},
file = {:C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/G. Dreyfus, J.-M. Martinez, M. Samuelides M. B. Gordon, F. Badran - 2006 - Apprentissage Apprentissage statistique.pdf:pdf},
isbn = {9782212114645},
keywords = {Bio-ing{\'{e}}nierie,Machine {\`{a}} Vecteurs Supports,Pr{\'{e}}vision,Reconaissance de formes,Robotique et commande de processus,R{\'{e}}seaux de neurones,cartes topologiques,data mining},
mendeley-tags = {Machine {\`{a}} Vecteurs Supports,R{\'{e}}seaux de neurones,cartes topologiques},
pages = {471},
title = {{Apprentissage Apprentissage statistique}},
year = {2006}
}
@book{Chatfield2004,
abstract = {"Since 1975, The Analysis of Time Series: An Introduction has introduced legions of statistics students and researchers to the theory and practice of time series analysis. The sixth edition provides an accessible, comprehensive introduction to the theory and practice of time series analysis. The treatment covers a wide range of topics, including ARIMA probability models, forecasting methods, spectral analysis, linear systems, state-space models, and the Kalman filter. It also addresses nonlinear, multivariate, and long-memory models. The author has carefully updated each chapter, added new discussions, incorporated new datasets, and made those datasets available at www.crcpress.com."--BOOK JACKET.},
author = {Chatfield, Christopher},
booktitle = {Texts in statistical science},
isbn = {1584883170},
keywords = {Time-series analysis.},
pages = {xiii, 333 p.},
pmid = {13166316},
title = {{The analysis of time series : an introduction}},
year = {2004}
}
@inproceedings{Frambourg2013a,
author = {Frambourg, C. and Douzal-Chouakria, A. and Gaussier, E.},
booktitle = {Intelligent Data Analysis},
pages = {198--209},
title = {{Learning multiple temporal matching for time series classification}},
year = {2013}
}
@article{Cover1967b,
abstract = {The nearest neighbor decision rule assigns to an unclassified sample point the classification of the nearest of a set of previously classified points. This rule is independent of the underlying joint distribution on the sample points and their classifications, and hence the probability of error<tex>R</tex>of such a rule must be at least as great as the Bayes probability of error<tex>R{\^{}}{\{}ast{\}}</tex>--the minimum probability of error over all decision rules taking underlying probability structure into account. However, in a large sample analysis, we will show in the<tex>M</tex>-category case that<tex>R{\^{}}{\{}ast{\}} leq R leq R{\^{}}{\{}ast{\}}(2 --MR{\^{}}{\{}ast{\}}/(M-1))</tex>, where these bounds are the tightest possible, for all suitably smooth underlying distributions. Thus for any number of categories, the probability of error of the nearest neighbor rule is bounded above by twice the Bayes probability of error. In this sense, it may be said that half the classification information in an infinite sample set is contained in the nearest neighbor.},
author = {Cover, T. and Hart, P.},
doi = {10.1109/TIT.1967.1053964},
isbn = {0018-9448},
issn = {0018-9448},
journal = {IEEE Transactions on Information Theory},
number = {1},
pages = {21--27},
pmid = {21919855},
title = {{Nearest neighbor pattern classification}},
volume = {13},
year = {1967}
}
@book{Bishop2006,
abstract = {The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic techniques. The practical applicability of Bayesian methods has been greatly enhanced by the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation, while new models based on kernels have had a significant impact on both algorithms and applications. This completely new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. The book is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. Extensive support is provided for course instructors, including more than 400 exercises, graded according to difficulty. Example solutions for a subset of the exercises are available from the book web site, while solutions for the remainder can be obtained by instructors from the publisher. The book is supported by a great deal of additional material, and the reader is encouraged to visit the book web site for the latest information. A forthcoming companion volume will deal with practical aspects of pattern recognition and machine learning, and will include free software implementations of the key algorithms along with example data sets and demonstration programs. Christopher Bishop is Assistant Director at Microsoft Research Cambridge, and also holds a Chair in Computer Science at the University of Edinburgh. He is a Fellow of Darwin College Cambridge, and was recently elected Fellow of the Royal Academy of Engineering. The author's previous textbook "Neural Networks for Pattern Recognition" has been widely adopted.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Bishop, Christopher M},
booktitle = {Pattern Recognition},
doi = {10.1117/1.2819119},
eprint = {0-387-31073-8},
file = {:C$\backslash$:/Users/SESA245227/Google Drive/Th{\`{e}}se/Bibliographie/Livre/Bishop - Pattern Recognition and Machine Learning.pdf:pdf},
isbn = {9780387310732},
issn = {10179909},
number = {4},
pages = {738},
pmid = {8943268},
title = {{Pattern Recognition and Machine Learning}},
url = {http://www.library.wisc.edu/selectedtocs/bg0137.pdf},
volume = {4},
year = {2006}
}
@article{Dietterich1997,
author = {Dietterich, T.},
file = {:C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dietterich - 1997 - Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms.pdf:pdf},
title = {{Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms}},
year = {1997}
}
@article{Weinberger2009,
abstract = {The accuracy of k-nearest neighbor (kNN) classification depends significantly on the metric used to compute distances between different examples. In this paper, we show how to learn a Maha- lanobis distance metric for kNN classification from labeled examples. The Mahalanobis metric can equivalently be viewed as a global linear transformation of the input space that precedes kNN classification using Euclidean distances. In our approach, the metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a largemargin. As in support vectormachines (SVMs), themargin criterion leads to a convex optimization based on the hinge loss. Unlike learning in SVMs, however, our approach re- quires no modification or extension for problems in multiway (as opposed to binary) classification. In our framework, the Mahalanobis distance metric is obtained as the solution to a semidefinite program. On several data sets of varying size and difficulty, we find that metrics trained in this way lead to significant improvements in kNN classification. Sometimes these results can be further improved by clustering the training examples and learning an individual metric within each cluster. We show how to learn and combine these local metrics in a globally integrated manner. Keywords: convex optimization, semi-definite programming,Mahalanobis distance,metric learn- ing, multi-class classification, support vector machines 1.},
author = {Weinberger, K. and Saul, L.},
file = {:C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Weinberger, Saul - 2009 - Distance Metric Learning for Large Margin Nearest Neighbor Classification(2).pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {convex optimization,ing,mahalanobis distance,metric learn-,multi-class classification,semi-definite programming,support vector machines},
pages = {207--244},
title = {{Distance Metric Learning for Large Margin Nearest Neighbor Classification}},
url = {http://www.jmlr.org/papers/volume10/weinberger09a/weinberger09a.pdf},
volume = {10},
year = {2009}
}
@article{Crammer2001,
abstract = {In this paper we describe the algorithmic implementation of multiclass kernel-based vector machines. Our starting point is a generalized notion of the margin to multiclass problems. Using this notion we cast multiclass categorization problems as a constrained optimization problem with a quadratic objective function. Unlike most of previous approaches which typically decompose a multiclass problem into multiple independent binary classification tasks, our notion of margin yields a direct method for training multiclass predictors. By using the dual of the optimization problem we are able to incorporate kernels with a compact set of constraints and decompose the dual problem into multiple optimization problems of reduced size. We describe an efficient fixed-point algorithm for solving the reduced optimization problems and prove its convergence. We then discuss technical details that yield significant running time improvements for large datasets. Finally, we describe various experiments with our approach comparing it to previously studied kernel-based methods. Our experiments indicate that for multiclass problems we attain state-of-the-art accuracy},
author = {Crammer, Koby and Singer, Yoram},
doi = {10.1162/15324430260185628},
file = {:C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Crammer, Singer - 2001 - On the Algorithmic Implementation of Multiclass Kernel-based Vector Machines.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {kernel machines,multiclass problems,svm},
pages = {265--292},
title = {{On the Algorithmic Implementation of Multiclass Kernel-based Vector Machines}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/CrammerS01.pdf$\backslash$nhttp://www.jmlr.org/papers/volume2/crammer01a/crammer01a.pdf},
volume = {2},
year = {2001}
}
@article{PANAGIOTAKIS2008,
abstract = {We present a shape-based method for automatic people detection and counting without any assumption concerning camera motion. In order to evaluate the robustness of the proposed method, we apply it for classifying athletics videos into two classes: videos of individual and videos of team sports. The videos used are real and characterized by dynamic and unconstrained environment. Moreover, in the case of team sport, we propose a shape deformations based method for running/hurdling discrimination (activity recognition). Robust, adaptive and independent from color, illumination changes and the camera motion, the proposed features are combined in the Transferable Belief Model (TBM) framework providing a two-level (frames and shot) video categorization. Experimental results of 97{\%} of accuracy for individual/team sport categorization using a dataset of 252 real videos of athletic meetings, acquired by moving cameras under varying view angles, indicate the stability and the good performance of the proposed scheme.},
author = {PANAGIOTAKIS, COSTAS and RAMASSO, EMMANUEL and TZIRITAS, GEORGIOS and ROMBAUT, MICH{\`{E}}LE and PELLERIN, DENIS},
doi = {10.1142/S0218001408006752},
issn = {0218-0014},
journal = {International Journal of Pattern Recognition and Artificial Intelligence},
keywords = {People detection,people counting,team activity recognition,transferable belief model,video analysis},
number = {06},
pages = {1187--1213},
title = {{SHAPE-BASED INDIVIDUAL/GROUP DETECTION FOR SPORT VIDEOS CATEGORIZATION}},
url = {http://www.worldscientific.com/doi/abs/10.1142/S0218001408006752?journalCode=ijprai},
volume = {22},
year = {2008}
}
@article{Montero2014a,
abstract = {Time series clustering is an active research area with applications in a wide range of fields. One key component in cluster analysis is determining a proper dissimilarity mea-sure between two data objects, and many criteria have been proposed in the literature to assess dissimilarity between two time series. The R package TSclust is aimed to im-plement a large set of well-established peer-reviewed time series dissimilarity measures, including measures based on raw data, extracted features, underlying parametric models, complexity levels, and forecast behaviors. Computation of these measures allows the user to perform clustering by using conventional clustering algorithms. TSclust also includes a clustering procedure based on p values from checking the equality of generating models, and some utilities to evaluate cluster solutions. The implemented dissimilarity functions are accessible individually for an easier extension and possible use out of the clustering context. The main features of TSclust are described and examples of its use are presented.},
author = {Montero, Pablo and Vilar, Jos{\'{e}}},
doi = {10.18637/jss.v062.i01},
issn = {1548-7660},
journal = {JSS Journal of Statistical Software},
keywords = {clustering,dissimilarity measure,time series data,validation indices},
number = {1},
pages = {1--43},
title = {{TSclust: An R Package for Time Series Clustering}},
url = {http://www.jstatsoft.org/},
volume = {62},
year = {2014}
}
@article{Silverman1989,
abstract = {In 1951, Evelyn Fix and J.L. Hodges, Jr. wrote a technical report which contained prophetic work on nonparametric discriminant analysis and probability density estimation, and which was never published by the authors. The report introduced several important concepts for the first time. It is of interest not only for historical reasons but also because it contains much material that is still of contemporary relevance. Here, the report is printed in full together with a commentary placing the paper in context and interpreting its ideas in the light of more modern developments. /// En 1951, E. Fix et J.L. Hodges, Jr. ont ï¿½crit un rapport technique prophï¿½tique sur l'analyse non-paramï¿½trique de discrimination et l'estimation de la densitï¿½ de probabilitï¿½, mais celui-ci ne fut jamais publiï¿½ par ses auteurs. Ce rapport introduit plusieurs idï¿½es nouvelles et importantes. Il nous intï¿½resse non seulement pour des raisons historiques, mais aussi parce qu'il contient des concepts qui sont encore importants de nos jours. Nous le publions ici en entier, accompagnï¿½ d'un commentaire qui l'interprï¿½te d'un point de vue plus moderne.},
author = {Silverman, B W and Jones, M C},
doi = {10.2307/1403796},
isbn = {03067734},
issn = {03067734},
journal = {International Statistical Review / Revue Internationale de Statistique},
number = {3},
pages = {pp. 233--238},
title = {{E. Fix and J.L. Hodges (1951): An Important Contribution to Nonparametric Discriminant Analysis and Density Estimation: Commentary on Fix and Hodges (1951)}},
url = {http://www.jstor.org/stable/1403796},
volume = {57},
year = {1989}
}
@book{Duda1973,
abstract = {Classic book on pattern recognition. Interesting points: 1) p. 66, and p. 114: Mentions the problems with dimensionality curse. 2) p. 243-246: Mentions Multidimensional scaling (MDS), Karhunen-Loeve and dimensionality reduction. Also, has the spiral data-set as a sample. 3) p. 333: mentions SVD/eigenvalues for linear fitting.},
author = {{O Duda}, Richard and {E Hart}, Peter},
booktitle = {Leonardo},
doi = {10.2307/1573081},
isbn = {0471223611},
issn = {0024094X},
pages = {482},
title = {{Pattern Classification and Scene Analysis}},
url = {http://www.jstor.org/stable/1573081?origin=crossref},
volume = {7},
year = {1973}
}
@article{ZhangX.-L.Z.-G.Luo2014,
author = {Zhang, X.-L. and Luo, Z.-G. and Li, M.},
journal = {Journal of Computer Science and Technology},
number = {6},
pages = {1072--1082},
title = {{Merge-weighted dynamic time warping for speech recognition}},
volume = {29},
year = {2014}
}
@book{Schlkopf2013,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and proteinâprotein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD â¤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Schlkopf, Bernhard and Smola, Alexander J.},
booktitle = {Journal of Chemical Information and Modeling},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/SESA245227/Google Drive/Th{\`{e}}se/Bibliographie/Livre/Schokopf, Smola-Learning with Kernels.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {icle},
pages = {1689--1699},
pmid = {25246403},
title = {{Learning with Kernels}},
volume = {53},
year = {2013}
}
@misc{Keogh2011,
author = {Keogh, E. and Zhu, Q. and Hu, B. and Hao, Y. and Xi, X. and Wei, L. and Ratanamahatana, C.A.},
title = {{The UCR Time Series Classification/Clustering Homepage}},
url = {www.cs.ucr.edu/{~}eamonn/time{\_}series{\_}data/},
year = {2011}
}
@article{Hwang2012,
author = {Hwang, Seok Hwan and Ham, Dae Heon and Kim, Joong Hoon},
doi = {10.1007/s12205-012-1519-3},
file = {:C$\backslash$:/Users/SESA245227/Google Drive/Th{\`{e}}se/Bibliographie/SVM + Time Series/Hwang-KSCE Journal of Civil Engineering-2012{\_}Forecasting performance of LS-SVM for nonlinear hydrological time series.pdf:pdf},
issn = {1226-7988},
journal = {KSCE Journal of Civil Engineering},
keywords = {forecasting,forecasting performance,support vector machine},
number = {5},
pages = {870--882},
title = {{Forecasting performance of LS-SVM for nonlinear hydrological time series}},
url = {http://link.springer.com/10.1007/s12205-012-1519-3},
volume = {16},
year = {2012}
}
@article{AhlameDouzal-Chouakria2011,
abstract = {This paper proposes an extension of classification trees to time series input variables. A new split criterion based on time series proximities is introduced. First, the criterion relies on an adaptive (i.e., parameterized) time series metric to cover both behaviors and values proximities. The metrics parameters may change from one internal node to another to achieve the best bisection of the set of time series. Second, the criterion involves the automatic extraction of the most discriminating subsequences. The proposed time series classification tree is applied to a wide range of datasets: public and new, real and synthetic, univariate and multivariate data. We show, through the experiments performed in this study, that the proposed tree outperforms temporal trees using standard time series distances and performs well compared to other competitive time series classifiers},
author = {Douzal-Chouakria, A. and Amblard, C.},
file = {:C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Douzal-Chouakria, Amblard - 2011 - Classification trees for time series.pdf:pdf},
isbn = {0000000000000},
issn = {2150-8097},
journal = {Pattern Recognition journal},
keywords = {Classification,Supervised classification,Time series proximity measures,trees Learning metric},
publisher = {VLDB Endowment},
title = {{Classification trees for time series}},
year = {2011}
}
@article{Kakizawa1998,
abstract = {... Applications to problems of clustering and classifying earthquakes and mining explosions are given. KEY WORDS: Chernoff; Divergence; Kullback-Leibler; Minimum discrimination information; Robustness; Seismology ; Spectral analysis. ... $\backslash$n},
author = {Kakizawa, Yoshihide and Shumway, Robert H and Taniguchi, Masanobu},
doi = {10.1080/01621459.1998.10474114},
isbn = {0162-1459$\backslash$n1537-274X},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
number = {441},
pages = {328--340},
title = {{Discrimination and Clustering for Multivariate Time Series}},
url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1998.10474114$\backslash$npapers2://publication/doi/10.1080/01621459.1998.10474114},
volume = {93},
year = {1998}
}
@book{Kruskall1983,
author = {Kruskall, J. and Liberman, M.},
booktitle = {TimeWarps, String Edits and Macromolecules},
edition = {Addison-We},
title = {{The symmetric time warping algorithm: From continuous to discrete. In TimeWarps}},
year = {1983}
}
@article{Bellet2013a,
abstract = {The need for appropriate ways to measure the distance or similarity between data is ubiquitous in machine learning, pattern recognition and data mining, but handcrafting such good metrics for specific problems is generally difficult. This has led to the emergence of metric learning, which aims at automatically learning a metric from data and has attracted a lot of interest in machine learning and related fields for the past ten years. This survey paper proposes a systematic review of the metric learning literature, highlighting the pros and cons of each approach. We pay particular attention to Mahalanobis distance metric learning, a well-studied and successful framework, but additionally present a wide range of methods that have recently emerged as powerful alternatives, including nonlinear metric learning, similarity learning and local metric learning. Recent trends and extensions, such as semi-supervised metric learning, metric learning for histogram data and the derivation of generalization guarantees, are also covered. Finally, this survey addresses metric learning for structured data, in particular edit distance learning, and attempts to give an overview of the remaining challenges in metric learning for the years to come.},
archivePrefix = {arXiv},
arxivId = {1306.6709},
author = {Bellet, Aur{\'{e}}lien and Habrard, Amaury and Sebban, Marc},
doi = {10.1073/pnas.0809777106},
eprint = {1306.6709},
issn = {00401951},
journal = {arXiv preprint arXiv:1306.6709},
keywords = {(),edit distance,mahalanobis distance,metric learning,similarity learning},
pages = {57},
pmid = {19342485},
title = {{A Survey on Metric Learning for Feature Vectors and Structured Data}},
url = {http://arxiv.org/abs/1306.6709},
year = {2013}
}
@article{Berndt1994c,
abstract = {Knowledge discovery in databases presents many interesting challenges within the context of providing computer tools for exploring large data archives. Electronic data repositories are growing qulckiy and contain data from commercial, scientific, and other domains. Much of this data is inherently temporal, such as stock prices or NASA telemetry data. Detecting patterns in such data streams or time series is an important knowledge discovery task. This paper describes some primary experiments with a dynamic programming approach to the problem. The pattern detection algorithm is based on the dynamic time warping technique used in the speech recognition field. Keywords: dynamic programming, dynamic time warping, knowledge discovery, pattern analysis, time series.},
author = {Berndt, Donald and Clifford, James},
journal = {Workshop on Knowledge Knowledge Discovery in Databases},
keywords = {dynamic programming,dynamic time warping,knowledge discovery,pat,tern analysis,time series},
pages = {359--370},
title = {{Using dynamic time warping to find patterns in time series}},
url = {http://www.aaai.org/Papers/Workshops/1994/WS-94-03/WS94-03-031.pdf},
volume = {398},
year = {1994}
}
@article{Aizerman1964,
abstract = {Introduction of kernels},
author = {Aizerman, M. and Braverman, E. and Rozonoer, L.},
journal = {Automation and Remote Control},
pages = {821--837},
title = {{Theoretical foundations of the potential function method in pattern recognition learning}},
volume = {25},
year = {1964}
}
@article{Chatpatanasiri2010,
abstract = {This paper focuses on developing a new framework of kernelizing Mahalanobis distance learners. The new KPCA trick framework offers several practical advantages over the classical kernel trick framework, e.g. no mathematical formulas and no reprogramming are required for a kernel implementation, a way to speed up an algorithm is provided with no extra work, the framework avoids troublesome problems such as singularity. Rigorous representer theorems in countably infinite dimensional spaces are given to validate our framework. Furthermore, unlike previous works which always apply brute force methods to select a kernel, we derive a kernel alignment formula based on quadratic programming which can efficiently construct an appropriate kernel for a given dataset. ?? 2010.},
author = {Chatpatanasiri, Ratthachat and Korsrilabutr, Teesid and Tangchanachaianan, Pasakorn and Kijsirikul, Boonserm},
doi = {10.1016/j.neucom.2009.11.037},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Dimensionality reduction,Distance metric learning,Kernel alignment,Kernel machines,Representer theorem},
number = {10-12},
pages = {1570--1579},
title = {{A new kernelization framework for Mahalanobis distance learning algorithms}},
url = {http://dx.doi.org/10.1016/j.neucom.2009.11.037$\backslash$nhttp://ac.els-cdn.com/S0925231210001165/1-s2.0-S0925231210001165-main.pdf?{\_}tid=923c9f62-a756-11e4-970c-00000aab0f6b{\&}acdnat=1422495279{\_}136843023ecd8d5dc1ba9d83e2539013},
volume = {73},
year = {2010}
}
@misc{LIG2014,
title = {{LIG-AMA Machine Learning Datasets Repository}},
url = {http://ama.liglab.fr/resourcestools/datasets/},
year = {2014}
}
@inproceedings{Abraham2010a,
abstract = {Zero-inflated time series data are commonly encountered in many applications, including climate and ecological modeling, disease monitoring, manufacturing defect detection, and traffic monitoring. Such data often leads to poor model fitting using standard regression methods because they tend to underestimate the frequency of zeros and the magnitude of non-zero values. This paper presents an integrated framework that simultaneously performs classification and regression to accurately predict future values of a zero-inflated time series. A regression model is initially applied to predict the value of the time series. The regression output is then fed into a classification model to determine whether the predicted value should be adjusted to zero. Our regression and classification models are trained to optimize a joint objective function that considers both classification errors on the time series and regression errors on data points that have non-zero values. We demonstrate the effectiveness of our framework in the context of its application to a precipitation downscaling problem for climate impact assessment studies. Read More: http://epubs.siam.org/doi/abs/10.1137/1.9781611972801.57},
author = {Abraham, Z. and Tan, P.N.},
booktitle = {ACM SIGKDD},
title = {{An Integrated Framework for Simultaneous Classification and Regression of Time-Series Data}},
url = {https://siam.org/proceedings/datamining/2010/dm10{\_}057{\_}abrahamz.pdf},
year = {2010}
}
@misc{Chen1996,
abstract = {Mining information and knowledge from large databases has been recognized by many researchers as a key research topic in database systems and machine learning, and by many industrial companies as an important area with an opportunity of major revenues. Researchers in many different fields have shown great interest in data mining. Several emerging applications in information-providing services, such as data warehousing and online services over the Internet, also call for various data mining techniques to better understand user behavior, to improve the service provided and to increase business opportunities. In response to such a demand, this article provides a survey, from a database researcher's point of view, on the data mining techniques developed recently. A classification of the available data mining techniques is provided and a comparative study of such techniques is presented},
author = {Chen, Ming Syan and Han, Jiawei and Yu, Philip S.},
booktitle = {IEEE Transactions on Knowledge and Data Engineering},
doi = {10.1109/69.553155},
isbn = {1041-4347},
issn = {10414347},
keywords = {Association rules,Classification,Data clustering,Data cubes,Data generalization and characterization,Data mining,Knowledge discovery,Multiple-dimensional databases,Pattern matching algorithms},
number = {6},
pages = {866--883},
title = {{Data mining: An Overview from a Database Perspective}},
volume = {8},
year = {1996}
}
@article{Douzal-Chouakria2010,
abstract = {This paper addresses the clustering and classification of active genes during the process of cell division. Cell division ensures the proliferation of cells, but it becomes increasingly abnormal in cancer cells. The genes studied here are described by their expression profiles (i.e. time series) during the cell division cycle. This work focuses on evaluating the efficiency of four major metrics for clustering and classifying genes expression profiles and is based on a random-periods model for the expression of cell-cycle genes. The model accounts for the observed attenuation in cycle amplitude or duration, variations in the initial amplitude, and drift in the expression profiles. ?? 2009 Elsevier B.V. All rights reserved.},
author = {Douzal-Chouakria, A. and Diallo, A. and Giroud, F.},
doi = {10.1016/j.patrec.2010.05.008},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Classification,Clustering,Distance,Genes expression profiles,Time series},
title = {{A random-periods model for the comparison of a metrics efficiency to classify cell-cycle expressed genes}},
year = {2010}
}
@article{Douzal-Chouakria2012a,
abstract = {This paper proposes an extension of classification trees to time series input variables. A new split criterion based on time series proximities is introduced. First, the criterion relies on an adaptive (i.e., parameterized) time series metric to cover both behaviors and values proximities. The metrics parameters may change from one internal node to another to achieve the best bisection of the set of time series. Second, the criterion involves the automatic extraction of the most discriminating subsequences. The proposed time series classification tree is applied to a wide range of datasets: public and new, real and synthetic, univariate and multivariate data. We show, through the experiments performed in this study, that the proposed tree outperforms temporal trees using standard time series distances and performs well compared to other competitive time series classifiers. Â© 2011 Elsevier Ltd. All rights reserved.},
author = {Douzal-Chouakria, Ahlame and Amblard, C{\'{e}}cile},
doi = {10.1016/j.patcog.2011.08.018},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Classification trees,Learning metric,Supervised classification,Time series proximity measures},
number = {3},
pages = {1076--1091},
title = {{Classification trees for time series}},
volume = {45},
year = {2012}
}
@inproceedings{Heisele2001,
abstract = {We present a component-based method and two global methods for face recognition and evaluate them with respect to robustness against pose changes. In the component system we first locate facial components, extract them and combine them into a single feature vector which is classified by a Support Vector Machine (SVM). The two global systems recognize faces by classifying a single feature vector consisting of the gray values of the whole face image. In the first global system we trained a single SVM classifier for each person in the database. The second system consists of sets of viewpoint-specific SVM classifiers and involves clustering during training. We performed extensive tests on a database which included faces rotated up to about 40Â° in depth. The component system clearly outperformed both global systems on all tests.},
author = {Heisele, B and Ho, P and Poggio, T},
booktitle = {IEEE International Conference on Computer Vision, ICCV},
doi = {10.1109/ICCV.2001.937693},
isbn = {0-7695-1143-0},
issn = {1089-7801},
keywords = {Active shape model,Biology computing,Face recognition,Image databases,Image recognition,Mouth,Robustness,SVM classifier,Solid modeling,Support vector machine classification,Support vector machines,clustering,component-based approach,facial components,feature extraction,feature vector,global methods,learning automata},
number = {July},
pages = {688--694},
pmid = {20075471},
title = {{Face recognition with support vector machines: global versus component-based approach}},
url = {http://dx.doi.org/10.1109/ICCV.2001.937693},
volume = {2},
year = {2001}
}
@article{Keogh2004,
abstract = {The problem of indexing time series has attracted much interest. Most algorithms used to index time series utilize the Euclidean distance or some variation thereof. However, it has been forcefully shown that the Euclidean distance is a very brittle distance measure. Dy- namic time warping (DTW) is a much more robust distance measure for time series, allowing similar shapes to match even if they are out of phase in the time axis. Because of this flexi- bility, DTW is widely used in science, medicine, industry and finance. Unfortunately, however, DTW does not obey the triangular inequality and thus has resisted attempts at exact indexing. Instead, many researchers have introduced approximate indexing techniques or abandoned the idea of indexing and concentrated on speeding up sequential searches. In this work, we intro- duce a novel technique for the exact indexing of DTW. We prove that our method guarantees no false dismissals and we demonstrate its vast superiority over all competing approaches in the largest and most comprehensive set of time series indexing experiments ever undertaken.},
author = {Keogh, Eamonn and Ratanamahatana, Chotirat Ann},
doi = {10.1007/s10115-004-0154-9},
file = {:C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Keogh, Ratanamahatana - 2004 - Exact indexing of dynamic time warping.pdf:pdf},
issn = {0219-1377},
journal = {Knowledge and Information Systems},
keywords = {dynamic time warping,indexing,lower bounding,time series},
month = {may},
number = {3},
pages = {358--386},
title = {{Exact indexing of dynamic time warping}},
url = {http://www.springerlink.com/index/10.1007/s10115-004-0154-9},
volume = {7},
year = {2004}
}
@article{Torrence1998,
abstract = {A practical step-by-step guide to wavelet analysis is given, with examples taken from time series of the El Ni{\~{n}}oâSouthern Oscillation (ENSO). The guide includes a comparison to the windowed Fourier transform, the choice of an appropriate wavelet basis function, edge effects due to finite-length time series, and the relationship between wavelet scale and Fourier frequency. New statistical significance tests for wavelet power spectra are developed by deriving theoretical wavelet spectra for white and red noise processes and using these to establish significance levels and confidence intervals. It is shown that smoothing in time or scale can be used to increase the confidence of the wavelet spectrum. Empirical formulas are given for the effect of smoothing on significance levels and confidence intervals. Extensions to wavelet analysis such as filtering, the power Hovm{\"{o}}ller, cross-wavelet spectra, and coherence are described. The statistical significance tests are used to give a quantitative measure of changes in ENSO variance on interdecadal timescales. Using new datasets that extend back to 1871, the Ni{\~{n}}o3 sea surface temperature and the Southern Oscillation index show significantly higher power during 1880â1920 and 1960â90, and lower power during 1920â60, as well as a possible 15-yr modulation of variance. The power Hovm{\"{o}}ller of sea level pressure shows significant variations in 2â8-yr wavelet power in both longitude and time.},
author = {Torrence, Christopher and Compo, Gilbert P.},
doi = {10.1175/1520-0477(1998)079<0061:APGTWA>2.0.CO;2},
isbn = {0871706881},
issn = {00030007},
journal = {Bulletin of the American Meteorological Society},
number = {1},
pages = {61--78},
pmid = {21229804},
title = {{A Practical Guide to Wavelet Analysis}},
volume = {79},
year = {1998}
}
@article{Altman1992,
abstract = {Nonparametric regression is a set of techniques for es- timating a regression curve without making strong as- sumptions about the shape of the true regression func- tion. These techniques are therefore useful for building and checking parametric models, as well as for data description. Kernel and nearest-neighbor regression es- timators are local versions of univariate location esti- mators, and so they can readily be introduced to be- ginning students and consulting clients who are familiar with such summaries as the sample mean and median.},
author = {Altman, Ns},
doi = {10.1080/00031305.1992.10475879},
isbn = {0003-1305},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {Confidence intervals,Local linear re- gression,Model building,Model checking,Smoothing.},
number = {3},
pages = {175--185},
title = {{An introduction to kernel and nearest-neighbor nonparametric regression}},
volume = {46},
year = {1992}
}
@article{Jeong2011,
abstract = {Dynamic time warping (DTW), which finds the minimum path by providing non-linear alignments between two time series, has been widely used as a distance measure for time series classification and clustering. However, DTW does not account for the relative importance regarding the phase difference between a reference point and a testing point. This may lead to misclassification especially in applications where the shape similarity between two sequences is a major consideration for an accurate recognition. Therefore, we propose a novel distance measure, called a weighted DTW (WDTW), which is a penalty-based DTW. Our approach penalizes points with higher phase difference between a reference point and a testing point in order to prevent minimum distance distortion caused by outliers. The rationale underlying the proposed distance measure is demonstrated with some illustrative examples. A new weight function, called the modified logistic weight function (MLWF), is also proposed to systematically assign weights as a function of the phase difference between a reference point and a testing point. By applying different weights to adjacent points, the proposed algorithm can enhance the detection of similarity between two time series. We show that some popular distance measures such as DTW and Euclidean distance are special cases of our proposed WDTW measure. We extend the proposed idea to other variants of DTW such as derivative dynamic time warping (DDTW) and propose the weighted version of DDTW. We have compared the performances of our proposed procedures with other popular approaches using public data sets available through the UCR Time Series Data Mining Archive for both time series classification and clustering problems. The experimental results indicate that the proposed approaches can achieve improved accuracy for time series classification and clustering problems.},
author = {Jeong, Young-Seon and Jeong, Myong K. and Omitaomu, Olufemi A.},
doi = {10.1016/j.patcog.2010.09.022},
isbn = {0031-3203},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Adaptive weights,Dynamic time warping,Modified logistic weight function,Time series classification,Time series clustering,Weighted dynamic time warping},
number = {9},
pages = {2231--2240},
title = {{Weighted dynamic time warping for time series classification}},
url = {http://www.sciencedirect.com/science/article/pii/S003132031000484X},
volume = {44},
year = {2011}
}
@article{Prekopcsak2012,
abstract = {To classify time series by nearest neighbors, we need to specify or learn one or several distance measures. We consider variations of the Mahalanobis distance measures which rely on the inverse covariance matrix of the data. Unfortunately --- for time series data --- the covariance matrix has often low rank. To alleviate this problem we can either use a pseudoinverse, covariance shrinking or limit the matrix to its diagonal. We review these alternatives and benchmark them against competitive methods such as the related Large Margin Nearest Neighbor Classification (LMNN) and the Dynamic Time Warping (DTW) distance. As we expected, we find that the DTW is superior, but the Mahalanobis distance measures are one to two orders of magnitude faster. To get best results with Mahalanobis distance measures, we recommend learning one distance measure per class using either covariance shrinking or the diagonal approach.},
archivePrefix = {arXiv},
arxivId = {1010.1526},
author = {Prekopcs{\'{a}}k, Zolt{\'{a}}n and Lemire, Daniel},
doi = {10.1007/s11634-012-0110-6},
eprint = {1010.1526},
isbn = {1163401201106},
issn = {18625347},
journal = {Advances in Data Analysis and Classification},
keywords = {Distance measure learning,Mahalanobis distance measure,Nearest Neighbor,Time-series classification},
number = {3},
pages = {185--200},
title = {{Time series classification by class-specific Mahalanobis distance measures}},
volume = {6},
year = {2012}
}
@article{Hsu2008,
abstract = {The support vector machine (SVM) is a popular classi cation technique. However, beginners who are not familiar with SVM often get unsatisfactory results since they miss some easy but signi cant steps. In this guide, we propose a simple procedure which usually gives reasonable results. developed well-differentiated superficial transitional cell bladder cancer. CONCLUSIONS: Patients with SCI often prefer SPC than other methods offered to them, because of quality-of-life issues. The incidence of significant complications might not be as high as previously reported, and with a commitment to careful follow-up, SPC can be a safe option for carefully selected patients if adequate surveillance can be ensured.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Hsu, Chih-Wei and Chang, Chih-Chung and Lin, Chih-Jen},
doi = {10.1177/02632760022050997},
eprint = {0-387-31073-8},
file = {:C$\backslash$:/Users/SESA245227/Google Drive/Th{\`{e}}se/Bibliographie/SVM Librairie/A Practical Guide to Support Vector Classification.pdf:pdf},
isbn = {013805326X},
issn = {1464-410X},
journal = {BJU international},
number = {1},
pages = {1396--400},
pmid = {18190633},
title = {{A Practical Guide to Support Vector Classification}},
url = {http://www.csie.ntu.edu.tw/{~}cjlin/papers/guide/guide.pdf},
volume = {101},
year = {2008}
}
@article{Goldberger2004,
abstract = {In this paper we propose a novel method for learning a$\backslash$r$\backslash$nMahalanobis distance measure to be used in the KNN classification$\backslash$r$\backslash$nalgorithm. The algorithm directly maximizes a stochastic variant of$\backslash$r$\backslash$nthe leave-one-out KNN score on the training set. It can also$\backslash$r$\backslash$nlearn a low-dimensional linear embedding of labeled data that can$\backslash$r$\backslash$nbe used for data visualization and fast classification.$\backslash$r$\backslash$nUnlike other methods, our classification model is non-parametric,$\backslash$r$\backslash$nmaking no assumptions about the shape of the class distributions or$\backslash$r$\backslash$nthe boundaries between them.  The performance of the method$\backslash$r$\backslash$nis demonstrated on several data sets, both for metric learning and$\backslash$r$\backslash$nlinear dimensionality reduction.},
author = {Goldberger, Jacob and Roweis, Sam T and Hinton, Geoffrey E and Salakhutdinov, Ruslan and Roweis, Sam T and Salakhutdinov, Ruslan},
doi = {10.1.1.108.7841},
journal = {Advances in Neural Information Processing Systems},
keywords = {Learning/Statistics {\&} Optimisation,Theory {\&} Algorithms},
pages = {513--520},
title = {{Neighbourhood Components Analysis}},
url = {http://eprints.pascal-network.org/archive/00001570/},
year = {2004}
}
@article{Keogh2001a,
abstract = {this paper we address both these problems by introducing a modification$\backslash$nof DTW. The crucial difference is in the features we consider when$\backslash$nattempting to find the correct warping. Rather than use the raw data,$\backslash$nwe consider only the (estimated) local derivatives of the data},
author = {Keogh, E J and Pazzani, M J},
doi = {10.1137/1.9781611972719.1},
journal = {Proceedings of the 1st SIAM International Conference on Data Mining},
keywords = {dynamic time warping},
pages = {1--11},
title = {{Derivative Dynamic Time Warping}},
year = {2001}
}
@article{Berndt1994b,
abstract = {Knowledge discovery in databases presents many interesting challenges within the context of providing computer tools for exploring large data archives. Electronic data repositories are growing qulckiy and contain data from commercial, scientific, and other domains. Much of this data is inherently temporal, such as stock prices or NASA telemetry data. Detecting patterns in such data streams or time series is an important knowledge discovery task. This paper describes some primary experiments with a dynamic programming approach to the problem. The pattern detection algorithm is based on the dynamic time warping technique used in the speech recognition field. Keywords: dynamic programming, dynamic time warping, knowledge discovery, pattern analysis, time series.},
author = {Berndt, Donald and Clifford, James},
journal = {Workshop on Knowledge Knowledge Discovery in Databases},
keywords = {dynamic programming,dynamic time warping,knowledge discovery,pat,tern analysis,time series},
pages = {359--370},
title = {{Using dynamic time warping to find patterns in time series}},
url = {http://www.aaai.org/Papers/Workshops/1994/WS-94-03/WS94-03-031.pdf},
volume = {398},
year = {1994}
}
@article{Denoeux1995,
abstract = {In this paper, the problem of classifying an unseen pattern on the basis of its nearest neighbors in a recorded data set is addressed from the point of view of Dempster-Shafer theory. Each neighbor of a sample to be classified is considered as an item of evidence that supports certain hypotheses regarding the class membership of that pattern. The degree of support is defined as a function of the distance between the two vectors. The evidence of the k nearest neighbors is then pooled by means of Dempster's rule of combination. This approach provides a global treatment of such issues as ambiguity and distance rejection, and imperfect knowledge regarding the class membership of training patterns. The effectiveness of this classification scheme as compared to the voting and distance-weighted k-NN procedures is demonstrated using several sets of simulated and real-world data},
author = {Denoeux, T.},
doi = {10.1109/21.376493},
file = {:C$\backslash$:/Users/SESA245227/Desktop/smc95.pdf:pdf},
issn = {00189472},
journal = {IEEE Transactions on Systems, Man, and Cybernetics},
keywords = {Dempster's rule of combination,Dempster-Shafer theory,Density functional theory,Error analysis,H infinity control,Medical services,Nearest neighbor searches,Neural networks,Voting,ambiguity,class membership,distance rejection,distance-weighted k-NN procedures,evidence,imperfect knowledge,inference mechanisms,k-nearest neighbor classification rule,pattern classification,statistical analysis,unseen pattern classification,voting},
number = {5},
pages = {804--813},
title = {{A k-nearest neighbor classification rule based on Dempster-Shafer theory}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=376493},
volume = {25},
year = {1995}
}
@article{Salvador,
abstract = {The dynamic time warping (DTW) algorithm is able to find the optimal alignment between two time series. It is often used to determine time series similarity, classification, and to find corresponding regions between two time series. DTW has a quadratic time and space complexity that limits its use to only small time series data sets. In this paper we introduce FastDTW, an approximation of DTW that has a linear time and space complexity. FastDTW uses a multilevel approach that recursively projects a solution from a coarse resolution and refines the projected solution. We prove the linear time and space complexity of FastDTW both theoretically and empirically. We also analyze the accuracy of FastDTW compared to two other existing approximate DTW algorithms: Sakoe-Chuba Bands and Data Abstraction. Our results show a large improvement in accuracy over the existing methods.},
author = {Salvador, Stan and Chan, Philip},
file = {:C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Salvador, Chan - Unknown - FastDTW Toward Accurate Dynamic Time Warping in Linear Time and Space.pdf:pdf},
keywords = {dynamic time warping,time series},
title = {{FastDTW : Toward Accurate Dynamic Time Warping in Linear Time and Space}}
}
@article{Dudani1976,
abstract = {Among the simplest and most intuitively appealing classes of nonprobabilistic classification procedures are those that weight the evidence of nearby sample observations most heavily. More specifically, one might wish to weight the evidence of a neighbor close to an unclassified observation more heavily than the evidence of another neighbor which is at a greater distance from the unclassified observation. One such classification rule is described which makes use of a neighbor weighting function for the purpose of assigning a class to an unclassified sample. The admissibility of such a rule is also considered.},
author = {Dudani, Sahibsingh a.},
doi = {10.1109/TSMC.1976.5408784},
isbn = {0018-9472},
issn = {00189472},
journal = {IEEE Transactions on Systems, Man and Cybernetics},
number = {4},
pages = {325--327},
title = {{DISTANCE-WEIGHTED k-NEAREST-NEIGHBOR RULE.}},
volume = {SMC-6},
year = {1976}
}
@article{Bellet2012,
abstract = {Similarity functions are a fundamental component of many learning algorithms. When dealing with string or tree-structured data, measures based on the edit distance are widely used, and there exist a few methods for learning them from data. However, these methods offer no theoretical guarantee as to the generalization ability and discriminative power of the learned similarities. In this paper, we propose an approach to edit similarity learning based on loss minimization, called GESL. It is driven by the notion of (Ïµ,$\gamma$,$\tau$)-goodness, a theory that bridges the gap between the properties of a similarity function and its performance in classification. Using the notion of uniform stability, we derive generalization guarantees that hold for a large class of loss functions. We also provide experimental results on two real-world datasets which show that edit similarities learned with GESL induce more accurate and sparser classifiers than other (standard or learned) edit similarities.},
author = {Bellet, Aur{\'{e}}lien and Habrard, Amaury and Sebban, Marc},
doi = {10.1007/s10994-012-5293-8},
isbn = {0885-6125},
issn = {0885-6125, 1573-0565},
journal = {Machine Learning},
number = {1-2},
pages = {5--35},
title = {{Good edit similarity learning by loss minimization}},
url = {http://link.springer.com/article/10.1007/s10994-012-5293-8$\backslash$nhttp://link.springer.com/article/10.1007{\%}2Fs10994-012-5293-8$\backslash$nhttp://link.springer.com/content/pdf/10.1007{\%}2Fs10994-012-5293-8.pdf},
volume = {89},
year = {2012}
}
@article{Sakoe1978b,
abstract = {This paper reports on an optimum dynamic programming (DP) based time-normalization algorithm for spoken word recognition. First, a general principle of time-normalization is given using timewarping function. Then, two time-normalized distance definitions, d e d symmetric and asymmetric forms, are derived from the principle. These two forms are compared with each other through theoretical discussions and experimental studies. The symmetric form algorithm superiority is established. A new technique, called slope constraint, is successfully introduced, iwn hich the warping function slope isr estricted so as to improve discrimination between words in different categories. The effective slope constraint characteristic is qualitatively analyzed, and the optimum slope constraint condition is determined through experiments. The optimized algorithm is then extensively subjected to experimentat comparison with various DP-algorithms, previously applied to spoken word recognition by different research groups. The experiment shows that the present algorithm gives no more than about twothirds errors, even compared to the best conventional algorithm.},
author = {Sakoe, Hiroaki and Chiba, Seibi},
doi = {10.1109/TASSP.1978.1163055},
isbn = {1558601244},
issn = {00963518},
journal = {IEEE transactions on acoustics, speech, and signal processing},
number = {1},
pages = {43--49},
title = {{Dynamic Programming Algorithm Optimization for Spoken Word Recognition}},
volume = {ASSP-26},
year = {1978}
}
@article{Cortes1995,
abstract = {The support-vector network is a new leaming machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high- dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data. High generalization ability of support-vector networks utilizing polynomial input transformations is demon- strated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Cortes, Corinna and Vapnik, Vladimir},
doi = {10.1007/BF00994018},
eprint = {arXiv:1011.1669v3},
isbn = {0885-6125},
issn = {08856125},
journal = {Machine Learning},
keywords = {efficient learning algorithms,neural networks,pattern recognition,polynomial classifiers,radial basis function classifiers},
number = {3},
pages = {273--297},
pmid = {9052598814225336358},
title = {{Support-vector networks}},
volume = {20},
year = {1995}
}
@article{Abraham2010b,
abstract = {Zero-inflated time series data are commonly encountered in many applications, including climate and ecological modeling, disease monitoring, manufacturing defect detection, and traffic monitoring. Such data often leads to poor model fitting using standard regression methods because they tend to underestimate the frequency of zeros and the magnitude of non-zero values. This paper presents an integrated framework that simultaneously performs classification and regression to accurately predict future values of a zero-inflated time series. A regression model is initially applied to predict the value of the time series. The regression output is then fed into a classification model to determine whether the predicted value should be adjusted to zero. Our regression and classification models are trained to optimize a joint objective function that considers both classification errors on the time series and regression errors on data points that have non-zero values. We demonstrate the effectiveness of our framework in the context of its application to a precipitation downscaling problem for climate impact assessment studies. Read More: http://epubs.siam.org/doi/abs/10.1137/1.9781611972801.57},
author = {Abraham, Zubin and Tan, PN},
isbn = {978-0-89871-703-7},
journal = {Proc of the ACM SIGKDD Int'l Conf on Data Mining},
pages = {653--664},
title = {{An Integrated Framework for Simultaneous Classification and Regression of Time-Series Data}},
url = {https://siam.org/proceedings/datamining/2010/dm10{\_}057{\_}abrahamz.pdf},
year = {2010}
}
@article{Belongie2002,
abstract = {We present a novel approach to measuring similarity between shapes and exploit it for object recognition. In our  framework, the measurement of similarity is preceded by 1) solving for correspondences between points on the two shapes, 2) using  the correspondences to estimate an aligning transform. In order to solve the correspondence problem, we attach a descriptor, the  shape context, to each point. The shape context at a reference point captures the distribution of the remaining points relative to it, thus  offering a globally discriminative characterization. Corresponding points on two similar shapes will have similar shape contexts,  enabling us to solve for correspondences as an optimal assignment problem. Given the point correspondences, we estimate the  transformation that best aligns the two shapes; regularized thin-plate splines provide a flexible class of transformation maps for this  purpose. The dissimilarity between the two shapes is computed as a sum of matching errors between corresponding points, together  with a term measuring the magnitude of the aligning transform. We treat recognition in a nearest-neighbor classification framework as  the problem of finding the stored prototype shape that is maximally similar to that in the image. Results are presented for silhouettes,  trademarks, handwritten digits, and the COIL data set.},
author = {Belongie, Serge and Malik, Jitendra and Puzicha, Jan},
doi = {10.1.1.18.8852},
isbn = {9781424455409},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
pages = {509--522},
pmid = {15376597},
title = {{Shape Matching and Object Recognition Using Shape Contexts}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.18.8852},
volume = {24},
year = {2002}
}
@article{Cao2001,
author = {Cao, Lijuan and Tay, Francis E H},
file = {:C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cao, Tay - 2001 - Financial Forecasting Using Support Vector Machines.pdf:pdf},
journal = {Neural Computing {\&} Applications},
keywords = {back propagation algorithm,financial,generalisation,multi-layer,perceptron,support vector machines,time series forecasting},
pages = {184--192},
title = {{Financial Forecasting Using Support Vector Machines}},
year = {2001}
}
@article{Chouakria2007,
abstract = {Abstract The most widely used measures of time series proximity are the Euclidean distance and dynamic time warping. The latter can be derived from the distance introduced by Maurice Frchet in 1906 to account for the proximity between curves. The major limitation of these proximity measures is that they are based on the closeness of the values regardless of the similarity w.r.t. the growth behavior of the time series. To alleviate this drawback we propose a new dissimilarity index, based on an automatic adaptive tuning function, to include both proximity measures w.r.t. values and w.r.t. behavior. A comparative numerical analysis between the proposed index and the classical distance measures is performed on the basis of two datasets: a synthetic dataset and a dataset from a public health study.},
author = {Douzal-Chouakria, A. and Nagabhushan, P.},
doi = {10.1007/s11634-006-0004-6},
issn = {18625347},
journal = {Advances in Data Analysis and Classification},
keywords = {Classification,Dynamic time warping,Fr??chet distance,Time Series},
title = {{Adaptive dissimilarity index for measuring time series proximity}},
year = {2007}
}
@article{Diaz2010,
abstract = {One key point in cluster analysis is to determine a similarity or dissimilarity measure between data objects. When working with time series, the concept of similarity can be established in different ways. In this paper, several non-parametric statistics originally designed to test the equality of the log-spectra of two stochastic processes are proposed as dissimilarity measures between time series data. Their behavior in time series clustering is analyzed throughout a simulation study, and compared with the performance of several model-free and model-based dissimilarity measures. Up to three different classification settings were considered: (i) to distinguish between stationary and non-stationary time series, (ii) to classify different ARMA processes and (iii) to classify several non-linear time series models. As it was expected, the performance of a particular dissimilarity metric strongly depended on the type of processes subjected to clustering. Among all the measures studied, the non-parametric distances showed the most robust behavior.},
author = {D{\'{\i}}az, Sonia P{\'{e}}rtega and Vilar, Jos{\'{e}} A.},
doi = {10.1007/s00357-010-9064-6},
issn = {0176-4268},
journal = {Journal of Classification},
keywords = {ARMA processes,Dissimilarity measures,Local linear regression,Non-linear processes,Stationary and non-stationary processes,Time series clustering},
number = {3},
pages = {333--362},
title = {{Comparing Several Parametric and Nonparametric Approaches to Time Series Clustering: A Simulation Study}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84871904170{\&}partnerID=tZOtx3y1},
volume = {27},
year = {2010}
}
@article{Fan2008,
abstract = {LIBLINEAR is an open source library for large-scale linear classification. It supports logistic regression and linear support vector machines. We provide easy-to-use command-line tools and library calls for users and developers. Comprehensive documents are available for both beginners and advanced users. Experiments demonstrate that LIBLINEAR is very efficient on large sparse data sets.},
author = {Fan, RE and Chang, KW and Hsieh, CJ},
doi = {10.1038/oby.2011.351},
isbn = {089791497X},
issn = {15324435},
journal = {The Journal of Machine Learning},
keywords = {large-scale linear classification,logistic regression,machine learning,open,source,support vector machines},
pmid = {22173572},
title = {{LIBLINEAR: A library for large linear classification}},
url = {http://dl.acm.org/citation.cfm?id=1442794},
year = {2008}
}
@article{Kijsirikul2002,
abstract = {Presents a method of extending support vector machines (SVMs) for dealing with multiclass problems. Motivated by the decision directed acyclic graph (DDAG), we propose the adaptive DAG (ADAG): a modified structure of the DDAG that has a lower number of decision levels and reduces the dependency on the sequence of nodes. Thus, the ADAG improves the accuracy of the DDAG while maintaining low computational requirement },
author = {Kijsirikul, B and Ussivakul, N},
doi = {10.1109/IJCNN.2002.1005608},
journal = {Neural Networks, 2002. IJCNN '02. Proceedings of the 2002 International Joint Conference on},
keywords = {decision directed acyclic graph,decision levels,directed graphs,learning (artificial intelligence),learning automata,linear support vector machines,multiclass support vector machines,pattern classification,probability adaptive directed acyclic graph},
pages = {980--985},
title = {{Multiclass Support Vector Machines using Adaptive Directed Acyclic Graph}},
volume = {1},
year = {2002}
}
@book{Campbell2011,
author = {Campbell, Colin and Ying, Yiming},
booktitle = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
doi = {10.2200/S00324ED1V01Y201102AIM010},
file = {:C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Campbell, Ying - 2011 - Learning with Support Vector Machines.pdf:pdf},
isbn = {9781608456161},
issn = {1939-4608},
month = {feb},
number = {1},
pages = {1--95},
title = {{Learning with Support Vector Machines}},
url = {http://www.morganclaypool.com/doi/abs/10.2200/S00324ED1V01Y201102AIM010},
volume = {5},
year = {2011}
}
@article{Duin2012,
abstract = {Human experts constitute pattern classes of natural objects based on their observed appearance. Automatic systems for pattern recognition may be designed on a structural description derived from sensor observations. Alternatively, training sets of examples can be used in statistical learning procedures. They are most powerful for vectorial object representations. Unfortunately, structural descriptions do not match well with vectorial representations. Consequently it is difficult to combine the structural and statistical approaches to pattern recognition. Structural descriptions may be used to compare objects. This leads to a set of pairwise dissimilarities from which vectors can be derived for the purpose of statistical learning. The resulting dissimilarity representation bridges thereby the structural and statistical approaches. The dissimilarity space is one of the possible spaces resulting from this representation. It is very general and easy to implement. This paper gives a historical review and discusses the properties of the dissimilarity space approaches illustrated by a set of examples on real world datasets. Â© 2011 Elsevier B.V. All rights reserved.},
author = {Duin, Robert P W and P{\c{c}}kalska, Elbieta},
doi = {10.1016/j.patrec.2011.04.019},
isbn = {0167-8655},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Dissimilarity representation,Dissimilarity space,Representation set,Structural pattern recognition,Vector space},
number = {7},
pages = {826--832},
title = {{The dissimilarity space: Bridging structural and statistical pattern recognition}},
volume = {33},
year = {2012}
}
@article{Shental2002,
abstract = {We present a method for training a similarity metric from data. The method can be used for recognition or verification applications where the number of categories is very large and not known during training, and where the number of training samples for a single category is very small. The idea is to learn a function that maps input patterns into a target space such that the L1 norm in the target space approximates the "semantic" distance in the input space. The method is applied to a face verification task. The learning process minimizes a discriminative loss function that drives the similarity metric to be small for pairs of faces from the same person, and large for pairs from different persons. The mapping from raw to the target space is a convolutional network whose architecture is designed for robustness to geometric distortions. The system is tested on the Purdue/AR face database which has a very high degree of variability in the pose, lighting, expression, position, and artificial occlusions such as dark glasses and obscuring scarves.},
author = {Shental, Noam and Hertz, Tomer and Weinshall, Daphna and Pavel, Misha},
doi = {10.1007/3-540-47979-1{\_}52},
isbn = {978-3-540-43748-2},
journal = {European Conference on Computer Vision (ECCV)},
pages = {776--790},
title = {{Adjustment Learning and Relevant Component Analysis}},
url = {http://dx.doi.org/10.1007/3-540-47979-1{\_}52},
volume = {2353},
year = {2002}
}
@article{Zhang2010,
abstract = {Distance metric learning plays a very crucial role in many data mining algorithms because the performance of an algorithm relies heavily on choosing a good metric. However, the labeled data available in many applications is scarce and hence the metrics learned are often unsatisfactory. In this paper, we consider a transfer learning setting in which some related source tasks with labeled data are available to help the learning of the target task. We first propose a convex formulation for multi-task metric learning by modeling the task relationships in the form of a task covariance matrix. Then we regard transfer learning as a special case of multitask learning and adapt the formulation of multi-task metric learning to the transfer learning setting for our method, called transfer metric learning (TML). In TML, we learn the metric and the task covariances between the source tasks and the target task under a unified convex formulation. To solve the convex optimization problem, we use an alternating method in which each subproblem has an efficient solution. Experimental results on some commonly used transfer learning applications demonstrate the effectiveness of our method. Â© 2010 ACM.},
author = {Zhang, Yu and Yeung, Dit-Yan},
doi = {10.1145/1835804.1835954},
isbn = {9781450300551},
journal = {Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '10},
keywords = {metric learning,multi-task learning,transfer learning},
pages = {1199},
title = {{Transfer metric learning by learning task relationships}},
url = {http://dl.acm.org/citation.cfm?doid=1835804.1835954},
year = {2010}
}
@article{Liang2012,
abstract = {Most data stream classification algorithms need to supply input with a large amount of precisely labeled data. However, in many data stream applications, streaming data contains inherent uncertainty, and labeled samples are difficult to be collected, while abundant data are unlabeled. In this paper, we focus on classifying uncertain data streams with only positive and unlabeled samples available. Based on concept-adapting very fast decision tree (CVFDT) algorithm, we propose an algorithm namely puuCVFDT (CVFDT for positive and unlabeled uncertain data). Experimental results on both synthetic and real-life datasets demonstrate the strong ability and efficiency of puuCVFDT to handle concept drift with uncertainty under positive and unlabeled learning scenario. Even when 90{\%} of the samples in the stream are unlabeled, the classification performance of the proposed algorithm is still compared to that of CVFDT, which is learned from fully labeled data without uncertainty. ?? 2012 Elsevier Inc. All rights reserved.},
author = {Liang, Chunquan and Zhang, Yang and Shi, Peng and Hu, Zhengguo},
doi = {10.1016/j.ins.2012.05.023},
file = {:C$\backslash$:/Users/SESA245227/Google Drive/Th{\`{e}}se/Bibliographie/Decision Tree/Liang-Elsevier-2012{\_}Learning very fast decision tree from uncertain data streams with positive and unlabeled samples.pdf:pdf},
issn = {00200255},
journal = {Information Sciences},
keywords = {Positive unlabeled learning,Uncertain attribute,Uncertain data stream,Very fast decision tree},
pages = {50--67},
publisher = {Elsevier Inc.},
title = {{Learning very fast decision tree from uncertain data streams with positive and unlabeled samples}},
url = {http://dx.doi.org/10.1016/j.ins.2012.05.023},
volume = {213},
year = {2012}
}
@article{DUrso2009,
abstract = {The traditional approaches to clustering a set of time series are generally applicable if there is a fixed underlying structure to the time series so that each will belong to one cluster or the other. However, time series often display dynamic behaviour in their evolution over time. This dynamic behaviour should be taken into account when attempting to cluster time series. For instance, during a certain period, a time series might belong to a certain cluster; afterwards its dynamics might be closer to that of another cluster. In this case, the traditional clustering approaches are unlikely to find and represent the underlying structure in the given time series. This switch from one time state to another, which is typically vague, can be naturally treated following a fuzzy approach. This paper proposes a fuzzy clustering approach based on the autocorrelation functions of time series, in which each time series is not assigned exclusively to only one cluster, but it is allowed to belong to different clusters with various membership degrees. ?? 2009 Elsevier B.V. All rights reserved.},
author = {D'Urso, Pierpaolo and Maharaj, Elizabeth Ann},
doi = {10.1016/j.fss.2009.04.013},
isbn = {0165-0114},
issn = {01650114},
journal = {Fuzzy Sets and Systems},
keywords = {Autocorrelation function,Crisp C-means clustering,Fuzzy C-means clustering,Switching time series,Time series},
number = {24},
pages = {3565--3589},
title = {{Autocorrelation-based fuzzy clustering of time series}},
volume = {160},
year = {2009}
}
@article{Faloutsos1994,
abstract = {We present an ecient indexing method to locate 1-dimensional subsequences within a collection of sequences, such that the subsequences match a given (query) pattern within a specified tolerance. The idea is to map each data sequence into a small set of multidimensional rectangles in feature space. Then, these rectangles can be readily indexed using traditional spatial access methods, like the R*-tree [9]. In more detail, we use a sliding window over the data sequence and extract its features; the result is a trail in feature space. We propose an efficient and effective algorithm to divide such trails into sub-trails, which are subsequently represented by their Minimum Bounding Rectangles (MBRs). We also examine queries of varying lengths, and we show how to handle each case eciently. We implemented our method and carried out experiments on synthetic and real data (stock price movements). We compared the method to sequential scanning, which is the only obvious competitor. The results were excellent: our method accelerated the search time from 3 times up to 100 times.},
author = {Faloutsos, Christos and Ranganathan, M. and Manolopoulos, Yannis},
doi = {10.1145/191843.191925},
isbn = {0897916395},
issn = {01635808},
journal = {ACM SIGMOD Record},
number = {2},
pages = {419--429},
title = {{Fast subsequence matching in time-series databases}},
url = {http://portal.acm.org/citation.cfm?doid=191843.191925},
volume = {23},
year = {1994}
}
@article{Son2008,
abstract = {Gene expression levels are often measured consecutively in time through microarray experiments to detect cellular processes underlying regulatory effects observed and to assign functionality to genes whose function is yet unknown. Clustering methods allow us to group genes that show similar time-course expression profiles and that are thus likely to be co-regulated. The correlation coefficient, the most well-liked similarity measure in the context of gene expression data, is not very reliable in representing the association of two temporal profile patterns. Moreover, the clustering methods with the correlation coefficient generate the same clustering result even when the time points are permuted arbitrarily. We propose a new similarity measure for clustering time-course gene expression data. The proposed measure is based on the correlation coefficient and the two indices representing the concordance of temporal profile patterns and that of the time points at which maximum and minimum expression levels are measured between two profiles, respectively. We applied the hierarchical clustering method with the proposed similarity measure to both synthetic and breast cancer cell line data. We observed favorable results compared to the correlation coefficient based method. The proposed similarity measure is simple to implement, and it is much more consistent for clustering than the correlation coefficient based method according to the cross-validation criterion.},
author = {Son, Young Sook and Baek, Jangsun},
doi = {10.1016/j.patrec.2007.09.015},
isbn = {0167-8655},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Clustering,Modified correlation coefficient,Pearsonâs correlation coefficient,Similarity,Spearmannâs correlation coefficient,Time-course gene expression data},
number = {3},
pages = {232--242},
title = {{A modified correlation coefficient based similarity measure for clustering time-course gene expression data}},
url = {http://www.sciencedirect.com/science/article/pii/S0167865507003005},
volume = {29},
year = {2008}
}
@incollection{Yang1999,
abstract = {This paper reports a controlled study with statistical signifi cance tests on five text categorization methods: the Support Vector Machines (SVM), a kNearest Neighbor (kNN) clas sifier, a neural network (NNet) approach, the Linear Least squares Fit (LLSF) mapping and a Naive Bayes (NB) classi fier. We focus on the robustness of these methods in dealing with a skewed category distribution, and their performance as function of the trainingset category frequency. Our re sults show that SVM, kNN and LLSF significantly outper form NNet and NB when the number of positive training instances per category are small (less than ten), and that all the methods perform comparably when the categories are sufficiently common (over 300 instances).},
author = {Yang, Yiming and Liu, Xin},
booktitle = {Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval SIGIR 99},
doi = {10.1145/312624.312647},
isbn = {1581130961},
pages = {42--49},
title = {{A re-examination of text categorization methods}},
year = {1999}
}
@article{Sahidullah2012,
abstract = {Standard Mel frequency cepstrum coefficient (MFCC) computation technique utilizes discrete cosine transform (DCT) for decorrelating log energies of filter bank output. The use of DCT is reasonable here as the covariance matrix of Mel filter bank log energy (MFLE) can be compared with that of highly correlated Markov-I process. This full-band based MFCC computation technique where each of the filter bank output has contribution to all coefficients, has two main disadvantages. First, the covariance matrix of the log energies does not exactly follow Markov-I property. Second, full-band based MFCC feature gets severely degraded when speech signal is corrupted with narrow-band channel noise, though few filter bank outputs may remain unaffected. In this work, we have studied a class of linear transformation techniques based on block wise transformation of MFLE which effectively decorrelate the filter bank log energies and also capture speech information in an efficient manner. A thorough study has been carried out on the block based transformation approach by investigating a new partitioning technique that highlights associated advantages. This article also reports a novel feature extraction scheme which captures complementary information to wide band information; that otherwise remains undetected by standard MFCC and proposed block transform (BT) techniques. The proposed features are evaluated on NIST SRE databases using Gaussian mixture model-universal background model (GMM-UBM) based speaker recognition system. We have obtained significant performance improvement over baseline features for both matched and mismatched condition, also for standard and narrow-band noises. The proposed method achieves significant performance improvement in presence of narrow-band noise when clubbed with missing feature theory based score computation scheme. ?? 2011 Elsevier B.V. All rights reserved.},
author = {Sahidullah, Md and Saha, Goutam},
doi = {10.1016/j.specom.2011.11.004},
isbn = {0167-6393},
issn = {01676393},
journal = {Speech Communication},
keywords = {Block transform,Correlation matrix,DCT,Decorrelation technique,Linear transformation,MFCC,Missing feature theory,Narrow-band noise,Speaker recognition},
number = {4},
pages = {543--565},
title = {{Design, analysis and experimental evaluation of block based transformation in MFCC computation for speaker recognition}},
url = {http://dx.doi.org/10.1016/j.specom.2011.11.004},
volume = {54},
year = {2012}
}
@inproceedings{Xi2006a,
abstract = {Many algorithms have been proposed for the problem of time series classification. However, it is clear that one-nearest-neighbor with Dynamic Time Warping (DTW) distance is exceptionally difficult to beat. This approach has one weakness, however; it is computationally too demanding for many realtime applications. One way to mitigate this problem is to speed up the DTW calculations. Nonetheless, there is a limit to how much this can help. In this work, we propose an additional technique, numerosity reduction, to speed up one-nearest-neighbor DTW. While the idea of numerosity reduction for nearest-neighbor classifiers has a long history, we show here that we can leverage off an original observation about the relationship between dataset size and DTW constraints to produce an extremely compact dataset with little or no loss in accuracy. We test our ideas with a comprehensive set of experiments, and show that it can efficiently produce extremely fast accurate classifiers.},
author = {Xi, Xiaopeng and Keogh, Eamonn and Shelton, Christian and Wei, Li and Ratanamahatana, Chotirat Ann},
booktitle = {Proceedings of the 23rd international conference on Machine learning (ICML)},
doi = {10.1145/1143844.1143974},
isbn = {1595933832},
pages = {1033----1040},
title = {{Fast time series classification using numerosity reduction}},
url = {http://dl.acm.org/citation.cfm?id=1143974},
year = {2006}
}
@book{Rabiner1993,
abstract = {Provides a theoretically sound, technically accurate, and complete description of the basic knowledge and ideas that constitute a modern system for speech recognition by machine. Covers production, perception, and acoustic-phonetic characterization of the speech signal; signal processing and analysis methods for speech recognition; pattern comparison techniques; speech recognition system design and implementation; theory and implementation of hidden Markov models; speech recognition based on connected word models; large vocabulary continuous speech recognition; and task- oriented application of automatic speech recognition. For practicing engineers, scientists, linguists, and programmers interested in speech recognition.},
author = {Rabiner, L. and Juang, B.},
booktitle = {Prentice Hall},
doi = {10.1002/ev.1647},
isbn = {0130151572},
title = {{Fundamentals of Speech Recognition}},
url = {http://cmp.felk.cvut.cz/cmp/support/phd112.html},
volume = {103},
year = {1993}
}
@article{Rydell2008a,
abstract = {Correlation is often used to measure the similarity between signals and is an important tool in signal and image processing. In some applications it is common that signals are corrupted by local bursts of noise. This adversely affects the performance of signal recognition algorithms. This paper presents a novel correlation estimator, which is robust to locally corrupted signals. The estimator is generalized to multivariate correlation analysis (general linear model, GLM, and canonical correlation analysis, CCA). Synthetic functional MRI data is used to demonstrate the estimator, and its robustness is shown to increase the performance of signal detection.},
author = {Rydell, Joakim and Borga, Magnus and Knutsson, Hans},
doi = {10.1109/ICASSP.2008.4517644},
isbn = {1424414849},
issn = {15206149},
journal = {Acoustics, Speech and Signal {\ldots}},
keywords = {biomedical MRI,canonical correlation analysis,correlation estimator,correlation methods,general linear model,locally corrupted signals,medical signal processing,multivariate correlation analysis,robust correlation analysis,signal detection,synthetic functional MRI data},
pages = {453--456},
title = {{Robust correlation analysis with an application to functional MRI}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4517644},
year = {2008}
}
@inproceedings{Najmeddine2012,
author = {Najmeddine, H. and Jay, A. and Marechal, P. and Mari{\'{e}}, S.},
booktitle = {RFIA},
file = {:C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Najmeddine et al. - 2012 - Mesures de similarit{\'{e}} pour lâaide {\`{a}} lâanalyse des donn{\'{e}}es {\'{e}}nerg{\'{e}}tiques de b{\^{a}}timents.pdf:pdf},
isbn = {9782953951523},
keywords = {Data mining,INCAS.,Time series,diagnosis and decision support,sensors,similarity measures},
title = {{Mesures de similarit{\'{e}} pour l'aide {\`{a}} l'analyse des donn{\'{e}}es {\'{e}}nerg{\'{e}}tiques de b{\^{a}}timents}},
url = {https://hal-cea.archives-ouvertes.fr/file/index/docid/661016/filename/article53{\_}modif.pdf},
year = {2012}
}
@inproceedings{Nguyen2012,
abstract = {More and more people express their opinions on social media such as Facebook and Twitter. Predictive analysis on social media time-series allows the stake-holders to leverage this immediate, accessible and vast reachable communication channel to react and proact against the public opinion. In particular, understanding and predicting the sentiment change of the public opinions will allow business and government agencies to react against negative sentiment and design strategies such as dispelling rumors and post balanced messages to revert the public opinion. In this paper, we present a strategy of building statistical models from the social media dynamics to predict collective sentiment dynamics. We model the collective sentiment change without delving into micro analysis of individual tweets or users and their corresponding low level network structures. Experiments on large-scale Twitter data show that the model can achieve above 85{\%} accuracy on directional sentiment prediction.},
author = {Nguyen, L. and Wu, P. and Chan, W. and Peng, W. and Zhang, Y.},
booktitle = {WISDOM},
doi = {10.1145/2346676.2346682},
isbn = {9781450315432},
keywords = {sentiment analysis,sentiment prediction,social network analysis},
title = {{Predicting collective sentiment dynamics from time-series social media}},
year = {2012}
}
@article{Morse2007,
abstract = {A variety of techniques currently exist for measuring the similarity between time series datasets. Of these techniques, the methods whosematching criteria is bounded by a specified Ç« threshold value, such as the LCSS and the EDR techniques, have been shown to be robust in the presence of noise, time shifts, and data scaling. Our work proposes a new algorithm, called the Fast Time Series Evaluation (FTSE) method, which can be used to evaluate such threshold value techniques, including LCSS and EDR. Using FTSE,we show that these techniques can be evaluated faster than using either traditional dynamic programming or even warp-restricting methods such as the Sakoe-Chiba band and the Itakura Parallelogram. We also show that FTSE can be used in a framework that can evaluate a richer range of epsilon threshold-based scoring techniques, of which EDR and LCSS are just two examples. This framework, called Swale, extends the epsilon threshold-based scoring techniques to include arbitrary match rewards and gap penalties. Through extensive empirical evaluation, we show that Swale can obtain greater accuracy than existing methods.},
author = {Morse, Michael D and Patel, Jignesh M},
doi = {10.1145/1247480.1247544},
isbn = {9781595936868},
journal = {ACM SIGMOD international conference on Management of data},
keywords = {clustering,time series,trajectory similarity},
pages = {569},
title = {{An efficient and accurate method for evaluating time series similarity}},
url = {http://dl.acm.org/citation.cfm?id=1247544$\backslash$nhttp://portal.acm.org/citation.cfm?doid=1247480.1247544},
year = {2007}
}
@article{Kalman1960,
abstract = {The classical filtering and prediction problem is re-examined using the Bode- Shannon representation of random processes and the ``state transition{\&}apos;{\&}apos; method of analysis of dynamic systems. New results are: (1) The formulation and methods of solution of the problem apply without modifica- tion to stationary and nonstationary statistics and to growing-memory and infinite- memory filters. (2) A nonlinear difference (or differential) equation is derived for the covariance matrix of the optimal estimation error. From the solution of this equation the co- efficients of the difference (or differential) equation of the optimal linear filter are ob- tained without further calculations. (3) The filtering problem is shown to be the dual of the noise-free regulator problem. The new method developed here is applied to two well-known problems, confirming and extending earlier results. The discussion is largely self-contained and proceeds from first principles; basic concepts of the theory of random processes are reviewed in the Appendix.},
author = {Kalman, R E},
doi = {10.1115/1.3662552},
isbn = {9783540769897},
issn = {0021-9223},
journal = {Transactions of the ASME Journal of Basic Engineering},
number = {Series D},
pages = {35--45},
pmid = {5311910},
title = {{A New Approach to Linear Filtering and Prediction Problems}},
volume = {82},
year = {1960}
}
@article{Sakoe1978a,
abstract = {This paper reports on an optimum dynamic programming (DP) based time-normalization algorithm for spoken word recognition. First, a general principle of time-normalization is given using timewarping function. Then, two time-normalized distance definitions, d e d symmetric and asymmetric forms, are derived from the principle. These two forms are compared with each other through theoretical discussions and experimental studies. The symmetric form algorithm superiority is established. A new technique, called slope constraint, is successfully introduced, iwn hich the warping function slope isr estricted so as to improve discrimination between words in different categories. The effective slope constraint characteristic is qualitatively analyzed, and the optimum slope constraint condition is determined through experiments. The optimized algorithm is then extensively subjected to experimentat comparison with various DP-algorithms, previously applied to spoken word recognition by different research groups. The experiment shows that the present algorithm gives no more than about twothirds errors, even compared to the best conventional algorithm.},
author = {Sakoe, H. and Chiba, S.},
doi = {10.1109/TASSP.1978.1163055},
isbn = {1558601244},
issn = {00963518},
journal = {IEEE transactions on acoustics, speech, and signal processing},
title = {{Dynamic Programming Algorithm Optimization for Spoken Word Recognition}},
year = {1978}
}
@article{Oncina2006,
abstract = {Many pattern recognition algorithms are based on the nearest-neighbour search and use the well-known edit distance, for which the primitive edit costs are usually fixed in advance. In this article, we aim at learning an unbiased stochastic edit distance in the form of a finite-state transducer from a corpus of (input, output) pairs of strings. Contrary to the other standard methods, which generally use the Expectation Maximisation algorithm, our algorithm learns a transducer independently on the marginal probability distribution of the input strings. Such an unbiased way to proceed requires to optimise the parameters of a conditional transducer instead of a joint one. We apply our new model in the context of handwritten digit recognition. We show, carrying out a large series of experiments, that it always outperforms the standard edit distance. ?? 2006 Pattern Recognition Society.},
author = {Oncina, Jose and Sebban, Marc},
doi = {10.1016/j.patcog.2006.03.011},
isbn = {0031-3203},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Finite-state transducers,Handwritten character recognition,Stochastic edit distance},
pages = {1575--1587},
title = {{Learning stochastic edit distance: Application in handwritten character recognition}},
volume = {39},
year = {2006}
}
