Automatically generated by Mendeley Desktop 1.15.1
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Cao2001,
author = {Cao, Lijuan and Tay, Francis E H},
file = {:C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cao, Tay - 2001 - Financial Forecasting Using Support Vector Machines.pdf:pdf},
journal = {Neural Computing {\&} Applications},
keywords = {back propagation algorithm,financial,generalisation,multi-layer,perceptron,support vector machines,time series forecasting},
pages = {184--192},
title = {{Financial Forecasting Using Support Vector Machines}},
year = {2001}
}
@article{Altman1992,
abstract = {Nonparametric regression is a set of techniques for es- timating a regression curve without making strong as- sumptions about the shape of the true regression func- tion. These techniques are therefore useful for building and checking parametric models, as well as for data description. Kernel and nearest-neighbor regression es- timators are local versions of univariate location esti- mators, and so they can readily be introduced to be- ginning students and consulting clients who are familiar with such summaries as the sample mean and median.},
author = {Altman, Ns},
doi = {10.1080/00031305.1992.10475879},
isbn = {0003-1305},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {Confidence intervals,Local linear re- gression,Model building,Model checking,Smoothing.},
number = {3},
pages = {175--185},
title = {{An introduction to kernel and nearest-neighbor nonparametric regression}},
volume = {46},
year = {1992}
}
@book{Dreyfus2006,
abstract = {En une vingtaine d’ann{\'{e}}es, l’apprentissage artificiel est devenu une branche majeure des math{\'{e}}matiques appliqu{\'{e}}es, {\`{a}} l’intersection des statistiques et de l’intelligence artificielle. Son objectif est de r{\'{e}}aliser des mod{\`{e}}les qui apprennent « par l’exemple » : il s’appuie sur des donn{\'{e}}es num{\'{e}}riques (r{\'{e}}sultats de mesures ou de simulations), contrairement aux mod{\`{e}}les « de connaissances » qui s’appuient sur des {\'{e}}quations issues des premiers principes de la physique, de la chimie, de la biologie, de l’{\'{e}}conomie, etc. L’apprentis- sage statistique est d’une grande utilit{\'{e}} lorsque l’on cherche {\`{a}} mod{\'{e}}liser des processus complexes, souvent non lin{\'{e}}aires, pour lesquels les connaissances th{\'{e}}oriques sont trop impr{\'{e}}cises pour permettre des pr{\'{e}}dictions pr{\'{e}}cises. Ses domaines d’applications sont multiples : fouille de donn{\'{e}}es, bio-informatique, g{\'{e}}nie des proc{\'{e}}d{\'{e}}s, aide au diagnostic m{\'{e}}dical, t{\'{e}}l{\'{e}}communications, interface cerveau-machines, et bien d’autres. Cet ouvrage refl{\`{e}}te en partie l’{\'{e}}volution de cette discipline, depuis ses balbutiements au d{\'{e}}but des ann{\'{e}}es 1980, jusqu’{\`{a}} sa situation actuelle ; il n’a pas du tout la pr{\'{e}}tention de faire un point, m{\^{e}}me partiel, sur l’ensemble des d{\'{e}}veloppements pass{\'{e}}s et actuels, mais plut{\^{o}}t d’insister sur les principes et sur les m{\'{e}}thodes {\'{e}}prouv{\'{e}}s, dont les bases scientifiques sont s{\^{u}}res. Dans un domaine sans cesse parcouru de modes multiples et {\'{e}}ph{\'{e}}m{\`{e}}res, il est utile, pour qui cherche {\`{a}} acqu{\'{e}}rir les connaissances et principes de base, d’insister sur les aspects p{\'{e}}rennes du domaine. Cet ouvrage fait suite {\`{a}} R{\'{e}}seaux de neurones, m{\'{e}}thodologies et applications, des m{\^{e}}mes auteurs, paru en 2000, r{\'{e}}{\'{e}}dit{\'{e}} en 2004, chez le m{\^{e}}me {\'{e}}diteur, puis publi{\'{e}} en traduction anglaise chez Springer. Consacr{\'{e}} essentiellement aux r{\'{e}}seaux de neurones et aux cartes auto-adaptatives, il a largement contribu{\'{e}} {\`{a}} populariser ces techniques et {\`{a}} convaincre leurs utilisateurs qu’il est possible d’obtenir des r{\'{e}}sultats remarquables, {\`{a}} condition de mettre en {\oe}uvre une m{\'{e}}thodologie de conception rigoureuse, scientifique- ment fond{\'{e}}e, dans un domaine o{\`{u}} l’empirisme a longtemps tenu lieu de m{\'{e}}thode. Tout en restant fid{\`{e}}le {\`{a}} l’esprit de cet ouvrage, combinant fondements math{\'{e}}matiques et m{\'{e}}thodologie de mise en {\oe}uvre, les auteurs ont {\'{e}}largi le champ de la pr{\'{e}}sentation, afin de permettre au lecteur d’aborder d’autres m{\'{e}}thodes d’apprentissage statistique que celles qui sont directement d{\'{e}}crites dans cet ouvrage. En effet, les succ{\`{e}}s de l’apprentissage dans un grand nombre de domaines ont pouss{\'{e}} au d{\'{e}}veloppement de tr{\`{e}}s nombreuses variantes, souvent destin{\'{e}}es {\`{a}} r{\'{e}}pondre efficacement aux exigences de telle ou telle classe d’applications. Toutes ces variantes ont n{\'{e}}anmoins des bases th{\'{e}}oriques et des aspects m{\'{e}}thodolo- giques communs, qu’il est important d’avoir pr{\'{e}}sents {\`{a}} l’esprit. Le terme d’apprentissage, comme celui de r{\'{e}}seau de neurones, {\'{e}}voque {\'{e}}videmment le fonctionnement du cerveau. Il ne faut pourtant pas s’attendre {\`{a}} trouver ici d’explications sur les m{\'{e}}canismes de traitement des informations dans les syst{\`{e}}mes nerveux ; ces derniers sont d’une grande complexit{\'{e}}, r{\'{e}}sultant de processus {\'{e}}lectriques et chimiques subtils, encore mal compris en d{\'{e}}pit de la grande quantit{\'{e}} de donn{\'{e}}es exp{\'{e}}rimentales disponibles. Si les m{\'{e}}thodes d’apprentissage statistique peuvent {\^{e}}tre d’une grande utilit{\'{e}} pour cr{\'{e}}er des mod{\`{e}}les empiriques de telle ou telle fonction r{\'{e}}alis{\'{e}}e par le syst{\`{e}}me nerveux, celles qui sont d{\'{e}}crites dans cet ouvrage n’ont aucunement la pr{\'{e}}tention d’imiter, m{\^{e}}me vaguement, le fonctionne- ment du cerveau. L’apprentissage artificiel, notamment statistique, permettra-t-il un jour de donner aux ordinateurs des capacit{\'{e}}s analogues {\`{a}} celles des {\^{e}}tres humains ? Se rapprochera-t-on de cet objectif en perfectionnant les techniques actuelles d’apprentissage, ou bien des approches radicalement nouvelles sont-elles indispensables ? Faut-il s’inspirer de ce que l’on sait, ou croit savoir, sur le fonctionnement du cerveau ? Ces questions font l’objet de d{\'{e}}bats passionn{\'{e}}s, et passionnants, au sein de la communaut{\'{e}} scientifique : on n’en trouvera pas les r{\'{e}}ponses ici.},
author = {{G. Dreyfus, J.-M. Martinez, M. Samuelides M. B. Gordon, F. Badran}, S. Thiria},
edition = {Eyrolles},
file = {:C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/G. Dreyfus, J.-M. Martinez, M. Samuelides M. B. Gordon, F. Badran - 2006 - Apprentissage Apprentissage statistique.pdf:pdf},
isbn = {9782212114645},
keywords = {Bio-ing{\'{e}}nierie,Machine {\`{a}} Vecteurs Supports,Pr{\'{e}}vision,Reconaissance de formes,Robotique et commande de processus,R{\'{e}}seaux de neurones,cartes topologiques,data mining},
mendeley-tags = {Machine {\`{a}} Vecteurs Supports,R{\'{e}}seaux de neurones,cartes topologiques},
pages = {471},
title = {{Apprentissage Apprentissage statistique}},
year = {2006}
}
@article{Berndt1994b,
abstract = {Knowledge discovery in databases presents many interesting challenges within the context of providing computer tools for exploring large data archives. Electronic data repositories are growing qulckiy and contain data from commercial, scientific, and other domains. Much of this data is inherently temporal, such as stock prices or NASA telemetry data. Detecting patterns in such data streams or time series is an important knowledge discovery task. This paper describes some primary experiments with a dynamic programming approach to the problem. The pattern detection algorithm is based on the dynamic time warping technique used in the speech recognition field. Keywords: dynamic programming, dynamic time warping, knowledge discovery, pattern analysis, time series.},
author = {Berndt, Donald and Clifford, James},
journal = {Workshop on Knowledge Knowledge Discovery in Databases},
keywords = {dynamic programming,dynamic time warping,knowledge discovery,pat,tern analysis,time series},
pages = {359--370},
title = {{Using dynamic time warping to find patterns in time series}},
url = {http://www.aaai.org/Papers/Workshops/1994/WS-94-03/WS94-03-031.pdf},
volume = {398},
year = {1994}
}
@misc{Keller1985,
abstract = {Classification of objects is an important area of research and application in a variety of fields. In the presence of full knowledge of the underlying probabilities, Bayes decision theory gives optimal error rates. In those cases where this information is not present, many algorithms make use of distance or similarity among samples as a means of classification. The K-nearest neighbor decision rule has often been used in these pattern recognition problems. One of the difficulties that arises when utilizing this technique is that each of the labeled samples is given equal importance in deciding the class memberships of the pattern to be classified, regardless of their `typicalness'. The theory of fuzzy sets is introduced into the K-nearest neighbor technique to develop a fuzzy version of the algorithm. Three methods of assigning fuzzy memberships to the labeled samples are proposed, and experimental results and comparisons to the crisp version are presented.},
author = {Keller, James M. and Gray, Michael R. and Givens, James a.},
booktitle = {IEEE Transactions on Systems, Man, and Cybernetics},
doi = {10.1109/TSMC.1985.6313426},
isbn = {0018-9472},
issn = {0018-9472},
number = {4},
pages = {580--585},
title = {{A fuzzy K-nearest neighbor algorithm}},
volume = {SMC-15},
year = {1985}
}
@article{Kijsirikul2002,
abstract = {Presents a method of extending support vector machines (SVMs) for dealing with multiclass problems. Motivated by the decision directed acyclic graph (DDAG), we propose the adaptive DAG (ADAG): a modified structure of the DDAG that has a lower number of decision levels and reduces the dependency on the sequence of nodes. Thus, the ADAG improves the accuracy of the DDAG while maintaining low computational requirement },
author = {Kijsirikul, B and Ussivakul, N},
doi = {10.1109/IJCNN.2002.1005608},
journal = {Neural Networks, 2002. IJCNN '02. Proceedings of the 2002 International Joint Conference on},
keywords = {decision directed acyclic graph,decision levels,directed graphs,learning (artificial intelligence),learning automata,linear support vector machines,multiclass support vector machines,pattern classification,probability adaptive directed acyclic graph},
pages = {980--985},
title = {{Multiclass Support Vector Machines using Adaptive Directed Acyclic Graph}},
volume = {1},
year = {2002}
}
@article{Denoeux1995,
abstract = {In this paper, the problem of classifying an unseen pattern on the basis of its nearest neighbors in a recorded data set is addressed from the point of view of Dempster-Shafer theory. Each neighbor of a sample to be classified is considered as an item of evidence that supports certain hypotheses regarding the class membership of that pattern. The degree of support is defined as a function of the distance between the two vectors. The evidence of the k nearest neighbors is then pooled by means of Dempster's rule of combination. This approach provides a global treatment of such issues as ambiguity and distance rejection, and imperfect knowledge regarding the class membership of training patterns. The effectiveness of this classification scheme as compared to the voting and distance-weighted k-NN procedures is demonstrated using several sets of simulated and real-world data},
author = {Denoeux, T.},
doi = {10.1109/21.376493},
file = {:C$\backslash$:/Users/SESA245227/Desktop/smc95.pdf:pdf},
issn = {00189472},
journal = {IEEE Transactions on Systems, Man, and Cybernetics},
keywords = {Dempster's rule of combination,Dempster-Shafer theory,Density functional theory,Error analysis,H infinity control,Medical services,Nearest neighbor searches,Neural networks,Voting,ambiguity,class membership,distance rejection,distance-weighted k-NN procedures,evidence,imperfect knowledge,inference mechanisms,k-nearest neighbor classification rule,pattern classification,statistical analysis,unseen pattern classification,voting},
number = {5},
pages = {804--813},
title = {{A k-nearest neighbor classification rule based on Dempster-Shafer theory}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=376493},
volume = {25},
year = {1995}
}
@article{Lhermitte2011a,
abstract = {Time series of remote sensing imagery or derived vegetation indices and biophysical products have been shown particularly useful to characterize land ecosystem dynamics. Various methods have been developed based on temporal trajectory analysis to characterize, classify and detect changes in ecosystem dynamics. Although time series similarity measures play an important role in these methods, a quantitative comparison of the similarity measures is lacking. The objective of this study was to provide an overview and quantitative comparison of the similarity measures in function of varying time series and ecosystem characteristics, such as amplitude, timing and noise effects. For this purpose, the performance was evaluated for the commonly used similarity measures (D), ranging from Manhattan (DMan), Euclidean (DE) and Mahalanobis (DMah) distance measures, to correlation (DCC), Principal Component Analysis (PCA; DPCA) and Fourier based (DFFT,D$\xi$,DFk) similarities. The quantitative comparison consists of a series of Monte-Carlo simulations based on subsets of global MODIS Normalized Difference Vegetation index (NDVI) and Enhanced Vegetation Index (EVI) and Leaf Area Index (LAI) data. Results of the simulations reveal four main groups of time series similarity measures with different sensitivities: (i) DMan, DE, DPCA, DFk quantify the difference in time series values, (ii) DMah accounts for temporal correlation and non-stationarity of variance, (iii) DCC measures the temporal correlation, and (iv) the Fourier based DFFT and D$\xi$ show their specific sensitivity based on the selected Fourier components. The difference measures show relatively the highest sensitivity to amplitude effects, whereas the correlation based measures are highly sensitive to variations in timing and noise. The Fourier based measures, finally, depend highly on the signal to noise ratio and the balance between amplitude and phase dominance. The heterogeneity in sensitivity of each D stresses the importance of (i) understanding the time series characteristics before applying any classification of change detection approach and (ii) defining the variability one wants to identify/account for. This requires an understanding of the ecosystem dynamics and time series characteristics related to the baseline, amplitude, timing, noise and variability of the ecosystem time series. This is also illustrated in the quantitative comparison, where the different sensitivities of D for the NDVI, EVI, and LAI data relate specifically to the temporal characteristics of each data set. Additionally, the effect of noise and intra- and interclass variability is demonstrated in a case study based on land cover classification.},
author = {Lhermitte, S. and Verbesselt, J. and Verstraeten, W.W. and Coppin, P.},
doi = {10.1016/j.rse.2011.06.020},
isbn = {0034-4257},
issn = {00344257},
journal = {Remote Sensing of Environment},
keywords = {Change detection,Classification,Ecosystem dynamics,Time series analysis},
number = {12},
pages = {3129--3152},
pmid = {9197400},
title = {{A comparison of time series similarity measures for classification and change detection of ecosystem dynamics}},
url = {http://www.sciencedirect.com/science/article/pii/S0034425711002446},
volume = {115},
year = {2011}
}
@article{Dudani1976,
abstract = {Among the simplest and most intuitively appealing classes of nonprobabilistic classification procedures are those that weight the evidence of nearby sample observations most heavily. More specifically, one might wish to weight the evidence of a neighbor close to an unclassified observation more heavily than the evidence of another neighbor which is at a greater distance from the unclassified observation. One such classification rule is described which makes use of a neighbor weighting function for the purpose of assigning a class to an unclassified sample. The admissibility of such a rule is also considered.},
author = {Dudani, Sahibsingh a.},
doi = {10.1109/TSMC.1976.5408784},
isbn = {0018-9472},
issn = {00189472},
journal = {IEEE Transactions on Systems, Man and Cybernetics},
number = {4},
pages = {325--327},
title = {{DISTANCE-WEIGHTED k-NEAREST-NEIGHBOR RULE.}},
volume = {SMC-6},
year = {1976}
}
@article{Ramasso2008,
abstract = {This paper focuses on human behavior recognition where the main problem$\backslash$nis to bridge the semantic gap between the analogue observations of the$\backslash$nreal world and the symbolic world of human interpretation. For that, a$\backslash$nfusion architecture based on the Transferable Belief Model framework is$\backslash$nproposed and applied to action recognition of an athlete in video$\backslash$nsequences of athletics meeting with moving camera. Relevant features are$\backslash$nextracted from videos, based on both the camera motion analysis and the$\backslash$ntracking of particular points on the athlete's silhouette. Some models$\backslash$nof interpretation are used to link the numerical features to the symbols$\backslash$nto be recognized, which are running, jumping and falling actions. A$\backslash$nTemporal Belief Filter is then used to improve the robustness of action$\backslash$nrecognition. The proposed approach demonstrates good performance when$\backslash$ntested on real videos of athletics sports videos (high jumps, pole$\backslash$nvaults, triple jumps and long jumps) acquired by a moving camera and$\backslash$ndifferent view angles. The proposed system is also compared to Bayesian$\backslash$nNetworks.},
author = {Ramasso, E. and Panagiotakis, C. and Pellerin, D. and Rombaut, M.},
doi = {10.1007/s10044-007-0073-y},
issn = {14337541},
journal = {Pattern Analysis and Applications},
keywords = {Human action recognition,Moving camera,Temporal Belief Filter,Transferable Belief Model},
number = {1},
pages = {1--19},
title = {{Human action recognition in videos based on the transferable belief model : AAAApplication to athletics jumps}},
volume = {11},
year = {2008}
}
@article{Liang2012,
abstract = {Most data stream classification algorithms need to supply input with a large amount of precisely labeled data. However, in many data stream applications, streaming data contains inherent uncertainty, and labeled samples are difficult to be collected, while abundant data are unlabeled. In this paper, we focus on classifying uncertain data streams with only positive and unlabeled samples available. Based on concept-adapting very fast decision tree (CVFDT) algorithm, we propose an algorithm namely puuCVFDT (CVFDT for positive and unlabeled uncertain data). Experimental results on both synthetic and real-life datasets demonstrate the strong ability and efficiency of puuCVFDT to handle concept drift with uncertainty under positive and unlabeled learning scenario. Even when 90{\%} of the samples in the stream are unlabeled, the classification performance of the proposed algorithm is still compared to that of CVFDT, which is learned from fully labeled data without uncertainty. ?? 2012 Elsevier Inc. All rights reserved.},
author = {Liang, Chunquan and Zhang, Yang and Shi, Peng and Hu, Zhengguo},
doi = {10.1016/j.ins.2012.05.023},
file = {:C$\backslash$:/Users/SESA245227/Google Drive/Th{\`{e}}se/Bibliographie/Decision Tree/Liang-Elsevier-2012{\_}Learning very fast decision tree from uncertain data streams with positive and unlabeled samples.pdf:pdf},
issn = {00200255},
journal = {Information Sciences},
keywords = {Positive unlabeled learning,Uncertain attribute,Uncertain data stream,Very fast decision tree},
pages = {50--67},
publisher = {Elsevier Inc.},
title = {{Learning very fast decision tree from uncertain data streams with positive and unlabeled samples}},
url = {http://dx.doi.org/10.1016/j.ins.2012.05.023},
volume = {213},
year = {2012}
}
@article{Belongie2002,
abstract = {We present a novel approach to measuring similarity between shapes and exploit it for object recognition. In our  framework, the measurement of similarity is preceded by 1) solving for correspondences between points on the two shapes, 2) using  the correspondences to estimate an aligning transform. In order to solve the correspondence problem, we attach a descriptor, the  shape context, to each point. The shape context at a reference point captures the distribution of the remaining points relative to it, thus  offering a globally discriminative characterization. Corresponding points on two similar shapes will have similar shape contexts,  enabling us to solve for correspondences as an optimal assignment problem. Given the point correspondences, we estimate the  transformation that best aligns the two shapes; regularized thin-plate splines provide a flexible class of transformation maps for this  purpose. The dissimilarity between the two shapes is computed as a sum of matching errors between corresponding points, together  with a term measuring the magnitude of the aligning transform. We treat recognition in a nearest-neighbor classification framework as  the problem of finding the stored prototype shape that is maximally similar to that in the image. Results are presented for silhouettes,  trademarks, handwritten digits, and the COIL data set.},
author = {Belongie, Serge and Malik, Jitendra and Puzicha, Jan},
doi = {10.1.1.18.8852},
isbn = {9781424455409},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
pages = {509--522},
pmid = {15376597},
title = {{Shape Matching and Object Recognition Using Shape Contexts}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.18.8852},
volume = {24},
year = {2002}
}
@article{Sadri2003,
abstract = {: A new method for recognition of isolated handwritten Arabic/Persian digits is presented. This method is based on Support Vector Machines (SVMs), and a new approach of feature extraction. Each digit is considered from four different views, and from each view 16 features are extracted and combined to obtain 64 features. Using these features, multiple SVM classifiers are trained to separate different classes of digits. CENPARMI Indian (Arabic/Persian) handwritten digit database is used for training and testing of SVM classifiers. Based on this database, differences between Arabic and Persian digits in digit recognition are shown. This database provides 7390 samples for training and 3035 samples for testing from the real life samples. Experiments show that the proposed features can provide a very good recognition result using Support Vector Machines at a recognition rate 94.14{\%}, compared with 91.25 {\%} obtained by MLP neural network classifier using the same features and test set.},
author = {Sadri, Javad and Suen, Ching Y and Bui, Tien D.},
journal = {Second Conference on Machine Vision and Image Processing {\&} Applications (MVIP 2003)},
keywords = {feature extraction,machine learning,mlp neural network,multiple support vector classifiers,ocr,optical character recognition,support,svm,vector machine},
pages = {300--307},
title = {{Application of Support Vector Machines for recognition of handwritten Arabic/Persian digits}},
volume = {1},
year = {2003}
}
@article{Chouakria2007,
abstract = {Abstract The most widely used measures of time series proximity are the Euclidean distance and dynamic time warping. The latter can be derived from the distance introduced by Maurice Frchet in 1906 to account for the proximity between curves. The major limitation of these proximity measures is that they are based on the closeness of the values regardless of the similarity w.r.t. the growth behavior of the time series. To alleviate this drawback we propose a new dissimilarity index, based on an automatic adaptive tuning function, to include both proximity measures w.r.t. values and w.r.t. behavior. A comparative numerical analysis between the proposed index and the classical distance measures is performed on the basis of two datasets: a synthetic dataset and a dataset from a public health study.},
author = {Douzal-Chouakria, A. and Nagabhushan, P.},
doi = {10.1007/s11634-006-0004-6},
issn = {18625347},
journal = {Advances in Data Analysis and Classification},
keywords = {Classification,Dynamic time warping,Fr??chet distance,Time Series},
title = {{Adaptive dissimilarity index for measuring time series proximity}},
year = {2007}
}
@inproceedings{Berndt1994,
abstract = {Knowledge discovery in databases presents many interesting challenges within the context of providing computer tools for exploring large data archives. Electronic data repositories are growing qulckiy and contain data from commercial, scientific, and other domains. Much of this data is inherently temporal, such as stock prices or NASA telemetry data. Detecting patterns in such data streams or time series is an important knowledge discovery task. This paper describes some primary experiments with a dynamic programming approach to the problem. The pattern detection algorithm is based on the dynamic time warping technique used in the speech recognition field. Keywords: dynamic programming, dynamic time warping, knowledge discovery, pattern analysis, time series.},
author = {Berndt, D. and Clifford, J.},
booktitle = {KDD},
keywords = {dynamic programming,dynamic time warping,knowledge discovery,pat,tern analysis,time series},
title = {{Using dynamic time warping to find patterns in time series}},
url = {http://www.aaai.org/Papers/Workshops/1994/WS-94-03/WS94-03-031.pdf},
volume = {398},
year = {1994}
}
@book{Bishop2006,
abstract = {The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic techniques. The practical applicability of Bayesian methods has been greatly enhanced by the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation, while new models based on kernels have had a significant impact on both algorithms and applications. This completely new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. The book is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. Extensive support is provided for course instructors, including more than 400 exercises, graded according to difficulty. Example solutions for a subset of the exercises are available from the book web site, while solutions for the remainder can be obtained by instructors from the publisher. The book is supported by a great deal of additional material, and the reader is encouraged to visit the book web site for the latest information. A forthcoming companion volume will deal with practical aspects of pattern recognition and machine learning, and will include free software implementations of the key algorithms along with example data sets and demonstration programs. Christopher Bishop is Assistant Director at Microsoft Research Cambridge, and also holds a Chair in Computer Science at the University of Edinburgh. He is a Fellow of Darwin College Cambridge, and was recently elected Fellow of the Royal Academy of Engineering. The author's previous textbook "Neural Networks for Pattern Recognition" has been widely adopted.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Bishop, Christopher M},
booktitle = {Pattern Recognition},
doi = {10.1117/1.2819119},
eprint = {0-387-31073-8},
file = {:C$\backslash$:/Users/SESA245227/Google Drive/Th{\`{e}}se/Bibliographie/Livre/Bishop - Pattern Recognition and Machine Learning.pdf:pdf},
isbn = {9780387310732},
issn = {10179909},
number = {4},
pages = {738},
pmid = {8943268},
title = {{Pattern Recognition and Machine Learning}},
url = {http://www.library.wisc.edu/selectedtocs/bg0137.pdf},
volume = {4},
year = {2006}
}
@book{Campbell2011,
author = {Campbell, Colin and Ying, Yiming},
booktitle = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
doi = {10.2200/S00324ED1V01Y201102AIM010},
file = {:C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Campbell, Ying - 2011 - Learning with Support Vector Machines.pdf:pdf},
isbn = {9781608456161},
issn = {1939-4608},
month = {feb},
number = {1},
pages = {1--95},
title = {{Learning with Support Vector Machines}},
url = {http://www.morganclaypool.com/doi/abs/10.2200/S00324ED1V01Y201102AIM010},
volume = {5},
year = {2011}
}
@inproceedings{Boser1992,
abstract = {A training algorithm that maximizes the margin  between the training patterns and the decision  boundary is presented. The technique  is applicable to a wide variety of classifiaction  functions, including Perceptrons, polynomials,  and Radial Basis Functions. The effective  number of parameters is adjusted automatically  to match the complexity of the problem.  The solution is expressed as a linear combination  of supporting patterns. These are the  subset of training patterns that are closest to  the decision boundary. Bounds on the generalization  performance based on the leave-one-out  method and the VC-dimension are given. Experimental  results on optical character recognition  problems demonstrate the good generalization  obtained when compared with other  learning algorithms.  1 INTRODUCTION  Good generalization performance of pattern classifiers is achieved when the capacity of the classification function is matched to the size of the training set. Classifiers with a large numb...},
author = {Boser, Bernhard E. and Guyon, Isabelle M. and Vapnik, Vladimir N.},
booktitle = {Proceedings of the 5th Annual ACM Workshop on Computational Learning Theory},
doi = {10.1.1.21.3818},
isbn = {089791497X},
issn = {0-89791-497-X},
pages = {144--152},
title = {{A Training Algorithm for Optimal Margin Classifiers}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.21.3818},
year = {1992}
}
@article{Brigham1967,
abstract = {The fast Fourier transform (FFT), a computer algorithm that computes the discrete Fourier transform much faster than other algorithms, is explained. Examples and detailed procedures are provided to assist the reader in learning how to use the algorithm. The savings in computer time can be huge; for example, an N = 210-point transform can be computed with the FFT 100 times faster than with the use of a direct approach.},
author = {Brigham, E. O. and Morrow, R. E.},
doi = {10.1109/MSPEC.1967.5217220},
isbn = {0018-9235},
issn = {0018-9235},
journal = {Spectrum, IEEE},
number = {12},
pages = {63 --70},
title = {{The fast Fourier transform}},
url = {http://ieeexplore.ieee.org/ielx5/6/5217195/05217220.pdf?tp={\&}arnumber=5217220{\&}isnumber=5217195$\backslash$nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?tp={\&}arnumber=5217220},
volume = {4},
year = {1967}
}
@article{Benesty2009,
abstract = {This chapter develops several forms of the Pearson correlation coefficient in the different domains. This coefficient can be used as an optimization criterion to derive different optimal noise reduction filters [14], but is even more useful for analyzing these optimal filters for their noise reduction performance.},
author = {Benesty, J. and Chen, J. and Huang, Y. and Cohen, I.},
doi = {10.1007/978-3-642-00296-0},
isbn = {978-3-642-00295-3},
journal = {Noise Reduction in Speech Processing},
title = {{Pearson correlation coefficient}},
url = {http://www.springerlink.com/index/10.1007/978-3-642-00296-0$\backslash$nhttp://link.springer.com/content/pdf/10.1007/978-3-642-00296-0{\_}5.pdf},
year = {2009}
}
@book{Rabiner1993,
abstract = {Provides a theoretically sound, technically accurate, and complete description of the basic knowledge and ideas that constitute a modern system for speech recognition by machine. Covers production, perception, and acoustic-phonetic characterization of the speech signal; signal processing and analysis methods for speech recognition; pattern comparison techniques; speech recognition system design and implementation; theory and implementation of hidden Markov models; speech recognition based on connected word models; large vocabulary continuous speech recognition; and task- oriented application of automatic speech recognition. For practicing engineers, scientists, linguists, and programmers interested in speech recognition.},
author = {Rabiner, L. and Juang, B.},
booktitle = {Prentice Hall},
doi = {10.1002/ev.1647},
isbn = {0130151572},
title = {{Fundamentals of Speech Recognition}},
url = {http://cmp.felk.cvut.cz/cmp/support/phd112.html},
volume = {103},
year = {1993}
}
@article{McNames2002,
abstract = {Local models have emerged as one of the most accurate methods of time series prediction, but their performance is sensitive to the choice of user-specified parameters such as the size of the neighborhood, the embedding dimension, and the distance metric. This paper describes a new method of optimizing these parameters to minimize the multi-step cross-validation error. Empirical results indicate that multi-step optimization is susceptible to shallow local minima unless the optimization is limited to 10 or fewer steps ahead. The models optimized using the new method consistently performed better than those optimized with adaptive analog forecasts. ?? 2002 Elsevier Science B.V. All rights reserved.},
author = {McNames, James},
doi = {10.1016/S0925-2312(01)00647-6},
file = {:C$\backslash$:/Users/SESA245227/Desktop/Neurocomputing.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Chaos,Embedding dimension,Local models,Metric optimization,Time series prediction},
number = {May 2001},
pages = {279--297},
title = {{Local averaging optimization for chaotic time series prediction}},
volume = {48},
year = {2002}
}
@article{Shental2002,
abstract = {We present a method for training a similarity metric from data. The method can be used for recognition or verification applications where the number of categories is very large and not known during training, and where the number of training samples for a single category is very small. The idea is to learn a function that maps input patterns into a target space such that the L1 norm in the target space approximates the "semantic" distance in the input space. The method is applied to a face verification task. The learning process minimizes a discriminative loss function that drives the similarity metric to be small for pairs of faces from the same person, and large for pairs from different persons. The mapping from raw to the target space is a convolutional network whose architecture is designed for robustness to geometric distortions. The system is tested on the Purdue/AR face database which has a very high degree of variability in the pose, lighting, expression, position, and artificial occlusions such as dark glasses and obscuring scarves.},
author = {Shental, Noam and Hertz, Tomer and Weinshall, Daphna and Pavel, Misha},
doi = {10.1007/3-540-47979-1{\_}52},
isbn = {978-3-540-43748-2},
journal = {European Conference on Computer Vision (ECCV)},
pages = {776--790},
title = {{Adjustment Learning and Relevant Component Analysis}},
url = {http://dx.doi.org/10.1007/3-540-47979-1{\_}52},
volume = {2353},
year = {2002}
}
@article{Weinberger2009,
abstract = {The accuracy of k-nearest neighbor (kNN) classification depends significantly on the metric used to compute distances between different examples. In this paper, we show how to learn a Maha- lanobis distance metric for kNN classification from labeled examples. The Mahalanobis metric can equivalently be viewed as a global linear transformation of the input space that precedes kNN classification using Euclidean distances. In our approach, the metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a largemargin. As in support vectormachines (SVMs), themargin criterion leads to a convex optimization based on the hinge loss. Unlike learning in SVMs, however, our approach re- quires no modification or extension for problems in multiway (as opposed to binary) classification. In our framework, the Mahalanobis distance metric is obtained as the solution to a semidefinite program. On several data sets of varying size and difficulty, we find that metrics trained in this way lead to significant improvements in kNN classification. Sometimes these results can be further improved by clustering the training examples and learning an individual metric within each cluster. We show how to learn and combine these local metrics in a globally integrated manner. Keywords: convex optimization, semi-definite programming,Mahalanobis distance,metric learn- ing, multi-class classification, support vector machines 1.},
author = {Weinberger, K. and Saul, L.},
file = {:C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Weinberger, Saul - 2009 - Distance Metric Learning for Large Margin Nearest Neighbor Classification.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {convex optimization,ing,mahalanobis distance,metric learn-,multi-class classification,semi-definite programming,support vector machines},
pages = {207--244},
title = {{Distance Metric Learning for Large Margin Nearest Neighbor Classification}},
url = {http://www.jmlr.org/papers/volume10/weinberger09a/weinberger09a.pdf},
volume = {10},
year = {2009}
}
@article{Cortes1995,
abstract = {The support-vector network is a new leaming machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high- dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data. High generalization ability of support-vector networks utilizing polynomial input transformations is demon- strated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Cortes, Corinna and Vapnik, Vladimir},
doi = {10.1007/BF00994018},
eprint = {arXiv:1011.1669v3},
isbn = {0885-6125},
issn = {08856125},
journal = {Machine Learning},
keywords = {efficient learning algorithms,neural networks,pattern recognition,polynomial classifiers,radial basis function classifiers},
number = {3},
pages = {273--297},
pmid = {9052598814225336358},
title = {{Support-vector networks}},
volume = {20},
year = {1995}
}
@incollection{Yang1999,
abstract = {This paper reports a controlled study with statistical signifi cance tests on five text categorization methods: the Support Vector Machines (SVM), a kNearest Neighbor (kNN) clas sifier, a neural network (NNet) approach, the Linear Least squares Fit (LLSF) mapping and a Naive Bayes (NB) classi fier. We focus on the robustness of these methods in dealing with a skewed category distribution, and their performance as function of the trainingset category frequency. Our re sults show that SVM, kNN and LLSF significantly outper form NNet and NB when the number of positive training instances per category are small (less than ten), and that all the methods perform comparably when the categories are sufficiently common (over 300 instances).},
author = {Yang, Yiming and Liu, Xin},
booktitle = {Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval SIGIR 99},
doi = {10.1145/312624.312647},
isbn = {1581130961},
pages = {42--49},
title = {{A re-examination of text categorization methods}},
year = {1999}
}
@inproceedings{Najmeddine2012,
author = {Najmeddine, H. and Jay, A. and Marechal, P. and Mari{\'{e}}, S.},
booktitle = {RFIA},
file = {:C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Najmeddine et al. - 2012 - Mesures de similarit{\'{e}} pour l’aide {\`{a}} l’analyse des donn{\'{e}}es {\'{e}}nerg{\'{e}}tiques de b{\^{a}}timents.pdf:pdf},
isbn = {9782953951523},
keywords = {Data mining,INCAS.,Time series,diagnosis and decision support,sensors,similarity measures},
title = {{Mesures de similarit{\'{e}} pour l’aide {\`{a}} l’analyse des donn{\'{e}}es {\'{e}}nerg{\'{e}}tiques de b{\^{a}}timents}},
url = {https://hal-cea.archives-ouvertes.fr/file/index/docid/661016/filename/article53{\_}modif.pdf},
year = {2012}
}
@article{Montero2014,
abstract = {Time series clustering is an active research area with applications in a wide range of fields. One key component in cluster analysis is determining a proper dissimilarity mea- sure between two data objects, and many criteria have been proposed in the literature to assess dissimilarity between two time series. The R package TSclust is aimed to im- plement a large set of well-established peer-reviewed time series dissimilarity measures, including measures based on raw data, extracted features, underlying parametric models, complexity levels, and forecast behaviors. Computation of these measures allows the user to perform clustering by using conventional clustering algorithms. TSclust also includes a clustering procedure based on p values from checking the equality of generating models, and some utilities to evaluate cluster solutions. The implemented dissimilarity functions are accessible individually for an easier extension and possible use out of the clustering context. The main features of TSclust are described and examples of its use are presented.},
author = {Montero, Pablo and Vilar, Jos{\'{e}}},
file = {:C$\backslash$:/Users/SESA245227/Desktop/TS clust.pdf:pdf},
journal = {Journal of Statistical Software November},
keywords = {clustering,dissimilarity measure,time series data,validation indices},
number = {1},
title = {{TSclust : An R Package for Time Series Clustering}},
url = {http://www.jstatsoft.org/v62/i01/paper},
volume = {62},
year = {2014}
}
@article{Crammer2001,
abstract = {In this paper we describe the algorithmic implementation of multiclass kernel-based vector machines. Our starting point is a generalized notion of the margin to multiclass problems. Using this notion we cast multiclass categorization problems as a constrained optimization problem with a quadratic objective function. Unlike most of previous approaches which typically decompose a multiclass problem into multiple independent binary classification tasks, our notion of margin yields a direct method for training multiclass predictors. By using the dual of the optimization problem we are able to incorporate kernels with a compact set of constraints and decompose the dual problem into multiple optimization problems of reduced size. We describe an efficient fixed-point algorithm for solving the reduced optimization problems and prove its convergence. We then discuss technical details that yield significant running time improvements for large datasets. Finally, we describe various experiments with our approach comparing it to previously studied kernel-based methods. Our experiments indicate that for multiclass problems we attain state-of-the-art accuracy},
author = {Crammer, Koby and Singer, Yoram},
doi = {10.1162/15324430260185628},
file = {:C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Crammer, Singer - 2001 - On the Algorithmic Implementation of Multiclass Kernel-based Vector Machines.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {kernel machines,multiclass problems,svm},
pages = {265--292},
title = {{On the Algorithmic Implementation of Multiclass Kernel-based Vector Machines}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/CrammerS01.pdf$\backslash$nhttp://www.jmlr.org/papers/volume2/crammer01a/crammer01a.pdf},
volume = {2},
year = {2001}
}
@book{Duda1973,
abstract = {Classic book on pattern recognition. Interesting points: 1) p. 66, and p. 114: Mentions the problems with dimensionality curse. 2) p. 243-246: Mentions Multidimensional scaling (MDS), Karhunen-Loeve and dimensionality reduction. Also, has the spiral data-set as a sample. 3) p. 333: mentions SVD/eigenvalues for linear fitting.},
author = {{O Duda}, Richard and {E Hart}, Peter},
booktitle = {Leonardo},
doi = {10.2307/1573081},
isbn = {0471223611},
issn = {0024094X},
pages = {482},
title = {{Pattern Classification and Scene Analysis}},
url = {http://www.jstor.org/stable/1573081?origin=crossref},
volume = {7},
year = {1973}
}
@article{Sahidullah2012,
abstract = {Standard Mel frequency cepstrum coefficient (MFCC) computation technique utilizes discrete cosine transform (DCT) for decorrelating log energies of filter bank output. The use of DCT is reasonable here as the covariance matrix of Mel filter bank log energy (MFLE) can be compared with that of highly correlated Markov-I process. This full-band based MFCC computation technique where each of the filter bank output has contribution to all coefficients, has two main disadvantages. First, the covariance matrix of the log energies does not exactly follow Markov-I property. Second, full-band based MFCC feature gets severely degraded when speech signal is corrupted with narrow-band channel noise, though few filter bank outputs may remain unaffected. In this work, we have studied a class of linear transformation techniques based on block wise transformation of MFLE which effectively decorrelate the filter bank log energies and also capture speech information in an efficient manner. A thorough study has been carried out on the block based transformation approach by investigating a new partitioning technique that highlights associated advantages. This article also reports a novel feature extraction scheme which captures complementary information to wide band information; that otherwise remains undetected by standard MFCC and proposed block transform (BT) techniques. The proposed features are evaluated on NIST SRE databases using Gaussian mixture model-universal background model (GMM-UBM) based speaker recognition system. We have obtained significant performance improvement over baseline features for both matched and mismatched condition, also for standard and narrow-band noises. The proposed method achieves significant performance improvement in presence of narrow-band noise when clubbed with missing feature theory based score computation scheme. ?? 2011 Elsevier B.V. All rights reserved.},
author = {Sahidullah, Md and Saha, Goutam},
doi = {10.1016/j.specom.2011.11.004},
isbn = {0167-6393},
issn = {01676393},
journal = {Speech Communication},
keywords = {Block transform,Correlation matrix,DCT,Decorrelation technique,Linear transformation,MFCC,Missing feature theory,Narrow-band noise,Speaker recognition},
number = {4},
pages = {543--565},
title = {{Design, analysis and experimental evaluation of block based transformation in MFCC computation for speaker recognition}},
url = {http://dx.doi.org/10.1016/j.specom.2011.11.004},
volume = {54},
year = {2012}
}
@book{Tan2005b,
abstract = {-This paper is review of current usage of data mining, machine learning and other algorithms for credit risk assessment. We are witnessing importance of credit risk assessment, especially after the global economic crisis on 2008.S o, it is very important to have a proper way to deal with the credit risk and provide powerful and accurate model for credit risk assessment. Many credit scoring techniques such as statistical techniques (logistic regression, discriminant analysis) or advanced techniques such as neural networks, decision trees, genetic algorithm, or support vector machines are used for credit risk assessment. Some of them are described in this article with theirs advantages/disadvantages. Even with many models and methods, it is still hard to say which model is the best or which classifier or which data mining technique is the best. Each model depends on particular data set or attributes set, so it is very important to develop flexible model which is adaptable to every dataset or attribute set.},
author = {Tan, Pang-Ning and Steinbach, Michael and Kumar, Vipin},
booktitle = {Addison Wesley},
isbn = {9789604743179},
keywords = {- credit risk assessments,credit scoring techniques,single classifiers},
pages = {500},
title = {{Introduction to Data Mining}},
year = {2005}
}
@article{Fan2008,
abstract = {LIBLINEAR is an open source library for large-scale linear classification. It supports logistic regression and linear support vector machines. We provide easy-to-use command-line tools and library calls for users and developers. Comprehensive documents are available for both beginners and advanced users. Experiments demonstrate that LIBLINEAR is very efficient on large sparse data sets.},
author = {Fan, RE and Chang, KW and Hsieh, CJ},
doi = {10.1038/oby.2011.351},
isbn = {089791497X},
issn = {15324435},
journal = {The Journal of Machine Learning},
keywords = {large-scale linear classification,logistic regression,machine learning,open,source,support vector machines},
pmid = {22173572},
title = {{LIBLINEAR: A library for large linear classification}},
url = {http://dl.acm.org/citation.cfm?id=1442794},
year = {2008}
}
@techreport{WienerN1942,
author = {{Wiener N}},
institution = {Report of the Services 19, Research Project DIC-6037 MIT},
title = {{Extrapolation, Interpolation {\&} Smoothing of Stationary Time Series - With Engineering Applications}},
year = {1942}
}
@article{Faloutsos1994,
abstract = {We present an ecient indexing method to locate 1-dimensional subsequences within a collection of sequences, such that the subsequences match a given (query) pattern within a specified tolerance. The idea is to map each data sequence into a small set of multidimensional rectangles in feature space. Then, these rectangles can be readily indexed using traditional spatial access methods, like the R*-tree [9]. In more detail, we use a sliding window over the data sequence and extract its features; the result is a trail in feature space. We propose an efficient and effective algorithm to divide such trails into sub-trails, which are subsequently represented by their Minimum Bounding Rectangles (MBRs). We also examine queries of varying lengths, and we show how to handle each case eciently. We implemented our method and carried out experiments on synthetic and real data (stock price movements). We compared the method to sequential scanning, which is the only obvious competitor. The results were excellent: our method accelerated the search time from 3 times up to 100 times.},
author = {Faloutsos, Christos and Ranganathan, M. and Manolopoulos, Yannis},
doi = {10.1145/191843.191925},
isbn = {0897916395},
issn = {01635808},
journal = {ACM SIGMOD Record},
number = {2},
pages = {419--429},
title = {{Fast subsequence matching in time-series databases}},
url = {http://portal.acm.org/citation.cfm?doid=191843.191925},
volume = {23},
year = {1994}
}
@article{Hwang2012,
author = {Hwang, Seok Hwan and Ham, Dae Heon and Kim, Joong Hoon},
doi = {10.1007/s12205-012-1519-3},
file = {:C$\backslash$:/Users/SESA245227/Google Drive/Th{\`{e}}se/Bibliographie/SVM + Time Series/Hwang-KSCE Journal of Civil Engineering-2012{\_}Forecasting performance of LS-SVM for nonlinear hydrological time series.pdf:pdf},
issn = {1226-7988},
journal = {KSCE Journal of Civil Engineering},
keywords = {forecasting,forecasting performance,support vector machine},
number = {5},
pages = {870--882},
title = {{Forecasting performance of LS-SVM for nonlinear hydrological time series}},
url = {http://link.springer.com/10.1007/s12205-012-1519-3},
volume = {16},
year = {2012}
}
@article{Cover1967b,
abstract = {The nearest neighbor decision rule assigns to an unclassified sample point the classification of the nearest of a set of previously classified points. This rule is independent of the underlying joint distribution on the sample points and their classifications, and hence the probability of error<tex>R</tex>of such a rule must be at least as great as the Bayes probability of error<tex>R{\^{}}{\{}ast{\}}</tex>--the minimum probability of error over all decision rules taking underlying probability structure into account. However, in a large sample analysis, we will show in the<tex>M</tex>-category case that<tex>R{\^{}}{\{}ast{\}} leq R leq R{\^{}}{\{}ast{\}}(2 --MR{\^{}}{\{}ast{\}}/(M-1))</tex>, where these bounds are the tightest possible, for all suitably smooth underlying distributions. Thus for any number of categories, the probability of error of the nearest neighbor rule is bounded above by twice the Bayes probability of error. In this sense, it may be said that half the classification information in an infinite sample set is contained in the nearest neighbor.},
author = {Cover, T. and Hart, P.},
doi = {10.1109/TIT.1967.1053964},
isbn = {0018-9448},
issn = {0018-9448},
journal = {IEEE Transactions on Information Theory},
number = {1},
pages = {21--27},
pmid = {21919855},
title = {{Nearest neighbor pattern classification}},
volume = {13},
year = {1967}
}
@article{Morse2007,
abstract = {A variety of techniques currently exist for measuring the similarity between time series datasets. Of these techniques, the methods whosematching criteria is bounded by a specified ǫ threshold value, such as the LCSS and the EDR techniques, have been shown to be robust in the presence of noise, time shifts, and data scaling. Our work proposes a new algorithm, called the Fast Time Series Evaluation (FTSE) method, which can be used to evaluate such threshold value techniques, including LCSS and EDR. Using FTSE,we show that these techniques can be evaluated faster than using either traditional dynamic programming or even warp-restricting methods such as the Sakoe-Chiba band and the Itakura Parallelogram. We also show that FTSE can be used in a framework that can evaluate a richer range of epsilon threshold-based scoring techniques, of which EDR and LCSS are just two examples. This framework, called Swale, extends the epsilon threshold-based scoring techniques to include arbitrary match rewards and gap penalties. Through extensive empirical evaluation, we show that Swale can obtain greater accuracy than existing methods.},
author = {Morse, Michael D and Patel, Jignesh M},
doi = {10.1145/1247480.1247544},
isbn = {9781595936868},
journal = {ACM SIGMOD international conference on Management of data},
keywords = {clustering,time series,trajectory similarity},
pages = {569},
title = {{An efficient and accurate method for evaluating time series similarity}},
url = {http://dl.acm.org/citation.cfm?id=1247544$\backslash$nhttp://portal.acm.org/citation.cfm?doid=1247480.1247544},
year = {2007}
}
@article{Dietterich1997,
author = {Dietterich, T.},
file = {:C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dietterich - 1997 - Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms.pdf:pdf},
title = {{Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms}},
year = {1997}
}
@book{Chatfield2004,
abstract = {"Since 1975, The Analysis of Time Series: An Introduction has introduced legions of statistics students and researchers to the theory and practice of time series analysis. The sixth edition provides an accessible, comprehensive introduction to the theory and practice of time series analysis. The treatment covers a wide range of topics, including ARIMA probability models, forecasting methods, spectral analysis, linear systems, state-space models, and the Kalman filter. It also addresses nonlinear, multivariate, and long-memory models. The author has carefully updated each chapter, added new discussions, incorporated new datasets, and made those datasets available at www.crcpress.com."--BOOK JACKET.},
author = {Chatfield, Christopher},
booktitle = {Texts in statistical science},
isbn = {1584883170},
keywords = {Time-series analysis.},
pages = {xiii, 333 p.},
pmid = {13166316},
title = {{The analysis of time series : an introduction}},
year = {2004}
}
@article{PANAGIOTAKIS2008,
abstract = {We present a shape-based method for automatic people detection and counting without any assumption concerning camera motion. In order to evaluate the robustness of the proposed method, we apply it for classifying athletics videos into two classes: videos of individual and videos of team sports. The videos used are real and characterized by dynamic and unconstrained environment. Moreover, in the case of team sport, we propose a shape deformations based method for running/hurdling discrimination (activity recognition). Robust, adaptive and independent from color, illumination changes and the camera motion, the proposed features are combined in the Transferable Belief Model (TBM) framework providing a two-level (frames and shot) video categorization. Experimental results of 97{\%} of accuracy for individual/team sport categorization using a dataset of 252 real videos of athletic meetings, acquired by moving cameras under varying view angles, indicate the stability and the good performance of the proposed scheme.},
author = {PANAGIOTAKIS, COSTAS and RAMASSO, EMMANUEL and TZIRITAS, GEORGIOS and ROMBAUT, MICH{\`{E}}LE and PELLERIN, DENIS},
doi = {10.1142/S0218001408006752},
issn = {0218-0014},
journal = {International Journal of Pattern Recognition and Artificial Intelligence},
keywords = {People detection,people counting,team activity recognition,transferable belief model,video analysis},
number = {06},
pages = {1187--1213},
title = {{SHAPE-BASED INDIVIDUAL/GROUP DETECTION FOR SPORT VIDEOS CATEGORIZATION}},
url = {http://www.worldscientific.com/doi/abs/10.1142/S0218001408006752?journalCode=ijprai},
volume = {22},
year = {2008}
}
@misc{Keogh2011,
author = {Keogh, E. and Zhu, Q. and Hu, B. and Hao, Y. and Xi, X. and Wei, L. and Ratanamahatana, C.A.},
title = {{The UCR Time Series Classification/Clustering Homepage}},
url = {www.cs.ucr.edu/{~}eamonn/time{\_}series{\_}data/},
year = {2011}
}
@article{Aizerman1964,
abstract = {Introduction of kernels},
author = {Aizerman, M. and Braverman, E. and Rozonoer, L.},
journal = {Automation and Remote Control},
pages = {821--837},
title = {{Theoretical foundations of the potential function method in pattern recognition learning}},
volume = {25},
year = {1964}
}
@article{AhlameDouzal-Chouakria2011,
abstract = {This paper proposes an extension of classification trees to time series input variables. A new split criterion based on time series proximities is introduced. First, the criterion relies on an adaptive (i.e., parameterized) time series metric to cover both behaviors and values proximities. The metrics parameters may change from one internal node to another to achieve the best bisection of the set of time series. Second, the criterion involves the automatic extraction of the most discriminating subsequences. The proposed time series classification tree is applied to a wide range of datasets: public and new, real and synthetic, univariate and multivariate data. We show, through the experiments performed in this study, that the proposed tree outperforms temporal trees using standard time series distances and performs well compared to other competitive time series classifiers},
author = {Douzal-Chouakria, A. and Amblard, C.},
file = {:C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Douzal-Chouakria, Amblard - 2011 - Classification trees for time series.pdf:pdf},
isbn = {0000000000000},
issn = {2150-8097},
journal = {Pattern Recognition journal},
keywords = {Classification,Supervised classification,Time series proximity measures,trees Learning metric},
publisher = {VLDB Endowment},
title = {{Classification trees for time series}},
year = {2011}
}
@inproceedings{Nguyen2012,
abstract = {More and more people express their opinions on social media such as Facebook and Twitter. Predictive analysis on social media time-series allows the stake-holders to leverage this immediate, accessible and vast reachable communication channel to react and proact against the public opinion. In particular, understanding and predicting the sentiment change of the public opinions will allow business and government agencies to react against negative sentiment and design strategies such as dispelling rumors and post balanced messages to revert the public opinion. In this paper, we present a strategy of building statistical models from the social media dynamics to predict collective sentiment dynamics. We model the collective sentiment change without delving into micro analysis of individual tweets or users and their corresponding low level network structures. Experiments on large-scale Twitter data show that the model can achieve above 85{\%} accuracy on directional sentiment prediction.},
author = {Nguyen, L. and Wu, P. and Chan, W. and Peng, W. and Zhang, Y.},
booktitle = {WISDOM},
doi = {10.1145/2346676.2346682},
isbn = {9781450315432},
keywords = {sentiment analysis,sentiment prediction,social network analysis},
title = {{Predicting collective sentiment dynamics from time-series social media}},
year = {2012}
}
@article{Silverman1989,
abstract = {In 1951, Evelyn Fix and J.L. Hodges, Jr. wrote a technical report which contained prophetic work on nonparametric discriminant analysis and probability density estimation, and which was never published by the authors. The report introduced several important concepts for the first time. It is of interest not only for historical reasons but also because it contains much material that is still of contemporary relevance. Here, the report is printed in full together with a commentary placing the paper in context and interpreting its ideas in the light of more modern developments. /// En 1951, E. Fix et J.L. Hodges, Jr. ont �crit un rapport technique proph�tique sur l'analyse non-param�trique de discrimination et l'estimation de la densit� de probabilit�, mais celui-ci ne fut jamais publi� par ses auteurs. Ce rapport introduit plusieurs id�es nouvelles et importantes. Il nous int�resse non seulement pour des raisons historiques, mais aussi parce qu'il contient des concepts qui sont encore importants de nos jours. Nous le publions ici en entier, accompagn� d'un commentaire qui l'interpr�te d'un point de vue plus moderne.},
author = {Silverman, B W and Jones, M C},
doi = {10.2307/1403796},
isbn = {03067734},
issn = {03067734},
journal = {International Statistical Review / Revue Internationale de Statistique},
number = {3},
pages = {pp. 233--238},
title = {{E. Fix and J.L. Hodges (1951): An Important Contribution to Nonparametric Discriminant Analysis and Density Estimation: Commentary on Fix and Hodges (1951)}},
url = {http://www.jstor.org/stable/1403796},
volume = {57},
year = {1989}
}
@article{Cochran1977,
abstract = {Commentary by : Cochran William C. Current Contents : {\#}19, May 9, 1977},
author = {Cochran, William C},
journal = {Citation Classics},
pages = {1},
title = {{Snedecor G W {\&} Cochran W G. Statistical methods applied to experiments in agriculture and biology. 5th ed. Ames, Iowa: Iowa State University Press, 1956.}},
url = {papers3://publication/uuid/8C5C843E-F853-4CB4-82BC-1141F0C01CB4},
volume = {19},
year = {1977}
}
@article{Berndt1994a,
abstract = {Knowledge discovery in databases presents many interesting challenges within the context of providing computer tools for exploring large data archives. Electronic data repositories are growing qulckiy and contain data from commercial, scientific, and other domains. Much of this data is inherently temporal, such as stock prices or NASA telemetry data. Detecting patterns in such data streams or time series is an important knowledge discovery task. This paper describes some primary experiments with a dynamic programming approach to the problem. The pattern detection algorithm is based on the dynamic time warping technique used in the speech recognition field. Keywords: dynamic programming, dynamic time warping, knowledge discovery, pattern analysis, time series.},
author = {Berndt, Donald and Clifford, James},
journal = {Workshop on Knowledge Knowledge Discovery in Databases},
keywords = {dynamic programming,dynamic time warping,knowledge discovery,pat,tern analysis,time series},
pages = {359--370},
title = {{Using dynamic time warping to find patterns in time series}},
url = {http://www.aaai.org/Papers/Workshops/1994/WS-94-03/WS94-03-031.pdf},
volume = {398},
year = {1994}
}
@misc{LIG2014,
title = {{LIG-AMA Machine Learning Datasets Repository}},
url = {http://ama.liglab.fr/resourcestools/datasets/},
year = {2014}
}
@inproceedings{Xi2006a,
abstract = {Many algorithms have been proposed for the problem of time series classification. However, it is clear that one-nearest-neighbor with Dynamic Time Warping (DTW) distance is exceptionally difficult to beat. This approach has one weakness, however; it is computationally too demanding for many realtime applications. One way to mitigate this problem is to speed up the DTW calculations. Nonetheless, there is a limit to how much this can help. In this work, we propose an additional technique, numerosity reduction, to speed up one-nearest-neighbor DTW. While the idea of numerosity reduction for nearest-neighbor classifiers has a long history, we show here that we can leverage off an original observation about the relationship between dataset size and DTW constraints to produce an extremely compact dataset with little or no loss in accuracy. We test our ideas with a comprehensive set of experiments, and show that it can efficiently produce extremely fast accurate classifiers.},
author = {Xi, Xiaopeng and Keogh, Eamonn and Shelton, Christian and Wei, Li and Ratanamahatana, Chotirat Ann},
booktitle = {Proceedings of the 23rd international conference on Machine learning (ICML)},
doi = {10.1145/1143844.1143974},
isbn = {1595933832},
pages = {1033----1040},
title = {{Fast time series classification using numerosity reduction}},
url = {http://dl.acm.org/citation.cfm?id=1143974},
year = {2006}
}
@article{Kalman1960,
abstract = {The classical filtering and prediction problem is re-examined using the Bode- Shannon representation of random processes and the ``state transition{\&}apos;{\&}apos; method of analysis of dynamic systems. New results are: (1) The formulation and methods of solution of the problem apply without modifica- tion to stationary and nonstationary statistics and to growing-memory and infinite- memory filters. (2) A nonlinear difference (or differential) equation is derived for the covariance matrix of the optimal estimation error. From the solution of this equation the co- efficients of the difference (or differential) equation of the optimal linear filter are ob- tained without further calculations. (3) The filtering problem is shown to be the dual of the noise-free regulator problem. The new method developed here is applied to two well-known problems, confirming and extending earlier results. The discussion is largely self-contained and proceeds from first principles; basic concepts of the theory of random processes are reviewed in the Appendix.},
author = {Kalman, R E},
doi = {10.1115/1.3662552},
isbn = {9783540769897},
issn = {0021-9223},
journal = {Transactions of the ASME Journal of Basic Engineering},
number = {Series D},
pages = {35--45},
pmid = {5311910},
title = {{A New Approach to Linear Filtering and Prediction Problems}},
volume = {82},
year = {1960}
}
@misc{Chen1996,
abstract = {Mining information and knowledge from large databases has been recognized by many researchers as a key research topic in database systems and machine learning, and by many industrial companies as an important area with an opportunity of major revenues. Researchers in many different fields have shown great interest in data mining. Several emerging applications in information-providing services, such as data warehousing and online services over the Internet, also call for various data mining techniques to better understand user behavior, to improve the service provided and to increase business opportunities. In response to such a demand, this article provides a survey, from a database researcher's point of view, on the data mining techniques developed recently. A classification of the available data mining techniques is provided and a comparative study of such techniques is presented},
author = {Chen, Ming Syan and Han, Jiawei and Yu, Philip S.},
booktitle = {IEEE Transactions on Knowledge and Data Engineering},
doi = {10.1109/69.553155},
isbn = {1041-4347},
issn = {10414347},
keywords = {Association rules,Classification,Data clustering,Data cubes,Data generalization and characterization,Data mining,Knowledge discovery,Multiple-dimensional databases,Pattern matching algorithms},
number = {6},
pages = {866--883},
title = {{Data mining: An Overview from a Database Perspective}},
volume = {8},
year = {1996}
}
@article{Hu2013,
abstract = {In this paper, a hybrid forecasting approach, which combines the Ensemble Empirical Mode Decomposition (EEMD) and the Support Vector Machine (SVM), is proposed to improve the quality of wind speed forecasting. The essence of the methodology incorporates three phases. First, the original data of wind speed are decomposed into a number of independent Intrinsic Mode Functions (IMFs) and one residual series by EEMD using the principle of decomposition. In order to forecast these IMFs, excepting the highest frequency acquired by EEMD, the respective estimates are yielded using the SVM algorithm. Finally, these respective estimates are combined into the final wind speed forecasts using the principle of ensemble. The proposed hybrid method is examined by forecasting the mean monthly wind speed of three wind farms located in northwest China. The obtained results confirm an observable improvement for the forecasting validity of the proposed hybrid approach. This tool shows great promise for the forecasting of intricate time series which are intrinsically highly volatile and irregular. ?? 2013 Elsevier Ltd.},
author = {Hu, Jianming and Wang, Jianzhou and Zeng, Guowei},
doi = {10.1016/j.renene.2013.05.012},
file = {:C$\backslash$:/Users/SESA245227/Google Drive/Th{\`{e}}se/Bibliographie/SVM + Time Series/Hu-Renewable Energy-2013{\_}A hybrid forecasting approach applied to wind speed time series.pdf:pdf},
isbn = {0960-1481},
issn = {09601481},
journal = {Renewable Energy},
keywords = {Ensemble Empirical Mode Decomposition (EEMD),Support Vector Machine (SVM),Wind farm,Wind speed forecasting},
pages = {185--194},
publisher = {Elsevier Ltd},
title = {{A hybrid forecasting approach applied to wind speed time series}},
url = {http://dx.doi.org/10.1016/j.renene.2013.05.012},
volume = {60},
year = {2013}
}
@article{Salvador,
abstract = {The dynamic time warping (DTW) algorithm is able to find the optimal alignment between two time series. It is often used to determine time series similarity, classification, and to find corresponding regions between two time series. DTW has a quadratic time and space complexity that limits its use to only small time series data sets. In this paper we introduce FastDTW, an approximation of DTW that has a linear time and space complexity. FastDTW uses a multilevel approach that recursively projects a solution from a coarse resolution and refines the projected solution. We prove the linear time and space complexity of FastDTW both theoretically and empirically. We also analyze the accuracy of FastDTW compared to two other existing approximate DTW algorithms: Sakoe-Chuba Bands and Data Abstraction. Our results show a large improvement in accuracy over the existing methods.},
author = {Salvador, Stan and Chan, Philip},
file = {:C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Salvador, Chan - Unknown - FastDTW Toward Accurate Dynamic Time Warping in Linear Time and Space.pdf:pdf},
keywords = {dynamic time warping,time series},
title = {{FastDTW : Toward Accurate Dynamic Time Warping in Linear Time and Space}}
}
@article{Cover1967a,
abstract = {The nearest neighbor decision rule assigns to an unclassified sample point the classification of the nearest of a set of previously classified points. This rule is independent of the underlying joint distribution on the sample points and their classifications, and hence the probability of error<tex>R</tex>of such a rule must be at least as great as the Bayes probability of error<tex>R{\^{}}{\{}ast{\}}</tex>--the minimum probability of error over all decision rules taking underlying probability structure into account. However, in a large sample analysis, we will show in the<tex>M</tex>-category case that<tex>R{\^{}}{\{}ast{\}} leq R leq R{\^{}}{\{}ast{\}}(2 --MR{\^{}}{\{}ast{\}}/(M-1))</tex>, where these bounds are the tightest possible, for all suitably smooth underlying distributions. Thus for any number of categories, the probability of error of the nearest neighbor rule is bounded above by twice the Bayes probability of error. In this sense, it may be said that half the classification information in an infinite sample set is contained in the nearest neighbor.},
author = {Cover, T. and Hart, P.},
doi = {10.1109/TIT.1967.1053964},
isbn = {0018-9448},
issn = {0018-9448},
journal = {IEEE Transactions on Information Theory},
number = {1},
pages = {21--27},
pmid = {21919855},
title = {{Nearest neighbor pattern classification}},
volume = {13},
year = {1967}
}
@article{Bottou2007,
abstract = {Considerable efforts have been devoted to the implementation of efficient optimization method for solving the Support Vector Machine dual problem. This document proposes an historical perspective and and in depth review of the algorithmic and computational issues associated with this problem.},
author = {Bottou, L and Lin, CJ},
isbn = {0262026252},
journal = {Large scale kernel machines},
pages = {1--27},
title = {{Support vector machine solvers}},
url = {http://140.112.30.28/{~}cjlin/papers/bottou{\_}lin.pdf},
year = {2007}
}
@inproceedings{Yin2008,
abstract = {Event detection is a critical task in sensor networks, especially for environmental monitoring applications. Traditional solutions to event detection are based on analyzing one-shot data points, which might incur a high false alarm rate because sensor data is inherently unreliable and noisy. To address this issue, we propose a novel Distributed Single-pass Incremental Clustering (DSIC) technique to cluster the time series obtained at sensor nodes based on their underlying trends. In order to achieve scalability and energy-efficiency, our DSIC technique uses a hierarchical structure of sensor networks as the underlying infrastructure. The algorithm first compresses the time series produced at individual sensor nodes into a compact representation using Haar wavelet transform, and then, based on dynamic time warping distances, hierarchically groups the approximate time series into a global clustering model in an incremental manner. Experimental results on both real data and synthetic data demonstrate that our DSIC algorithm is accurate, energy-efficient and robust with respect to network topology changes.},
author = {Yin, J. and Gaber, M.},
booktitle = {ICDM},
doi = {10.1109/ICDM.2008.58},
isbn = {9780769535029},
issn = {15504786},
title = {{Clustering distributed time series in sensor networks}},
year = {2008}
}
@article{Jain1999,
abstract = {Clustering is the unsupervised classification of patterns (observations, data items, or feature vectors) into groups (clusters). The clustering problem has been addressed in many contexts and by researchers in many disciplines; this reflects its broad appeal and usefulness as one of the steps in exploratory data analysis. However, clustering is a difficult problem combinatorially, and differences in assumptions and contexts in different communities has made the transfer of useful generic concepts and methodologies slow to occur. This paper presents an overview of pattern clustering methods from a statistical pattern recognition perspective, with a goal of providing useful advice and references to fundamental concepts accessible to the broad community of clustering practitioners. We present a taxonomy of clustering techniques, and identify cross-cutting themes and recent advances. We also describe some important applications of clustering algorithms such as image segmentation, object recognition, and information retrieval.},
archivePrefix = {arXiv},
arxivId = {arXiv:1101.1881v2},
author = {Jain, a. K. and Murty, M. N. and Flynn, P. J.},
doi = {10.1145/331499.331504},
eprint = {arXiv:1101.1881v2},
isbn = {0360-0300},
issn = {03600300},
journal = {ACM Computing Surveys},
number = {3},
pages = {264--323},
pmid = {17707831},
title = {{Data clustering: a review}},
url = {http://portal.acm.org/citation.cfm?doid=331499.331504},
volume = {31},
year = {1999}
}
@article{Simard1992,
abstract = {Memory-based classification algorithms such as radial basis functions or K-nearest neighbors typically rely on simple distances (Euclidean, dot product...), which are not particularly meaningful on pattern vectors. More complex, better suited distance measures are often expensive and rather ad-hoc (elastic matching, deformable templates). We propose a new distance measure which (a) can be made locally invariant to any set of transformations of the input and (b) can be computed efficiently. We tested the method on large handwritten character databases provided by the Post Office and the NIST. Using invariances with respect to translation, rotation, scaling, shearing and line thickness, the method consistently outperformed all other systems on the same databases.},
author = {Simard, Patrice and LeCun, Yann and Denker, John S.},
file = {:C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Simard, LeCun, Denker - 1992 - Efficient pattern recognition using a new transformation distance.pdf:pdf},
isbn = {1558602747},
journal = {Advances in Neural Information Processing Systems (NIPS)},
pages = {50--58},
title = {{Efficient pattern recognition using a new transformation distance}},
url = {http://papers.nips.cc/paper/656-efficient-pattern-recognition-using-a-new-transformation-distance},
year = {1992}
}
@article{Hsu2008,
abstract = {The support vector machine (SVM) is a popular classi cation technique. However, beginners who are not familiar with SVM often get unsatisfactory results since they miss some easy but signi cant steps. In this guide, we propose a simple procedure which usually gives reasonable results. developed well-differentiated superficial transitional cell bladder cancer. CONCLUSIONS: Patients with SCI often prefer SPC than other methods offered to them, because of quality-of-life issues. The incidence of significant complications might not be as high as previously reported, and with a commitment to careful follow-up, SPC can be a safe option for carefully selected patients if adequate surveillance can be ensured.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Hsu, Chih-Wei and Chang, Chih-Chung and Lin, Chih-Jen},
doi = {10.1177/02632760022050997},
eprint = {0-387-31073-8},
file = {:C$\backslash$:/Users/SESA245227/Google Drive/Th{\`{e}}se/Bibliographie/SVM Librairie/A Practical Guide to Support Vector Classification.pdf:pdf},
isbn = {013805326X},
issn = {1464-410X},
journal = {BJU international},
number = {1},
pages = {1396--400},
pmid = {18190633},
title = {{A Practical Guide to Support Vector Classification}},
url = {http://www.csie.ntu.edu.tw/{~}cjlin/papers/guide/guide.pdf},
volume = {101},
year = {2008}
}
@book{Schlkopf2013,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Schlkopf, Bernhard and Smola, Alexander J.},
booktitle = {Journal of Chemical Information and Modeling},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/SESA245227/Google Drive/Th{\`{e}}se/Bibliographie/Livre/Schokopf, Smola-Learning with Kernels.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {icle},
pages = {1689--1699},
pmid = {25246403},
title = {{Learning with Kernels}},
volume = {53},
year = {2013}
}
@inproceedings{Heisele2001,
abstract = {We present a component-based method and two global methods for face recognition and evaluate them with respect to robustness against pose changes. In the component system we first locate facial components, extract them and combine them into a single feature vector which is classified by a Support Vector Machine (SVM). The two global systems recognize faces by classifying a single feature vector consisting of the gray values of the whole face image. In the first global system we trained a single SVM classifier for each person in the database. The second system consists of sets of viewpoint-specific SVM classifiers and involves clustering during training. We performed extensive tests on a database which included faces rotated up to about 40° in depth. The component system clearly outperformed both global systems on all tests.},
author = {Heisele, B and Ho, P and Poggio, T},
booktitle = {IEEE International Conference on Computer Vision, ICCV},
doi = {10.1109/ICCV.2001.937693},
isbn = {0-7695-1143-0},
issn = {1089-7801},
keywords = {Active shape model,Biology computing,Face recognition,Image databases,Image recognition,Mouth,Robustness,SVM classifier,Solid modeling,Support vector machine classification,Support vector machines,clustering,component-based approach,facial components,feature extraction,feature vector,global methods,learning automata},
number = {July},
pages = {688--694},
pmid = {20075471},
title = {{Face recognition with support vector machines: global versus component-based approach}},
url = {http://dx.doi.org/10.1109/ICCV.2001.937693},
volume = {2},
year = {2001}
}
@article{Dietterich1995,
abstract = {The performance of the error backpropagation (BP) and ID3 learning algorithms was compared on the task of mapping English text to phonemes and stresses. Under the distributed output code developed by Sejnowski and Rosenberg, it is shown that BP consistently out-performs ID3 on this task by several percentage points. Three hypotheses explaining this difference were explored: (a) ID3 is overfitting the training data, (b) BP is able to share hidden units across several output units and hence can learn the output units better, and (c) BP captures statistical information that ID3 does not. We conclude that only hypothesis (c) is correct. By augmenting ID3 with a simple statistical learning procedure, the performance of BP can be closely matched. More complex statistical procedures can improve the performance of both BP and ID3 substantially in this domain.},
author = {Dietterich, Thomas G. and Hild, Hermann and Bakiri, Ghulum},
doi = {10.1007/BF00993821},
isbn = {0885-6125},
issn = {08856125},
journal = {Machine Learning},
keywords = {ID3,backpropagation,experimental comparisons,text-to-speech},
number = {1},
pages = {51--80},
title = {{A comparison of ID3 and backpropagation for English text-to-speech mapping}},
volume = {18},
year = {1995}
}
@article{Tan2006,
abstract = {OBJECTIVES: Reductions in exposure to environmental tobacco smoke have been shown to attenuate the risk of cardiovascular disease. We examined whether the 2003 implementation of a comprehensive smoking ban in New York State was associated with reduced hospital admissions for acute myocardial infarction and stroke, beyond the effect of moderate, local and statewide smoking restrictions, and independent of secular trends. METHODS: We analyzed trends in county-level, age-adjusted, monthly hospital admission rates for acute myocardial infarction and stroke from 1995 to 2004 to identify any association between admission rates and implementation of the smoking ban. We used regression models to adjust for the effects of pre-existing smoking restrictions, seasonal trends in admissions, differences across counties, and secular trends. RESULTS: In 2004, there were 3813 fewer hospital admissions for acute myocardial infarction than would have been expected in the absence of the comprehensive smoking ban. Direct health care cost savings of {\$}56 million were realized in 2004. There was no reduction in the number of admissions for stroke. CONCLUSIONS: Hospital admission rates for acute myocardial infarction were reduced by 8{\%} as a result of a comprehensive smoking ban in New York State after we controlled for other relevant factors. Comprehensive smoking bans constitute a simple, effective intervention to substantially improve the public's health},
author = {Tan, Pang-Ning and Steinbach, Michael and Kumar, Vipin},
doi = {10.1016/0022-4405(81)90007-8},
file = {:C$\backslash$:/MyData/Code/Stage/2011 CaoTri/Bibliographie/Machine Learning et Automatique/Chapitre 4 - Classification - Basic Concepts Decision Tree and Model Evaluation.pdf:pdf},
isbn = {0321321367},
issn = {00224405},
journal = {Introduction to Data Mining},
number = {17},
pages = {145--205},
pmid = {20386987},
title = {{Classification : Basic Concepts , Decision Trees , and}},
url = {http://www-users.cs.umn.edu/{~}kumar/dmbook/index.php},
volume = {67},
year = {2006}
}
@article{Sakoe1978a,
abstract = {This paper reports on an optimum dynamic programming (DP) based time-normalization algorithm for spoken word recognition. First, a general principle of time-normalization is given using timewarping function. Then, two time-normalized distance definitions, d e d symmetric and asymmetric forms, are derived from the principle. These two forms are compared with each other through theoretical discussions and experimental studies. The symmetric form algorithm superiority is established. A new technique, called slope constraint, is successfully introduced, iwn hich the warping function slope isr estricted so as to improve discrimination between words in different categories. The effective slope constraint characteristic is qualitatively analyzed, and the optimum slope constraint condition is determined through experiments. The optimized algorithm is then extensively subjected to experimentat comparison with various DP-algorithms, previously applied to spoken word recognition by different research groups. The experiment shows that the present algorithm gives no more than about twothirds errors, even compared to the best conventional algorithm.},
author = {Sakoe, H. and Chiba, S.},
doi = {10.1109/TASSP.1978.1163055},
isbn = {1558601244},
issn = {00963518},
journal = {IEEE transactions on acoustics, speech, and signal processing},
title = {{Dynamic Programming Algorithm Optimization for Spoken Word Recognition}},
year = {1978}
}
@article{Torrence1998,
abstract = {A practical step-by-step guide to wavelet analysis is given, with examples taken from time series of the El Ni{\~{n}}o–Southern Oscillation (ENSO). The guide includes a comparison to the windowed Fourier transform, the choice of an appropriate wavelet basis function, edge effects due to finite-length time series, and the relationship between wavelet scale and Fourier frequency. New statistical significance tests for wavelet power spectra are developed by deriving theoretical wavelet spectra for white and red noise processes and using these to establish significance levels and confidence intervals. It is shown that smoothing in time or scale can be used to increase the confidence of the wavelet spectrum. Empirical formulas are given for the effect of smoothing on significance levels and confidence intervals. Extensions to wavelet analysis such as filtering, the power Hovm{\"{o}}ller, cross-wavelet spectra, and coherence are described. The statistical significance tests are used to give a quantitative measure of changes in ENSO variance on interdecadal timescales. Using new datasets that extend back to 1871, the Ni{\~{n}}o3 sea surface temperature and the Southern Oscillation index show significantly higher power during 1880–1920 and 1960–90, and lower power during 1920–60, as well as a possible 15-yr modulation of variance. The power Hovm{\"{o}}ller of sea level pressure shows significant variations in 2–8-yr wavelet power in both longitude and time.},
author = {Torrence, Christopher and Compo, Gilbert P.},
doi = {10.1175/1520-0477(1998)079<0061:APGTWA>2.0.CO;2},
isbn = {0871706881},
issn = {00030007},
journal = {Bulletin of the American Meteorological Society},
number = {1},
pages = {61--78},
pmid = {21229804},
title = {{A Practical Guide to Wavelet Analysis}},
volume = {79},
year = {1998}
}
@article{Vlachos2006,
abstract = {While most time series data mining research has concentrated on providing solutions for a single distance function, in this work we motivate the need for an index structure that can support multiple distance measures. Our specific area of interest is the efficient retrieval and analysis of similar trajectories. Trajectory datasets are very common in environmental applications, mobility experiments, and video surveillance and are especially important for the discovery of certain biological patterns. Our primary similarity measure is based on the longest common subsequence (LCSS) model that offers enhanced robustness, particularly for noisy data, which are encountered very often in real-world applications. However, our index is able to accommodate other distance measures as well, including the ubiquitous Euclidean distance and the increasingly popular dynamic time warping (DTW). While other researchers have advocated one or other of these similarity measures, a major contribution of our work is the ability to support all these measures without the need to restructure the index. Our framework guarantees no false dismissals and can also be tailored to provide much faster response time at the expense of slightly reduced precision/recall. The experimental results demonstrate that our index can help speed up the computation of expensive similarity measures such as the LCSS and the DTW.},
author = {Vlachos, Michail and Hadjieleftheriou, Marios and Gunopulos, Dimitrios and Keogh, Eamonn},
doi = {10.1007/s00778-004-0144-2},
isbn = {1-58113-737-0},
issn = {10668888},
journal = {VLDB Journal},
keywords = {Dynamic time warping,Ensemble index,Longest common subsequence,Motion capture,Trajectories},
number = {1},
pages = {1--20},
title = {{Indexing multidimensional time-series}},
volume = {15},
year = {2006}
}
@inproceedings{Abraham2010a,
abstract = {Zero-inflated time series data are commonly encountered in many applications, including climate and ecological modeling, disease monitoring, manufacturing defect detection, and traffic monitoring. Such data often leads to poor model fitting using standard regression methods because they tend to underestimate the frequency of zeros and the magnitude of non-zero values. This paper presents an integrated framework that simultaneously performs classification and regression to accurately predict future values of a zero-inflated time series. A regression model is initially applied to predict the value of the time series. The regression output is then fed into a classification model to determine whether the predicted value should be adjusted to zero. Our regression and classification models are trained to optimize a joint objective function that considers both classification errors on the time series and regression errors on data points that have non-zero values. We demonstrate the effectiveness of our framework in the context of its application to a precipitation downscaling problem for climate impact assessment studies. Read More: http://epubs.siam.org/doi/abs/10.1137/1.9781611972801.57},
author = {Abraham, Z. and Tan, P.N.},
booktitle = {ACM SIGKDD},
title = {{An Integrated Framework for Simultaneous Classification and Regression of Time-Series Data}},
url = {https://siam.org/proceedings/datamining/2010/dm10{\_}057{\_}abrahamz.pdf},
year = {2010}
}
@article{Prekopcsak2012,
abstract = {To classify time series by nearest neighbors, we need to specify or learn one or several distance measures. We consider variations of the Mahalanobis distance measures which rely on the inverse covariance matrix of the data. Unfortunately --- for time series data --- the covariance matrix has often low rank. To alleviate this problem we can either use a pseudoinverse, covariance shrinking or limit the matrix to its diagonal. We review these alternatives and benchmark them against competitive methods such as the related Large Margin Nearest Neighbor Classification (LMNN) and the Dynamic Time Warping (DTW) distance. As we expected, we find that the DTW is superior, but the Mahalanobis distance measures are one to two orders of magnitude faster. To get best results with Mahalanobis distance measures, we recommend learning one distance measure per class using either covariance shrinking or the diagonal approach.},
archivePrefix = {arXiv},
arxivId = {1010.1526},
author = {Prekopcs{\'{a}}k, Zolt{\'{a}}n and Lemire, Daniel},
doi = {10.1007/s11634-012-0110-6},
eprint = {1010.1526},
isbn = {1163401201106},
issn = {18625347},
journal = {Advances in Data Analysis and Classification},
keywords = {Distance measure learning,Mahalanobis distance measure,Nearest Neighbor,Time-series classification},
number = {3},
pages = {185--200},
title = {{Time series classification by class-specific Mahalanobis distance measures}},
volume = {6},
year = {2012}
}
@article{Wang2002,
abstract = {Recently a new learning method called support vector machines (SVM) has shown comparable or better results than neural networks on some applications. In this thesis we exploit the possibility of using SVM for three important issues of bioinformatics: the prediction of protein secondary structure, multi-class protein fold recognition, and the prediction of human signal peptide cleavage sites. By using similar data, we demonstrate that SVM can easily achieve comparable accuracy as using neural networks. Therefore, in the future it is a promising direction to apply SVM on more bioinformatics applications.},
author = {Wang, Jung-Ying},
journal = {Bioinformatics},
pages = {1--56},
title = {{Support Vector Machines ( SVM ) in bioinformatics Bioinformatics applications}},
url = {http://www.csie.ntu.edu.tw/{~}p88012/Bio{\_}SVM.pdf},
year = {2002}
}
@inproceedings{Do,
abstract = {In order to classify time series, many machine learning algorithms such as the kNN classier require a metric. We propose in this work a framework to learn a combination of multiple metrics for a robust kNN classier. This combined metric includes both temporal (value and behavior) and frequential components. By introducing the concept of pairwise space, the combination function is learned in this new space through a "large margin" optimization process. The effciency of the learned metric is compared to the major alternative metrics on large public datasets.},
address = {Porto, Portugal},
author = {DO, Cao Tri and Douzal-Chouakria, Ahlame and Mari{\'{e}}, Sylvain and Rombaut, Mich{\`{e}}le},
booktitle = {Workshop on Advanced Analytics and Learning from Temporal Data},
file = {:C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Do et al. - 2015 - Temporal and frequential multiple metric learning for time series kNN classication.pdf:pdf},
keywords = {Classication,Multiple metric learning,Spectral,Time series,classificationcation,kNN,knn,metrics.,multiple metric learning,spectral,time series},
mendeley-tags = {Classication,Multiple metric learning,Spectral,Time series,kNN,metrics.},
pages = {39--45},
title = {{Temporal and frequential multiple metric learning for time series kNN classication}},
volume = {1425},
year = {2015}
}
@article{Ding2008,
abstract = {The last decade has witnessed a tremendous growths of interests in applications that deal with querying and mining of time series data. Numerous representation methods for dimensionality reduction and similarity measures geared towards time series have been introduced. Each individual work introducing a particular method has made specific claims and, aside from the occasional theoretical justifications, provided quantitative experimental observations. However, for the most part, the comparative aspects of these experiments were too narrowly focused on demonstrating the benefits of the proposed methods over some of the previously introduced ones. In order to provide a comprehensive validation, we conducted an extensive set of time series experiments re-implementing 8 different representation methods and 9 similarity measures and their variants, and testing their effectiveness on 38 time series data sets from a wide variety of application domains. In this paper, we give an overview of these different techniques and present our comparative experimental findings regarding their effectiveness. Our experiments have provided both a unified validation of some of the existing achievements, and in some cases, suggested that certain claims in the literature may be unduly optimistic. 1.},
archivePrefix = {arXiv},
arxivId = {1012.2789v1},
author = {Ding, Hui and Trajcevski, Goce and Scheuermann, Peter and Wang, Xiaoyue and Keogh, Eamonn},
doi = {10.1145/1454159.1454226},
eprint = {1012.2789v1},
file = {:C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ding, Trajcevski, Scheuermann - 2008 - Querying and Mining of Time Series Data Experimental Comparison of Representations and Distance.pdf:pdf},
isbn = {0000000000000},
issn = {2150-8097},
journal = {Proceedings of the VLDB Endowment},
number = {2},
pages = {1542--1552},
publisher = {VLDB Endowment},
title = {{Querying and Mining of Time Series Data : Experimental Comparison of Representations and Distance Measures}},
url = {http://dl.acm.org/citation.cfm?id=1454226},
volume = {1},
year = {2008}
}
@article{Goldberger2004,
abstract = {In this paper we propose a novel method for learning a$\backslash$r$\backslash$nMahalanobis distance measure to be used in the KNN classification$\backslash$r$\backslash$nalgorithm. The algorithm directly maximizes a stochastic variant of$\backslash$r$\backslash$nthe leave-one-out KNN score on the training set. It can also$\backslash$r$\backslash$nlearn a low-dimensional linear embedding of labeled data that can$\backslash$r$\backslash$nbe used for data visualization and fast classification.$\backslash$r$\backslash$nUnlike other methods, our classification model is non-parametric,$\backslash$r$\backslash$nmaking no assumptions about the shape of the class distributions or$\backslash$r$\backslash$nthe boundaries between them.  The performance of the method$\backslash$r$\backslash$nis demonstrated on several data sets, both for metric learning and$\backslash$r$\backslash$nlinear dimensionality reduction.},
author = {Goldberger, Jacob and Roweis, Sam T and Hinton, Geoffrey E and Salakhutdinov, Ruslan and Roweis, Sam T and Salakhutdinov, Ruslan},
doi = {10.1.1.108.7841},
journal = {Advances in Neural Information Processing Systems},
keywords = {Learning/Statistics {\&} Optimisation,Theory {\&} Algorithms},
pages = {513--520},
title = {{Neighbourhood Components Analysis}},
url = {http://eprints.pascal-network.org/archive/00001570/},
year = {2004}
}
@article{Keogh2004,
abstract = {The problem of indexing time series has attracted much interest. Most algorithms used to index time series utilize the Euclidean distance or some variation thereof. However, it has been forcefully shown that the Euclidean distance is a very brittle distance measure. Dy- namic time warping (DTW) is a much more robust distance measure for time series, allowing similar shapes to match even if they are out of phase in the time axis. Because of this flexi- bility, DTW is widely used in science, medicine, industry and finance. Unfortunately, however, DTW does not obey the triangular inequality and thus has resisted attempts at exact indexing. Instead, many researchers have introduced approximate indexing techniques or abandoned the idea of indexing and concentrated on speeding up sequential searches. In this work, we intro- duce a novel technique for the exact indexing of DTW. We prove that our method guarantees no false dismissals and we demonstrate its vast superiority over all competing approaches in the largest and most comprehensive set of time series indexing experiments ever undertaken.},
author = {Keogh, Eamonn and Ratanamahatana, Chotirat Ann},
doi = {10.1007/s10115-004-0154-9},
file = {:C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Keogh, Ratanamahatana - 2004 - Exact indexing of dynamic time warping.pdf:pdf},
issn = {0219-1377},
journal = {Knowledge and Information Systems},
keywords = {dynamic time warping,indexing,lower bounding,time series},
month = {may},
number = {3},
pages = {358--386},
title = {{Exact indexing of dynamic time warping}},
url = {http://www.springerlink.com/index/10.1007/s10115-004-0154-9},
volume = {7},
year = {2004}
}
@inproceedings{Do2015,
abstract = {Time series are complex data objects, they may present noise, varying delays or involve several temporal granularities. To classify time series, promising solutions refer to the combination of multiple basic metrics to compare time series according to several characteristics. This work proposes a new framework to learn a combi- nation of multiple metrics for a robust kNN classifier. By introducing the concept of pairwise space, the com- bination function is learned in this new space through a "large margin" optimization process. We apply it to compare time series on both their values and behaviors. The efficiency of the learned metric is compared to the major alternative metrics on large public datasets.},
address = {Nice, France},
author = {DO, Cao Tri and Douzal-Chouakria, Ahlame and Mari{\'{e}}, Sylvain and Rombaut, Mich{\`{e}}le},
booktitle = {Signal Processing Conference (EUSIPCO), 2015 Proceedings of the 23rd European},
file = {:C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Do et al. - 2015 - Multiple Metric Learning for large margin kNN Classification of time series.pdf:pdf},
keywords = {Classification,Multiple metric learning,Time series,kNN},
mendeley-tags = {Classification,Multiple metric learning,Time series,kNN},
pages = {2391 -- 2395},
title = {{Multiple Metric Learning for large margin kNN Classification of time series}},
year = {2015}
}
@inproceedings{Chopra2005,
abstract = {We present a method for training a similarity metric from data. The method can be used for recognition or verification applications where the number of categories is very large and not known during training, and where the number of training samples for a single category is very small. The idea is to learn a function that maps input patterns into a target space such that the L1 norm in the target space approximates the "semantic" distance in the input space. The method is applied to a face verification task. The learning process minimizes a discriminative loss function that drives the similarity metric to be small for pairs of faces from the same person, and large for pairs from different persons. The mapping from raw to the target space is a convolutional network whose architecture is designed for robustness to geometric distortions. The system is tested on the Purdue/AR face database which has a very high degree of variability in the pose, lighting, expression, position, and artificial occlusions such as dark glasses and obscuring scarves.},
author = {Chopra, Sumit and Hadsell, Raia and LeCun, Yann},
booktitle = {CVPR},
doi = {10.1109/CVPR.2005.202},
isbn = {0769523722},
issn = {10636919},
pages = {539--546},
title = {{Learning a similarity metric discriminatively, with application to face verification}},
volume = {1},
year = {2005}
}
