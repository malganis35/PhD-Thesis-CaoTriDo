Automatically generated by Mendeley Desktop 1.14
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Aizerman1964,
abstract = {Introduction of kernels},
author = {Aizerman, M. and Braverman, E. and Rozonoer, L.},
journal = {Automation and Remote Control},
pages = {821--837},
title = {{Theoretical foundations of the potential function method in pattern recognition learning}},
volume = {25},
year = {1964}
}
@inproceedings{Boser1992,
abstract = {A training algorithm that maximizes the margin  between the training patterns and the decision  boundary is presented. The technique  is applicable to a wide variety of classifiaction  functions, including Perceptrons, polynomials,  and Radial Basis Functions. The effective  number of parameters is adjusted automatically  to match the complexity of the problem.  The solution is expressed as a linear combination  of supporting patterns. These are the  subset of training patterns that are closest to  the decision boundary. Bounds on the generalization  performance based on the leave-one-out  method and the VC-dimension are given. Experimental  results on optical character recognition  problems demonstrate the good generalization  obtained when compared with other  learning algorithms.  1 INTRODUCTION  Good generalization performance of pattern classifiers is achieved when the capacity of the classification function is matched to the size of the training set. Classifiers with a large numb...},
author = {Boser, Bernhard E. and Guyon, Isabelle M. and Vapnik, Vladimir N.},
booktitle = {Proceedings of the 5th Annual ACM Workshop on Computational Learning Theory},
doi = {10.1.1.21.3818},
isbn = {089791497X},
issn = {0-89791-497-X},
pages = {144--152},
title = {{A Training Algorithm for Optimal Margin Classifiers}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.21.3818},
year = {1992}
}
@article{Cortes1995,
abstract = {The support-vector network is a new leaming machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high- dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data. High generalization ability of support-vector networks utilizing polynomial input transformations is demon- strated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Cortes, Corinna and Vapnik, Vladimir},
doi = {10.1007/BF00994018},
eprint = {arXiv:1011.1669v3},
isbn = {0885-6125},
issn = {08856125},
journal = {Machine Learning},
keywords = {efficient learning algorithms,neural networks,pattern recognition,polynomial classifiers,radial basis function classifiers},
number = {3},
pages = {273--297},
pmid = {9052598814225336358},
title = {{Support-vector networks}},
volume = {20},
year = {1995}
}
@inproceedings{Do,
abstract = {In order to classify time series, many machine learning algorithms such as the kNN classier require a metric. We propose in this work a framework to learn a combination of multiple metrics for a robust kNN classier. This combined metric includes both temporal (value and behavior) and frequential components. By introducing the concept of pairwise space, the combination function is learned in this new space through a "large margin" optimization process. The effciency of the learned metric is compared to the major alternative metrics on large public datasets.},
address = {Porto, Portugal},
author = {DO, Cao Tri and Douzal-Chouakria, Ahlame and Mari\'{e}, Sylvain and Rombaut, Mich\`{e}le},
booktitle = {Workshop on Advanced Analytics and Learning from Temporal Data},
file = {:C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Do et al. - 2015 - Temporal and frequential multiple metric learning for time series kNN classication.pdf:pdf},
keywords = {Classication,Multiple metric learning,Spectral,Time series,classificationcation,kNN,knn,metrics.,multiple metric learning,spectral,time series},
mendeley-tags = {Classication,Multiple metric learning,Spectral,Time series,kNN,metrics.},
pages = {39--45},
title = {{Temporal and frequential multiple metric learning for time series kNN classication}},
volume = {1425},
year = {2015}
}
@book{Campbell2011,
author = {Campbell, Colin and Ying, Yiming},
booktitle = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
doi = {10.2200/S00324ED1V01Y201102AIM010},
file = {:C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Campbell, Ying - 2011 - Learning with Support Vector Machines.pdf:pdf},
isbn = {9781608456161},
issn = {1939-4608},
month = feb,
number = {1},
pages = {1--95},
title = {{Learning with Support Vector Machines}},
url = {http://www.morganclaypool.com/doi/abs/10.2200/S00324ED1V01Y201102AIM010},
volume = {5},
year = {2011}
}
@inproceedings{Do2015,
abstract = {Time series are complex data objects, they may present noise, varying delays or involve several temporal granularities. To classify time series, promising solutions refer to the combination of multiple basic metrics to compare time series according to several characteristics. This work proposes a new framework to learn a combi- nation of multiple metrics for a robust kNN classifier. By introducing the concept of pairwise space, the com- bination function is learned in this new space through a "large margin" optimization process. We apply it to compare time series on both their values and behaviors. The efficiency of the learned metric is compared to the major alternative metrics on large public datasets.},
address = {Nice, France},
author = {DO, Cao Tri and Douzal-Chouakria, Ahlame and Mari\'{e}, Sylvain and Rombaut, Mich\`{e}le},
booktitle = {Signal Processing Conference (EUSIPCO), 2015 Proceedings of the 23rd European},
file = {:C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Do et al. - 2015 - Multiple Metric Learning for large margin kNN Classification of time series.pdf:pdf},
keywords = {Classification,Multiple metric learning,Time series,kNN},
mendeley-tags = {Classification,Multiple metric learning,Time series,kNN},
pages = {2391 -- 2395},
title = {{Multiple Metric Learning for large margin kNN Classification of time series}},
year = {2015}
}
@book{Duda1973,
abstract = {Classic book on pattern recognition. Interesting points: 1) p. 66, and p. 114: Mentions the problems with dimensionality curse. 2) p. 243-246: Mentions Multidimensional scaling (MDS), Karhunen-Loeve and dimensionality reduction. Also, has the spiral data-set as a sample. 3) p. 333: mentions SVD/eigenvalues for linear fitting.},
author = {{O Duda}, Richard and {E Hart}, Peter},
booktitle = {Leonardo},
doi = {10.2307/1573081},
isbn = {0471223611},
issn = {0024094X},
pages = {482},
title = {{Pattern Classification and Scene Analysis}},
url = {http://www.jstor.org/stable/1573081?origin=crossref},
volume = {7},
year = {1973}
}
@article{Weinberger2009,
abstract = {The accuracy of k-nearest neighbor (kNN) classification depends significantly on the metric used to compute distances between different examples. In this paper, we show how to learn a Maha- lanobis distance metric for kNN classification from labeled examples. The Mahalanobis metric can equivalently be viewed as a global linear transformation of the input space that precedes kNN classification using Euclidean distances. In our approach, the metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a largemargin. As in support vectormachines (SVMs), themargin criterion leads to a convex optimization based on the hinge loss. Unlike learning in SVMs, however, our approach re- quires no modification or extension for problems in multiway (as opposed to binary) classification. In our framework, the Mahalanobis distance metric is obtained as the solution to a semidefinite program. On several data sets of varying size and difficulty, we find that metrics trained in this way lead to significant improvements in kNN classification. Sometimes these results can be further improved by clustering the training examples and learning an individual metric within each cluster. We show how to learn and combine these local metrics in a globally integrated manner. Keywords: convex optimization, semi-definite programming,Mahalanobis distance,metric learn- ing, multi-class classification, support vector machines 1.},
author = {Weinberger, K. and Saul, L.},
file = {:C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Weinberger, Saul - 2009 - Distance Metric Learning for Large Margin Nearest Neighbor Classification.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {convex optimization,ing,mahalanobis distance,metric learn-,multi-class classification,semi-definite programming,support vector machines},
pages = {207--244},
title = {{Distance Metric Learning for Large Margin Nearest Neighbor Classification}},
url = {http://www.jmlr.org/papers/volume10/weinberger09a/weinberger09a.pdf},
volume = {10},
year = {2009}
}
@inproceedings{Najmeddine2012,
author = {Najmeddine, H. and Jay, A. and Marechal, P. and Mari\'{e}, S.},
booktitle = {RFIA},
file = {:C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Najmeddine et al. - 2012 - Mesures de similarit\'{e} pour l’aide \`{a} l’analyse des donn\'{e}es \'{e}nerg\'{e}tiques de b\^{a}timents.pdf:pdf},
isbn = {9782953951523},
keywords = {Data mining,INCAS.,Time series,diagnosis and decision support,sensors,similarity measures},
title = {{Mesures de similarit\'{e} pour l’aide \`{a} l’analyse des donn\'{e}es \'{e}nerg\'{e}tiques de b\^{a}timents}},
url = {https://hal-cea.archives-ouvertes.fr/file/index/docid/661016/filename/article53\_modif.pdf},
year = {2012}
}
@article{Montero2014,
abstract = {Time series clustering is an active research area with applications in a wide range of fields. One key component in cluster analysis is determining a proper dissimilarity mea- sure between two data objects, and many criteria have been proposed in the literature to assess dissimilarity between two time series. The R package TSclust is aimed to im- plement a large set of well-established peer-reviewed time series dissimilarity measures, including measures based on raw data, extracted features, underlying parametric models, complexity levels, and forecast behaviors. Computation of these measures allows the user to perform clustering by using conventional clustering algorithms. TSclust also includes a clustering procedure based on p values from checking the equality of generating models, and some utilities to evaluate cluster solutions. The implemented dissimilarity functions are accessible individually for an easier extension and possible use out of the clustering context. The main features of TSclust are described and examples of its use are presented.},
author = {Montero, Pablo and Vilar, Jos\'{e}},
file = {:C$\backslash$:/Users/SESA245227/Desktop/TS clust.pdf:pdf},
journal = {Journal of Statistical Software November},
keywords = {clustering,dissimilarity measure,time series data,validation indices},
number = {1},
title = {{TSclust : An R Package for Time Series Clustering}},
url = {http://www.jstatsoft.org/v62/i01/paper},
volume = {62},
year = {2014}
}
@book{Tan2005b,
abstract = {-This paper is review of current usage of data mining, machine learning and other algorithms for credit risk assessment. We are witnessing importance of credit risk assessment, especially after the global economic crisis on 2008.S o, it is very important to have a proper way to deal with the credit risk and provide powerful and accurate model for credit risk assessment. Many credit scoring techniques such as statistical techniques (logistic regression, discriminant analysis) or advanced techniques such as neural networks, decision trees, genetic algorithm, or support vector machines are used for credit risk assessment. Some of them are described in this article with theirs advantages/disadvantages. Even with many models and methods, it is still hard to say which model is the best or which classifier or which data mining technique is the best. Each model depends on particular data set or attributes set, so it is very important to develop flexible model which is adaptable to every dataset or attribute set.},
author = {Tan, Pang-Ning and Steinbach, Michael and Kumar, Vipin},
booktitle = {Addison Wesley},
isbn = {9789604743179},
keywords = {- credit risk assessments,credit scoring techniques,single classifiers},
pages = {500},
title = {{Introduction to Data Mining}},
year = {2005}
}
@misc{Keogh2011,
author = {Keogh, E. and Zhu, Q. and Hu, B. and Hao, Y. and Xi, X. and Wei, L. and Ratanamahatana, C.A.},
title = {{The UCR Time Series Classification/Clustering Homepage}},
url = {www.cs.ucr.edu/~eamonn/time\_series\_data/},
year = {2011}
}
@book{Chatfield2004,
abstract = {"Since 1975, The Analysis of Time Series: An Introduction has introduced legions of statistics students and researchers to the theory and practice of time series analysis. The sixth edition provides an accessible, comprehensive introduction to the theory and practice of time series analysis. The treatment covers a wide range of topics, including ARIMA probability models, forecasting methods, spectral analysis, linear systems, state-space models, and the Kalman filter. It also addresses nonlinear, multivariate, and long-memory models. The author has carefully updated each chapter, added new discussions, incorporated new datasets, and made those datasets available at www.crcpress.com."--BOOK JACKET.},
author = {Chatfield, Christopher},
booktitle = {Texts in statistical science},
isbn = {1584883170},
keywords = {Time-series analysis.},
pages = {xiii, 333 p.},
pmid = {13166316},
title = {{The analysis of time series : an introduction}},
year = {2004}
}
@article{Ding2008,
abstract = {The last decade has witnessed a tremendous growths of interests in applications that deal with querying and mining of time series data. Numerous representation methods for dimensionality reduction and similarity measures geared towards time series have been introduced. Each individual work introducing a particular method has made specific claims and, aside from the occasional theoretical justifications, provided quantitative experimental observations. However, for the most part, the comparative aspects of these experiments were too narrowly focused on demonstrating the benefits of the proposed methods over some of the previously introduced ones. In order to provide a comprehensive validation, we conducted an extensive set of time series experiments re-implementing 8 different representation methods and 9 similarity measures and their variants, and testing their effectiveness on 38 time series data sets from a wide variety of application domains. In this paper, we give an overview of these different techniques and present our comparative experimental findings regarding their effectiveness. Our experiments have provided both a unified validation of some of the existing achievements, and in some cases, suggested that certain claims in the literature may be unduly optimistic. 1.},
archivePrefix = {arXiv},
arxivId = {1012.2789v1},
author = {Ding, Hui and Trajcevski, Goce and Scheuermann, Peter and Wang, Xiaoyue and Keogh, Eamonn},
doi = {10.1145/1454159.1454226},
eprint = {1012.2789v1},
file = {:C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ding, Trajcevski, Scheuermann - 2008 - Querying and Mining of Time Series Data Experimental Comparison of Representations and Distance.pdf:pdf},
isbn = {0000000000000},
issn = {2150-8097},
journal = {Proceedings of the VLDB Endowment},
number = {2},
pages = {1542--1552},
publisher = {VLDB Endowment},
title = {{Querying and Mining of Time Series Data : Experimental Comparison of Representations and Distance Measures}},
url = {http://dl.acm.org/citation.cfm?id=1454226},
volume = {1},
year = {2008}
}
@inproceedings{Nguyen2012,
abstract = {More and more people express their opinions on social media such as Facebook and Twitter. Predictive analysis on social media time-series allows the stake-holders to leverage this immediate, accessible and vast reachable communication channel to react and proact against the public opinion. In particular, understanding and predicting the sentiment change of the public opinions will allow business and government agencies to react against negative sentiment and design strategies such as dispelling rumors and post balanced messages to revert the public opinion. In this paper, we present a strategy of building statistical models from the social media dynamics to predict collective sentiment dynamics. We model the collective sentiment change without delving into micro analysis of individual tweets or users and their corresponding low level network structures. Experiments on large-scale Twitter data show that the model can achieve above 85\% accuracy on directional sentiment prediction.},
author = {Nguyen, L. and Wu, P. and Chan, W. and Peng, W. and Zhang, Y.},
booktitle = {WISDOM},
doi = {10.1145/2346676.2346682},
isbn = {9781450315432},
keywords = {sentiment analysis,sentiment prediction,social network analysis},
title = {{Predicting collective sentiment dynamics from time-series social media}},
year = {2012}
}
@misc{LIG2014,
title = {{LIG-AMA Machine Learning Datasets Repository}},
url = {http://ama.liglab.fr/resourcestools/datasets/},
year = {2014}
}
@inproceedings{Yin2008,
abstract = {Event detection is a critical task in sensor networks, especially for environmental monitoring applications. Traditional solutions to event detection are based on analyzing one-shot data points, which might incur a high false alarm rate because sensor data is inherently unreliable and noisy. To address this issue, we propose a novel Distributed Single-pass Incremental Clustering (DSIC) technique to cluster the time series obtained at sensor nodes based on their underlying trends. In order to achieve scalability and energy-efficiency, our DSIC technique uses a hierarchical structure of sensor networks as the underlying infrastructure. The algorithm first compresses the time series produced at individual sensor nodes into a compact representation using Haar wavelet transform, and then, based on dynamic time warping distances, hierarchically groups the approximate time series into a global clustering model in an incremental manner. Experimental results on both real data and synthetic data demonstrate that our DSIC algorithm is accurate, energy-efficient and robust with respect to network topology changes.},
author = {Yin, J. and Gaber, M.},
booktitle = {ICDM},
doi = {10.1109/ICDM.2008.58},
isbn = {9780769535029},
issn = {15504786},
title = {{Clustering distributed time series in sensor networks}},
year = {2008}
}
@inproceedings{Xi2006a,
abstract = {Many algorithms have been proposed for the problem of time series classification. However, it is clear that one-nearest-neighbor with Dynamic Time Warping (DTW) distance is exceptionally difficult to beat. This approach has one weakness, however; it is computationally too demanding for many realtime applications. One way to mitigate this problem is to speed up the DTW calculations. Nonetheless, there is a limit to how much this can help. In this work, we propose an additional technique, numerosity reduction, to speed up one-nearest-neighbor DTW. While the idea of numerosity reduction for nearest-neighbor classifiers has a long history, we show here that we can leverage off an original observation about the relationship between dataset size and DTW constraints to produce an extremely compact dataset with little or no loss in accuracy. We test our ideas with a comprehensive set of experiments, and show that it can efficiently produce extremely fast accurate classifiers.},
author = {Xi, Xiaopeng and Keogh, Eamonn and Shelton, Christian and Wei, Li and Ratanamahatana, Chotirat Ann},
booktitle = {Proceedings of the 23rd international conference on Machine learning (ICML)},
doi = {10.1145/1143844.1143974},
isbn = {1595933832},
pages = {1033----1040},
title = {{Fast time series classification using numerosity reduction}},
url = {http://dl.acm.org/citation.cfm?id=1143974},
year = {2006}
}
@book{Dreyfus2006,
abstract = {En une vingtaine d’ann\'{e}es, l’apprentissage artificiel est devenu une branche majeure des math\'{e}matiques appliqu\'{e}es, \`{a} l’intersection des statistiques et de l’intelligence artificielle. Son objectif est de r\'{e}aliser des mod\`{e}les qui apprennent « par l’exemple » : il s’appuie sur des donn\'{e}es num\'{e}riques (r\'{e}sultats de mesures ou de simulations), contrairement aux mod\`{e}les « de connaissances » qui s’appuient sur des \'{e}quations issues des premiers principes de la physique, de la chimie, de la biologie, de l’\'{e}conomie, etc. L’apprentis- sage statistique est d’une grande utilit\'{e} lorsque l’on cherche \`{a} mod\'{e}liser des processus complexes, souvent non lin\'{e}aires, pour lesquels les connaissances th\'{e}oriques sont trop impr\'{e}cises pour permettre des pr\'{e}dictions pr\'{e}cises. Ses domaines d’applications sont multiples : fouille de donn\'{e}es, bio-informatique, g\'{e}nie des proc\'{e}d\'{e}s, aide au diagnostic m\'{e}dical, t\'{e}l\'{e}communications, interface cerveau-machines, et bien d’autres. Cet ouvrage refl\`{e}te en partie l’\'{e}volution de cette discipline, depuis ses balbutiements au d\'{e}but des ann\'{e}es 1980, jusqu’\`{a} sa situation actuelle ; il n’a pas du tout la pr\'{e}tention de faire un point, m\^{e}me partiel, sur l’ensemble des d\'{e}veloppements pass\'{e}s et actuels, mais plut\^{o}t d’insister sur les principes et sur les m\'{e}thodes \'{e}prouv\'{e}s, dont les bases scientifiques sont s\^{u}res. Dans un domaine sans cesse parcouru de modes multiples et \'{e}ph\'{e}m\`{e}res, il est utile, pour qui cherche \`{a} acqu\'{e}rir les connaissances et principes de base, d’insister sur les aspects p\'{e}rennes du domaine. Cet ouvrage fait suite \`{a} R\'{e}seaux de neurones, m\'{e}thodologies et applications, des m\^{e}mes auteurs, paru en 2000, r\'{e}\'{e}dit\'{e} en 2004, chez le m\^{e}me \'{e}diteur, puis publi\'{e} en traduction anglaise chez Springer. Consacr\'{e} essentiellement aux r\'{e}seaux de neurones et aux cartes auto-adaptatives, il a largement contribu\'{e} \`{a} populariser ces techniques et \`{a} convaincre leurs utilisateurs qu’il est possible d’obtenir des r\'{e}sultats remarquables, \`{a} condition de mettre en \oe uvre une m\'{e}thodologie de conception rigoureuse, scientifique- ment fond\'{e}e, dans un domaine o\`{u} l’empirisme a longtemps tenu lieu de m\'{e}thode. Tout en restant fid\`{e}le \`{a} l’esprit de cet ouvrage, combinant fondements math\'{e}matiques et m\'{e}thodologie de mise en \oe uvre, les auteurs ont \'{e}largi le champ de la pr\'{e}sentation, afin de permettre au lecteur d’aborder d’autres m\'{e}thodes d’apprentissage statistique que celles qui sont directement d\'{e}crites dans cet ouvrage. En effet, les succ\`{e}s de l’apprentissage dans un grand nombre de domaines ont pouss\'{e} au d\'{e}veloppement de tr\`{e}s nombreuses variantes, souvent destin\'{e}es \`{a} r\'{e}pondre efficacement aux exigences de telle ou telle classe d’applications. Toutes ces variantes ont n\'{e}anmoins des bases th\'{e}oriques et des aspects m\'{e}thodolo- giques communs, qu’il est important d’avoir pr\'{e}sents \`{a} l’esprit. Le terme d’apprentissage, comme celui de r\'{e}seau de neurones, \'{e}voque \'{e}videmment le fonctionnement du cerveau. Il ne faut pourtant pas s’attendre \`{a} trouver ici d’explications sur les m\'{e}canismes de traitement des informations dans les syst\`{e}mes nerveux ; ces derniers sont d’une grande complexit\'{e}, r\'{e}sultant de processus \'{e}lectriques et chimiques subtils, encore mal compris en d\'{e}pit de la grande quantit\'{e} de donn\'{e}es exp\'{e}rimentales disponibles. Si les m\'{e}thodes d’apprentissage statistique peuvent \^{e}tre d’une grande utilit\'{e} pour cr\'{e}er des mod\`{e}les empiriques de telle ou telle fonction r\'{e}alis\'{e}e par le syst\`{e}me nerveux, celles qui sont d\'{e}crites dans cet ouvrage n’ont aucunement la pr\'{e}tention d’imiter, m\^{e}me vaguement, le fonctionne- ment du cerveau. L’apprentissage artificiel, notamment statistique, permettra-t-il un jour de donner aux ordinateurs des capacit\'{e}s analogues \`{a} celles des \^{e}tres humains ? Se rapprochera-t-on de cet objectif en perfectionnant les techniques actuelles d’apprentissage, ou bien des approches radicalement nouvelles sont-elles indispensables ? Faut-il s’inspirer de ce que l’on sait, ou croit savoir, sur le fonctionnement du cerveau ? Ces questions font l’objet de d\'{e}bats passionn\'{e}s, et passionnants, au sein de la communaut\'{e} scientifique : on n’en trouvera pas les r\'{e}ponses ici.},
author = {{G. Dreyfus, J.-M. Martinez, M. Samuelides M. B. Gordon, F. Badran}, S. Thiria},
edition = {Eyrolles},
file = {:C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/G. Dreyfus, J.-M. Martinez, M. Samuelides M. B. Gordon, F. Badran - 2006 - Apprentissage Apprentissage statistique.pdf:pdf},
isbn = {9782212114645},
keywords = {Bio-ing\'{e}nierie,Machine \`{a} Vecteurs Supports,Pr\'{e}vision,Reconaissance de formes,Robotique et commande de processus,R\'{e}seaux de neurones,cartes topologiques,data mining},
mendeley-tags = {Machine \`{a} Vecteurs Supports,R\'{e}seaux de neurones,cartes topologiques},
pages = {471},
title = {{Apprentissage Apprentissage statistique}},
year = {2006}
}
@book{Schlkopf2013,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 \AA for the interface backbone atoms) increased from 21\% with default Glide SP settings to 58\% with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63\% success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40\% of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Schlkopf, Bernhard and Smola, Alexander J.},
booktitle = {Journal of Chemical Information and Modeling},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/SESA245227/Google Drive/Th\`{e}se/Bibliographie/Livre/Schokopf, Smola-Learning with Kernels.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {icle},
pages = {1689--1699},
pmid = {25246403},
title = {{Learning with Kernels}},
volume = {53},
year = {2013}
}
