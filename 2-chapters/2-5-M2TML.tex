\chapter{$M^2TML$: formalization}
\label{sec:unchapitre}
\minitoc

\noindent Chapeau introductif
\begin{itemize}
	\item Rappeler : quel problème on résout : pull des targets et push des individus de classes différentes
	\item Formaliser le problème général avec D
\end{itemize}

\section{LP optimization problem}
\begin{itemize}
	\item Formaliser le problème sous forme d'un problème d'optimisation sous contraintes
\end{itemize}

\section{QP optimization problem}
\begin{itemize}
	\item Passer de la forme LP (forme primale) et par transformation, arriver à la forme duale
	\item Montrer les similitudes avec la résolution SVM
	\item Montrer que l'on peut kerneliser la méthode
\end{itemize}


\section{SVM approximation}
\begin{itemize}
	\item Faire remarquer que le problème LP ressemble à un problème SVM
	\item Faire la démonstration de l'équivalence (ou mettre la démonstration en annexe).
	\item Expliquer les différences entre la résolution LP/QP et la résolution SVM. (ajout de sur-contraintes dans le problème SVM)
	\item Expliquer pourquoi on va préférer le cadre SVM. Expliquer mathématiquement et avec des interprétations géométriques. 
	\item Cadre connu
	\item Utilisation de librairie standard de Machine Learning
	\item Extension directe à l'apprentissage de métrique non linéaire grâce au kernel trick
\end{itemize}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../roque-phdthesis"
%%% End: 
