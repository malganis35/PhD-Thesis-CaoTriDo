\chapter{Log Normalization}
\label{chap:publications}

\todo[inline]{Compléter avec le mail de Sylvain. Partie peut être à mettre en annexe}
Avec Cao on a discuté de la normalisation log hier et du coup j'ai regardé comment la justifier un peu mieux. Voici une intuition "avec les mains" montrant que l'on peut approximer la distribution de la distance euclidienne DE avec une loi log-normale:


Version simple : 1 feature, 
1. On considère pour simplifier des données à 1 dimension (1 feature pour les vecteurs d'exemples). Soit x cette feature. On considère qu'elle suit une loi normale de variance (1/2) et de moyenne Mu.

2. la différence entre deux exemples x et y,  d = (x-y) = (x + (-y)), suit donc une loi normale de moyenne zéro et de variance 1.  Explication:
* inverse de lois normale y = loi normale avec inverse de la moyenne et même variance ; 
* puis somme de deux lois normales (x) et (-y) = loi normale de moyenne égale à la somme des moyennes et variance égale à la somme des variances 
Voir: \url{https://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables}

3. Le carré de la distance euclidienne entre deux exemples x et y est $DE^2 = (x-y)^2 = d^2$ . Puisque d suit une loi normale de variance 1, $DE^2$ suit une loi du $chi^2$ non centrée d'ordre k=1.
Voir: \url{https://fr.wikipedia.org/wiki/Loi_du_%CF%87%C2%B2_non_centr%C3%A9e}
	
	4. Certaines personnes ont prouvé que la loi du $Xhi^2$ peut être approximée par une loi log-normale : \url{http://www.rennes.supelec.fr/ren/perso/cmoy/papers/URSI_4pages.pdf}
	(il faudrait regarder un peu mieux si il y a des hypothèses là-dedans)
	
	5. puisque $log(DE) = log(sqrt(DE^2)) = 1/2*log(DE^2)$  , alors log(DE) suit une loi normale (car la loi d'une variable normale multipliée par une constante est normale). 
	
	=> DE peut donc être approximée par une loi log-normale
	
	
	Extension à DE sur plusieurs features avec covariance diagonale égale à (1/2)I
	1. on considère qu'elles sont tirées d'une distribution jointe normale multivariée (une gaussienne en n dimension, avec covariance identité*1/2)
	2. fonctionne encore 
	3. Fonctionne encore : "la loi du $chi^2$ non centrée est le carré de la norme d'un vecteur aléatoire de loi N(mean, covariance=Identité). Simplement cela augmente l'ordre k de la distribution, k devient le nombre de features.
	
	
	Extension à (co-)variances non (1/2) 
	mon intuition est que si une feature a une variance beaucoup plus importante que les autres elle va "tuer" la variabilité des autres et donc ça va faire comme si il n'y avait qu'une seule feature.
	Du coup je pense que l'on retombe sur une loi du $chi^2$, mais avec un ordre k plus réduit (k < nb de features)
	
	
	\url{https://mail.google.com/mail/u/0/#search/sylvain+normalisation/14fb15fd972e75bc}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../roque-phdthesis"
%%% End: 
