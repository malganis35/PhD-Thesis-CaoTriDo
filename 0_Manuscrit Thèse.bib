Automatically generated by Mendeley Desktop 1.14
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@book{Chatfield2004,
abstract = {"Since 1975, The Analysis of Time Series: An Introduction has introduced legions of statistics students and researchers to the theory and practice of time series analysis. The sixth edition provides an accessible, comprehensive introduction to the theory and practice of time series analysis. The treatment covers a wide range of topics, including ARIMA probability models, forecasting methods, spectral analysis, linear systems, state-space models, and the Kalman filter. It also addresses nonlinear, multivariate, and long-memory models. The author has carefully updated each chapter, added new discussions, incorporated new datasets, and made those datasets available at www.crcpress.com."--BOOK JACKET.},
author = {Chatfield, Christopher},
booktitle = {Texts in statistical science},
isbn = {1584883170},
keywords = {Time-series analysis.},
pages = {xiii, 333 p.},
pmid = {13166316},
title = {{The analysis of time series : an introduction}},
year = {2004}
}
@article{Ding2008,
abstract = {The last decade has witnessed a tremendous growths of interests in applications that deal with querying and mining of time series data. Numerous representation methods for dimensionality reduction and similarity measures geared towards time series have been introduced. Each individual work introducing a particular method has made specific claims and, aside from the occasional theoretical justifications, provided quantitative experimental observations. However, for the most part, the comparative aspects of these experiments were too narrowly focused on demonstrating the benefits of the proposed methods over some of the previously introduced ones. In order to provide a comprehensive validation, we conducted an extensive set of time series experiments re-implementing 8 different representation methods and 9 similarity measures and their variants, and testing their effectiveness on 38 time series data sets from a wide variety of application domains. In this paper, we give an overview of these different techniques and present our comparative experimental findings regarding their effectiveness. Our experiments have provided both a unified validation of some of the existing achievements, and in some cases, suggested that certain claims in the literature may be unduly optimistic. 1.},
archivePrefix = {arXiv},
arxivId = {1012.2789v1},
author = {Ding, Hui and Trajcevski, Goce and Scheuermann, Peter and Wang, Xiaoyue and Keogh, Eamonn},
doi = {10.1145/1454159.1454226},
eprint = {1012.2789v1},
file = {:C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ding, Trajcevski, Scheuermann - 2008 - Querying and Mining of Time Series Data Experimental Comparison of Representations and Distance.pdf:pdf},
isbn = {0000000000000},
issn = {2150-8097},
journal = {Proceedings of the VLDB Endowment},
number = {2},
pages = {1542--1552},
publisher = {VLDB Endowment},
title = {{Querying and Mining of Time Series Data : Experimental Comparison of Representations and Distance Measures}},
url = {http://dl.acm.org/citation.cfm?id=1454226},
volume = {1},
year = {2008}
}
@article{Montero2014,
abstract = {Time series clustering is an active research area with applications in a wide range of fields. One key component in cluster analysis is determining a proper dissimilarity mea- sure between two data objects, and many criteria have been proposed in the literature to assess dissimilarity between two time series. The R package TSclust is aimed to im- plement a large set of well-established peer-reviewed time series dissimilarity measures, including measures based on raw data, extracted features, underlying parametric models, complexity levels, and forecast behaviors. Computation of these measures allows the user to perform clustering by using conventional clustering algorithms. TSclust also includes a clustering procedure based on p values from checking the equality of generating models, and some utilities to evaluate cluster solutions. The implemented dissimilarity functions are accessible individually for an easier extension and possible use out of the clustering context. The main features of TSclust are described and examples of its use are presented.},
author = {Montero, Pablo and Vilar, Jos\'{e}},
file = {:C$\backslash$:/Users/SESA245227/Desktop/TS clust.pdf:pdf},
journal = {Journal of Statistical Software November},
keywords = {clustering,dissimilarity measure,time series data,validation indices},
number = {1},
title = {{TSclust : An R Package for Time Series Clustering}},
url = {http://www.jstatsoft.org/v62/i01/paper},
volume = {62},
year = {2014}
}
@misc{Keogh2011,
author = {Keogh, E. and Zhu, Q. and Hu, B. and Hao, Y. and Xi, X. and Wei, L. and Ratanamahatana, C.A.},
title = {{The UCR Time Series Classification/Clustering Homepage}},
url = {www.cs.ucr.edu/~eamonn/time\_series\_data/},
year = {2011}
}
@misc{LIG2014,
title = {{LIG-AMA Machine Learning Datasets Repository}},
url = {http://ama.liglab.fr/resourcestools/datasets/},
year = {2014}
}
@inproceedings{Do,
abstract = {In order to classify time series, many machine learning algorithms such as the kNN classier require a metric. We propose in this work a framework to learn a combination of multiple metrics for a robust kNN classier. This combined metric includes both temporal (value and behavior) and frequential components. By introducing the concept of pairwise space, the combination function is learned in this new space through a "large margin" optimization process. The effciency of the learned metric is compared to the major alternative metrics on large public datasets.},
address = {Porto, Portugal},
author = {DO, Cao Tri and Douzal-Chouakria, Ahlame and Mari\'{e}, Sylvain and Rombaut, Mich\`{e}le},
booktitle = {Workshop on Advanced Analytics and Learning from Temporal Data},
file = {:C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Do et al. - 2015 - Temporal and frequential multiple metric learning for time series kNN classication.pdf:pdf},
keywords = {Classication,Multiple metric learning,Spectral,Time series,classificationcation,kNN,knn,metrics.,multiple metric learning,spectral,time series},
mendeley-tags = {Classication,Multiple metric learning,Spectral,Time series,kNN,metrics.},
pages = {39--45},
title = {{Temporal and frequential multiple metric learning for time series kNN classication}},
volume = {1425},
year = {2015}
}
@inproceedings{Do2015,
abstract = {Time series are complex data objects, they may present noise, varying delays or involve several temporal granularities. To classify time series, promising solutions refer to the combination of multiple basic metrics to compare time series according to several characteristics. This work proposes a new framework to learn a combi- nation of multiple metrics for a robust kNN classifier. By introducing the concept of pairwise space, the com- bination function is learned in this new space through a "large margin" optimization process. We apply it to compare time series on both their values and behaviors. The efficiency of the learned metric is compared to the major alternative metrics on large public datasets.},
address = {Nice, France},
author = {DO, Cao Tri and Douzal-Chouakria, Ahlame and Mari\'{e}, Sylvain and Rombaut, Mich\`{e}le},
booktitle = {Signal Processing Conference (EUSIPCO), 2015 Proceedings of the 23rd European},
file = {:C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Do et al. - 2015 - Multiple Metric Learning for large margin kNN Classification of time series.pdf:pdf},
keywords = {Classification,Multiple metric learning,Time series,kNN},
mendeley-tags = {Classification,Multiple metric learning,Time series,kNN},
pages = {2391 -- 2395},
title = {{Multiple Metric Learning for large margin kNN Classification of time series}},
year = {2015}
}
@article{Weinberger2009,
abstract = {The accuracy of k-nearest neighbor (kNN) classification depends significantly on the metric used to compute distances between different examples. In this paper, we show how to learn a Maha- lanobis distance metric for kNN classification from labeled examples. The Mahalanobis metric can equivalently be viewed as a global linear transformation of the input space that precedes kNN classification using Euclidean distances. In our approach, the metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a largemargin. As in support vectormachines (SVMs), themargin criterion leads to a convex optimization based on the hinge loss. Unlike learning in SVMs, however, our approach re- quires no modification or extension for problems in multiway (as opposed to binary) classification. In our framework, the Mahalanobis distance metric is obtained as the solution to a semidefinite program. On several data sets of varying size and difficulty, we find that metrics trained in this way lead to significant improvements in kNN classification. Sometimes these results can be further improved by clustering the training examples and learning an individual metric within each cluster. We show how to learn and combine these local metrics in a globally integrated manner. Keywords: convex optimization, semi-definite programming,Mahalanobis distance,metric learn- ing, multi-class classification, support vector machines 1.},
author = {Weinberger, K. and Saul, L.},
file = {:C$\backslash$:/Users/SESA245227/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Weinberger, Saul - 2009 - Distance Metric Learning for Large Margin Nearest Neighbor Classification.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {convex optimization,ing,mahalanobis distance,metric learn-,multi-class classification,semi-definite programming,support vector machines},
pages = {207--244},
title = {{Distance Metric Learning for Large Margin Nearest Neighbor Classification}},
url = {http://www.jmlr.org/papers/volume10/weinberger09a/weinberger09a.pdf},
volume = {10},
year = {2009}
}
