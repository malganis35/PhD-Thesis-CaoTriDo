\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax 
\defcounter {refsection}{0}\relax 
\select@language {american}
\defcounter {refsection}{0}\relax 
\ttl@tocsep 
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {xchapter}{Machine Learning: state of the art}{7}{chapter.1}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {1.1}{\ignorespaces The Beveridge wheat price index is the average in nearly 50 places in various countries measured in successive years from 1500 to 1869. \footnotemark \relax }}{8}{figure.caption.11}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {1.2}{\ignorespaces Division of a dataset into 3 datasets: training, test and evaluation.\relax }}{10}{figure.caption.13}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {1.3}{\ignorespaces General framework for building a supervised (classification/regression) model. Example with 3 features and 2 classes ('Yes' and 'No').\relax }}{11}{figure.caption.14}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {1.4}{\ignorespaces An example of overfitting in the case of classification. The objective is to separate blue points from red points. Black line shows a classifier $f_1$ with low complexity where as green line illustrates a classifier $f_2$ with high complexity. On training examples (blue and red points), the model $f_2$ separates all the classes perfectly but may lead to poor generalization on new unseen examples. Model $f_1$ is often preferred.\relax }}{11}{figure.caption.15}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {1.5}{\ignorespaces Example of a 2 dimensional grid search for parameters $C$ and $\gamma $. It defines a grid where each cell of the grid contains a combination ($C$, $\gamma $). Each combination is used to learn the model and is evaluated on the validation set.\relax }}{12}{figure.caption.16}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {1.6}{\ignorespaces $v$-fold Cross-validation for one combination of parameters. For each of $v$ experiments, use $v-1$ folds for training and a different fold for Testing, then the training error for this combination of parameter is the mean of all testing errors. This procedure is illustrated for $v=4$.\relax }}{12}{figure.caption.17}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {1.7}{\ignorespaces Confusion matrix for a 2-class problem.\relax }}{13}{figure.caption.19}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {1.8}{\ignorespaces A nearly log-normal distribution, and its log \footnotemark \relax }}{16}{figure.caption.21}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {1.9}{\ignorespaces Example of $k$-NN classification. The test sample (green circle) is classified either to the first class (blue squares) or to the second class (red triangles). If $k = 3$ (solid line circle) it is assigned to the second class because there are 2 triangles and only 1 square inside the inner circle. If $k = 5$ (dashed line circle) it is assigned to the first class (3 squares vs. 2 triangles inside the outer circle).\relax }}{17}{figure.caption.23}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {1.10}{\ignorespaces Example of linear classifiers in a 2-dimensional plot. For a set of points of classes +1 and -1 that are linearly separable, there exists an infinite number of separating hyperplanes corresponding to $\textbf {w}.\textbf {x} + b = 0.$\relax }}{19}{figure.caption.29}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {1.11}{\ignorespaces The argument inside the decision function of a classifier is $\textbf {w}.\textbf {x} + b$. The separating hyperplane corresponding to $\textbf {w}.\textbf {x} + b = 0$ is shown as a line in this 2-dimensional plot. This hyperplane separates the two classes of data with points on one side labeled $y_i = +1$ ($\textbf {w}.\textbf {x} + b \geq 0$) and points on the other side labeled $y_i=-1$ ($\textbf {w}.\textbf {x} + b < 0$). Support vectors are circled in blue and lies on the hyperplanes $\textbf {w}.\textbf {x} + b = +1$ and $\textbf {w}.\textbf {x} + b = -1$\relax }}{20}{figure.caption.30}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {1.12}{\ignorespaces Obtained hyperplane after a dual resolution (full blue line). The 2 canonical hyperplanes (dash blue line) contains the support vectors whose $\alpha _i > 0$. Other points have their $\alpha _i = 0$ and the equation of the hyperplane is only affected by the support vectors.\relax }}{23}{figure.caption.31}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {1.13}{\ignorespaces Left: in two dimensions these two classes of data are mixed together, and it is not possible to separate them by a line: the data is not linearly separable. Right: using a Gaussian kernel, these two classes of data (cross and circle) become separable by a hyperplane in feature space, which maps to the nonlinear boundary shown, back in input space.\relax }}{24}{figure.caption.32}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {1.14}{\ignorespaces Illustration of the Gaussian kernel in the 1-dimensional input space for a small and large $\gamma $.\relax }}{25}{figure.caption.33}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {1.15}{\ignorespaces Example of several SVMs and how to interpret the weight vector $\textbf {w}$\relax }}{27}{figure.caption.35}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {1.16}{\ignorespaces Geometric representation of SVM.\relax }}{28}{figure.caption.36}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {1.17}{\ignorespaces Illustration of SVM regression (left), showing the regression curve with the $\epsilon $-insensitive "tube" (right). Samples $\textbf {x}_i$ above the $\epsilon $-tube have $\xi _1 > 0$ and $\xi _1 = 0$, points below the $\epsilon $-tube have $\xi _2 = 0$ and $\xi _2 > 0$, and points inside the $\epsilon $-tube have $\xi = 0$.\relax }}{29}{figure.caption.37}
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {xchapter}{Time series basic metrics}{33}{chapter.2}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {2.1}{\ignorespaces An example of 4 time series that can be compared on different distinct modalities. The objective is to determine which time series is closer to $\textbf {x}_3$.\relax }}{35}{figure.caption.39}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {2.2}{\ignorespaces Example of a same sentence said by two different speakers. Time series are shifted, compressed and dilatated in the time.\relax }}{38}{figure.caption.46}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {2.3}{\ignorespaces Example of DTW grid\relax }}{39}{figure.caption.48}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {2.4}{\ignorespaces UMD dataset. The dataset is made of 3 classes : Up, Middle and Down. The 'Up' class has a characteristic upward bell at the beginning or at the end of the time series. The 'Down' class has a characteristic downward bell at the beginning or at the end of the time series. The 'Middle' class has no characteristic bell. Circle red show the region of interest of these bells. This region are local and standard global metric fails to show these characteristics.\relax }}{40}{figure.caption.52}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {2.5}{\ignorespaces Multi-scale amplitude-based measures $d^{Is}_A$\relax }}{41}{figure.caption.53}
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {xchapter}{Time series advanced metrics}{43}{chapter.3}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.1}{\ignorespaces \relax }}{44}{figure.caption.56}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.2}{\ignorespaces \relax }}{44}{figure.caption.57}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.3}{\ignorespaces Pushed and pulled samples in the $k=3$ target neighborhood of $\textbf {x}_i$ before (left) and after (right) learning. The pushed (vs. pulled) samples are indicated by a white (vs. black) arrows (Weinberger \& Sault~\cite {Weinberger2009}).\relax }}{47}{figure.caption.59}
\defcounter {refsection}{0}\relax 
\ttl@tocsep 
\defcounter {refsection}{0}\relax 
\ttl@tocsep 
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {xchapter}{Projection in the pairwise space}{53}{chapter.4}
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {xchapter}{$M^2TML$: formalization}{55}{chapter.5}
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {xchapter}{$M^2TML$: implementation}{57}{chapter.6}
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {xchapter}{$M^2TML$: Experiments}{59}{chapter.7}
\defcounter {refsection}{0}\relax 
\ttl@tocsep 
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {xchapter}{Detailed presentation of the datasets}{65}{appendix.A}
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {xchapter}{Solver library}{67}{appendix.B}
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {xchapter}{SVM librairy}{69}{appendix.C}
