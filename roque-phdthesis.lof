\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax 
\defcounter {refsection}{0}\relax 
\select@language {american}
\defcounter {refsection}{0}\relax 
\ttl@tocsep 
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {xchapter}{Machine Learning: state of the art}{7}{chapter.1}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {1.1}{\ignorespaces The Beveridge wheat price index is the average in nearly 50 places in various countries measured in successive years from 1500 to 1869. \footnotemark \relax }}{8}{figure.caption.12}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {1.2}{\ignorespaces Example of $k$-NN classification. The test sample (green circle) is classified either to the first class (blue squares) or to the second class (red triangles). If $k = 3$ (solid line circle) it is assigned to the second class because there are 2 triangles and only 1 square inside the inner circle. If $k = 5$ (dashed line circle) it is assigned to the first class (3 squares vs. 2 triangles inside the outer circle).\relax }}{10}{figure.caption.17}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {1.3}{\ignorespaces Example of linear classifiers in a 2-dimensional plot. For a set of points of classes +1 and -1 that are linearly separable, there exists an infinite number of separating hyperplans corresponding to $\textbf {w}.\textbf {x} + b = 0.$\relax }}{12}{figure.caption.24}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {1.4}{\ignorespaces The argument inside the decision function of a classifier is $\textbf {w}.\textbf {x} + b$. The separating hyperplane corresponding to $\textbf {w}.\textbf {x} + b = 0$ is shown as a line in this 2-dimensional plot. This hyperplane separates the two classes of data with points on one side labelled $y_i = +1$ ($\textbf {w}.\textbf {x} + b \geq 0$) and points on the other side labelled $y_i=-1$ ($\textbf {w}.\textbf {x} + b < 0$). Support vectors are circled in blue and lies on the hyperplans $\textbf {w}.\textbf {x} + b = +1$ and $\textbf {w}.\textbf {x} + b = -1$\relax }}{12}{figure.caption.25}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {1.5}{\ignorespaces Obtained hyperplan after a dual resolution (full blue line). The 2 canonical hyperplans (dash blue line) contains the support vectors whose $\alpha _i > 0$. Other points have their $\alpha _i = 0$ and the equation of the hyperplan is only affected by the support vectors.\relax }}{16}{figure.caption.26}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {1.6}{\ignorespaces Left: in two dimensions these two classes of data are mixed together, and it is not possible to separate them by a line: the data is not linearly separable. Right: using a Gaussian kernel, these two classes of data (cross and circle) become separable by a hyperplane in feature space, which maps to the nonlinear boundary shown, back in input space.\relax }}{17}{figure.caption.27}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {1.7}{\ignorespaces Illustration of the Gaussian Kernel in the input space.\relax }}{18}{figure.caption.30}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {1.8}{\ignorespaces Example of several SVMs and how to interpret the weight vector $\textbf {w}$\relax }}{20}{figure.caption.32}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {1.9}{\ignorespaces \relax }}{21}{figure.caption.33}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {1.10}{\ignorespaces Illustration of SVM regression (left), showing the regression curve with the $\epsilon $-insensitive "tube" (right). Samples $\textbf {x}_i$ above the $\epsilon $-tube have $\xi _1 > 0$ and $\xi _1 = 0$, points below the $\epsilon $-tube have $\xi _2 = 0$ and $\xi _2 > 0$, and points inside the $\epsilon $-tube have $\xi = 0$.\relax }}{22}{figure.caption.35}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {1.11}{\ignorespaces General framework for building a supervised (classification/regression) model.\relax }}{24}{figure.caption.37}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {1.12}{\ignorespaces An example of overfitting for a classification problem. The objective is to separate blue points from red points. Black line shows a separator $f_1$ with low complexity where as green line illustrates a model $f_2$ with high compelexity. On training examples (blue and red points), the model $f_2$ separates all the classes perfectly but may lead to poor generalization on new unseen examples. \relax }}{24}{figure.caption.38}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {1.13}{\ignorespaces Example of a 2 dimensional grid search for parameters $C$ and $\gamma $. It defines a grid where each cell of the grid contains a combination ($C$, $\gamma $). Each combination is used to learn the model and is evaluated on the validation set.\relax }}{25}{figure.caption.39}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {1.14}{\ignorespaces $v$-fold Cross-validation for one combination of parameters. For each of $v$ experiments, use $v-1$ folds for training and a different fold for Testing, then the training error for this combination of parameter is the mean of all testing errors. This procedure is illustrated for $v=4$.\relax }}{26}{figure.caption.40}
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {xchapter}{Time series basic metrics}{27}{chapter.2}
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {xchapter}{Time series advanced metrics}{31}{chapter.3}
\defcounter {refsection}{0}\relax 
\ttl@tocsep 
\defcounter {refsection}{0}\relax 
\ttl@tocsep 
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {xchapter}{Projection in the pairwise space}{37}{chapter.4}
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {xchapter}{$M^2TML$: formalization}{39}{chapter.5}
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {xchapter}{$M^2TML$: implementation}{41}{chapter.6}
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {xchapter}{$M^2TML$: Experiments}{43}{chapter.7}
\defcounter {refsection}{0}\relax 
\ttl@tocsep 
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {xchapter}{Detailed presentation of the datasets}{49}{appendix.A}
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {xchapter}{Solver library}{51}{appendix.B}
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {xchapter}{SVM librairy}{53}{appendix.C}
